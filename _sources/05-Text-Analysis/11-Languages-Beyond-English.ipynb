{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working in Languages Beyond English\n",
    "\n",
    "By [Quinn Dombrowski](http://www.quinndombrowski.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: lightblue; padding: 10px\">\n",
    "<p class=\"title\">Note</p>\n",
    "This section, \"Working in Languages Beyond English,\" is authored by <a href=\"https://dlcl.stanford.edu/people/quinn-dombrowski/\">Quinn Dombrowski</a>, the Academic Technology Specialist at Stanford University and a leading voice in multilingual digital humanities. I'm grateful to Quinn for helping expand this textbook to serve languages beyond English. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tools and tutorials you'll find for computational text analysis assume that you're working with English language text. This section is dedicated to helping students and scholars accomplish text analysis tasks in languages beyond English. Select lessons are adapted for non-English languages including Danish, Spanish, Chinese, and Russian.\n",
    "\n",
    "## Two Kinds of Text Analysis\n",
    "The steps you need to take to analyze a language beyond English will depend on the kind of text analysis method that you are interesting in using. \n",
    "\n",
    "The methods introduced in this chapter can be broadly organized into two groups:\n",
    "\n",
    "1) Methods based on word counts — such as TF-IDF and topic modeling\n",
    "\n",
    "2) Methods that use language-specific NLP models — such as Named Entity Recognition and part-of-speech-tagging\n",
    "\n",
    "There are more resources to support non-English text analysis in the first group of methods than in the second group.\n",
    "\n",
    "To apply the first group of methods to non-English texts, you will need to *pre-process* your texts — in other words, to create a derivative version of your text that will work better with these tools. \n",
    "\n",
    "To apply the second group of methods to non-English texts, you will need to find a language-specific version of the NLP models. Unfortunately, for most of the roughly 6,500 languages spoken in the world, there are currently few if any language-specific tools or resources to support computational analysis. Out of the 100 languages with the greatest number of speakers, at least 2/3 are missing the tools you'll need to complete all the activities in this section of the textbook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis Based on Word Counts: Pre-Processing Non-English Texts\n",
    "\n",
    "The pre-processing steps needed to make texts in other languages usable with computational text analysis methods vary depending on the language. For example, some languages, such as Chinese, do not separate words with spaces, and texts in these languages will need to have artifical spaces inserted before text analysis.\n",
    "\n",
    "Other languages with more *inflection* than English (e.g. where words appear in different forms, depending on how they're used) need to be *lemmatized*, replacing every variant word form with the dictionary form, or *stemmed*, cutting off the inflection at the end of the word. Lemmatizing or stemming usually (but not always) leaves you with something resembling the root. For example, in Spanish `hablar` (\"to speak\") and its inflected forms `hablo` (\"I speak\") and `hablas` (\"you speak\") all become `hab` when stemmed.\n",
    "\n",
    "The situation is even more complicated for languages known as *agglutinative languages*, in which words are formed by repeatedly gluing together *morphemes*, or small bits of meaning. In agglutinative languages, a single \"word\" can be translated as an entire English sentence. How would you reduce a word like Turkish *Çekoslovakyalılaştıramadıklarımızdanmışsınız* — meaning, \"you are reportedly one of those that we could not make Czechoslovakian\" — down to a root that you could count?\n",
    "\n",
    "When doing text analysis in English, you can do things like word frequency without thinking too much about questions like \"what, actually, is a word?\" However, the ways you have to modify text in many other languages to make it compatible with computational text analysis — even to the point of harming human readability — mean that you have to grapple with this question more directly when working with other languages.\n",
    "\n",
    "If you want to do text analysis with word counts for Danish or Spanish, you will first need to pre-process the texts for your chosen language, and then use the derivative text for the TF-IDF or topic modeling code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis Based on NLP Models: Non-English NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The named-entity recognition and part-of-speech keywords sections also have language-specific dependencies; there are separate versions of those tutorials for a number of requested languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
