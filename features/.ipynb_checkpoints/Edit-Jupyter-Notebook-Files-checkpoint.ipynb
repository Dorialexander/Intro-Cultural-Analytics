{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Jupyter Notebook Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "rootdir = Path('/Volumes/MelData/Intro-Cultural-Analytics/features/')\n",
    "# Return a list of regular files only, not directories\n",
    "file_list = [f for f in rootdir.glob('**/*.ipynb') if f.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/course-schedule.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Untitled.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Mapping copy.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Custom-Maps.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Google-Geocode.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Publish-Your-Map.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Untitled-Copy1.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Mapping-Home.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/Mapping.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Publish-Your-Map-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Untitled-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Custom-Maps-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Mapping-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Mapping-Home-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Google-Geocode-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Mapping/.ipynb_checkpoints/Mapping copy-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling-Code.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling-Text-Files.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/NER-Cluster-Characters.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling-CSV.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF-Copy1.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF-Code-Copy1.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/2020-The-House-on-Mango-Street-character-network-clean.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling-Time-Series.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling-Overview.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Untitled.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Named-Entity-Recognition.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling-Set-Up.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Make-Character-Network.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/POS-Keywords-Code.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/KWIC.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/NER-Code.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Network-Analysis.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/2020-Mango-Character-Network-Bimodal-Unimodal.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF-Code.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Text-Analysis.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/POS-Keywords.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Topic-Modeling.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/Network-Analysis-Expanded.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/little_mallet_test/demo.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/POS-Keywords-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Network-Analysis-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-Text-Files-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/NER-Cluster-Characters-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/NER-Code-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/TF-IDF-Code-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Text-Analysis-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/KWIC-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-Time-Series-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-Overview-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/POS-Keywords-Code-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Make-Character-Network-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/TF-IDF-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Models-Text-Files-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-Code-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/2020-Mango-Character-Network-Bimodal-Unimodal-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Text-Analysis-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Untitled-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-CSV-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Named-Entity-Recognition-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/TF-IDF-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/TF-IDF-Code-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Network-Analysis-Expanded-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/2020-The-House-on-Mango-Street-character-network-clean-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Text-Analysis/.ipynb_checkpoints/Topic-Modeling-Set-Up-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/Pandas-Targeted.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/Intro.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/Pandas-Merge-Datasets.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/Pandas-EDA.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/Pandas.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/.ipynb_checkpoints/Intro-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/.ipynb_checkpoints/Pandas-Targeted-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/.ipynb_checkpoints/Pandas-Merge-Datasets-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/.ipynb_checkpoints/Pandas-EDA-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Cultural-Data-Analysis/.ipynb_checkpoints/Pandas-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Final-Project/Final-Project.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Final-Project/.ipynb_checkpoints/Final-Project-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Lists-Loops.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/String-Methods.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Installation.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/What-Were-Not-Covering.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Data-Types.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Why-Python.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Character-Encoding.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Common-Python-Errors.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/More-Lists-Loops.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Functions.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Variables.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Dictionaries.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/How-to-Use-Jupyter.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Conditionals-Comparisons.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Jupyter-Keyboard-Shortcuts.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Life-Anatomy-Python-Script.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/Randomize-Students.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Character-Encoding-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Functions-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/life-anatomy-python-script-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Jupyter-Keyboard-Shortcuts-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Get-Beyonce-Lemonade-Lyrics-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Lists-Loops-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/life-anatomy-python-script-with-extras-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/How-to-Use-Jupyter-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/More-Lists-Loops-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Installation-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Conditionals-Comparisons-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/What-Were-Not-Covering-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Randomize-Students-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Common-Python-Errors-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Variables-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Why-Python-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Pandas-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Dictionaries-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Get-Genius-Song-Lyrics-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/String-Methods-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/.ipynb_checkpoints/Data-Types-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/Conditionals-Comparisons-Answer-Key.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/Expressions-Conditionals-Copy1.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/Expressions-Conditionals-Copy2.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/Filler3.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/Get-Song-Lyrics.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/Fundamentals.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/.ipynb_checkpoints/Conditionals-Comparisons-Answer-Key-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/.ipynb_checkpoints/Expressions-Conditionals-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/.ipynb_checkpoints/Expressions-Conditionals-Copy2-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/.ipynb_checkpoints/Filler3-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/.ipynb_checkpoints/Get-Song-Lyrics-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Python/other_files/.ipynb_checkpoints/Fundamentals-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Data-Visualization/Data-Visualization.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Data-Visualization/.ipynb_checkpoints/Data-Visualization-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Datasets/Datasets.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Datasets/.ipynb_checkpoints/Datasets-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Command-Line/The-Command-Line.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Command-Line/.ipynb_checkpoints/The-Command-Line-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-4-Pandas.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-3-Lists-Loops.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-9-Topic-Modeling.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-10-NER.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-3-Conditionals-Comparisons.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-7-Twitter-Data.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-2-Variables-Data-Types.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-6-Twitter-Setup.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-5-Functions-Pandas.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/HW-8-TF-IDF.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/Save-Txt.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-5-Functions-Pandas-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-6-Twitter-Setup-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-3-Conditionals-Comparisons-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-9-Topic-Modeling-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-8-TF-IDF-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-4-Pandas-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-7-Twitter-Data-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-3-Lists-Loops-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/Save-Txt-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-2-Variables-Data-Types-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Homework/.ipynb_checkpoints/HW-10-NER-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/.ipynb_checkpoints/course-schedule-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/.ipynb_checkpoints/Untitled-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/Make-Character-Network.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/Making-Network-Viz-with-Bokeh.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/Network-Analysis.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/Network-Analysis-Home.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/Make-Network-Viz-Quick-Function.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/.ipynb_checkpoints/Network-Analysis-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/.ipynb_checkpoints/Make-Network-Viz-Quick-Function-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/.ipynb_checkpoints/Network-Analysis-Home-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/.ipynb_checkpoints/Make-Character-Network-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Network-Analysis/.ipynb_checkpoints/Making-Network-Viz-with-Bokeh-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Twitter-Collection-Setup.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Reddit-Data-Collection-With-Praw.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Song-Genius-Data-Collection.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Reddit-Data-Collection.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Display-Images-with-Pandas.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Getting-Cultural-Data.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Convert-Kindle-to-Text.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Twitter-Data-Sharing.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Get-All-Song-Lyrics-From-Album.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Untitled.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Reddit-Data-Collection-With-Pushshift.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Twitter-Data-Analysis.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/LyricsGenius.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Song-Lyrics-Analysis.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Web-Scraping.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/JSON-Normalize.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/APIs.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Twitter-Data-Collection-Home.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Configure-Twarc-Cloud.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Twitter-Data-Collection.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Web-Scraping-Plus-RegEx.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Genius-API.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/Git-GitHub.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/in-progress/Web-Scraping-Answer-Key.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/in-progress/Web-Scraping-Extended.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/in-progress/Twitter-Data-Copy1.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/in-progress/.ipynb_checkpoints/Web-Scraping-Extended-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/in-progress/.ipynb_checkpoints/Web-Scraping-Answer-Key-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Configure-Twarc-Cloud-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Web-Scraping-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Git-GitHub-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Genius-API-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/LyricsGenius-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Advanced-Twarc-Start-Stop-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Twitter-Data-Copy1-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Twitter-Data-Analysis-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/JSON-Normalize-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Getting-Cultural-Data-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Get-All-Song-Lyrics-From-Album-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Web-Scraping-Plus-RegEx-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Song-Lyrics-Analysis-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Display-Images-with-Pandas-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/APIs-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Twitter-Data-Collection-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Twitter-Data-Collection-Home-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Untitled-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Reddit-Data-Collection-With-Praw-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Reddit-Data-Collection-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Convert-Kindle-to-Text-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Reddit-Data-Collection-With-Pushshift-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Twitter-Collection-Setup-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Song-Genius-Data-Collection-checkpoint.ipynb'),\n",
       " PosixPath('/Volumes/MelData/Intro-Cultural-Analytics/features/Collecting-Cultural-Data/.ipynb_checkpoints/Twitter-Data-Sharing-checkpoint.ipynb')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add border to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/melaniewalsh\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Hours with Prof. Walsh <br>\n",
      "Thursday 1-3pm by appt https://melaniewalsh.youcanbook.me // Gates 211\n",
      "\n",
      "TA Study Hall<br>\n",
      "Wednesday 5:30-6:30pm // Rhodes 408 <br>\n",
      "Friday 2:30-3:30pm // Rhodes 597\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"/Volumes/MelData/Intro-Cultural-Analytics/features/course-schedule.ipynb\") as f:\n",
    "    data = json.load(f)\n",
    "    #print(data['source'])\n",
    "    #print(data)\n",
    "    for cell in data['cells']:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            if \"Walsh\" in cell['source']:\n",
    "                print(cell['source'])\n",
    "#                 #content = [\"\".join(inner_content) for inner_content in content]\n",
    "\n",
    "#                 cell['source'] = \"\".join(cell['source'])\n",
    "#                 #print(type(content))\n",
    "#                         #content = content.replace('\", border=2>', '\" border=2>')\n",
    "#                     #print(content)\n",
    "#                     #cell['source'] = list(content)\n",
    "#                     #ell['source']\n",
    "#                     #print(content)\n",
    "#                 print(cell['source'])\n",
    "#                 #with open('/Volumes/MelData/Intro-Cultural-Analytics/features/course-schedule.ipynb', 'w') as json_file:\n",
    "#                  #   json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        #print(data['source'])\n",
    "\n",
    "        for cell in data['cells']:\n",
    "            #if cell['cell_type'] == 'markdown':\n",
    "                #for content in cell['source']:\n",
    "                    #print(cell['source'])\n",
    "                    #content = [\"\".join(inner_content) for inner_content in content]\n",
    "                    \n",
    "            cell['source'] = \"\".join(cell['source'])\n",
    "                    #print(type(content))\n",
    "                            #content = content.replace('\", border=2>', '\" border=2>')\n",
    "                        #print(content)\n",
    "                        #cell['source'] = list(content)\n",
    "                        #ell['source']\n",
    "            #print(content)\n",
    "            #with open(file, 'w') as json_file:\n",
    "                #json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        #print(data['source'])\n",
    "\n",
    "        for cell in data['cells']:\n",
    "            if cell['cell_type'] == 'markdown':\n",
    "                #for content in cell['source']:\n",
    "                    #print(cell['source'])\n",
    "                    #content = [\"\".join(inner_content) for inner_content in content]\n",
    "                if \"<img\" in cell['source']:\n",
    "                    cell['source'] = cell['source'].replace(', border=2>', ' >')\n",
    "                    #print(cell['source'])\n",
    "                        #content = content\n",
    "                    with open(file, 'w') as json_file:\n",
    "                        json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Hours with Prof. Walsh <br>\n",
      "Thursday 1-3pm by appt https://melaniewalsh.youcanbook.me // Gates 211\n",
      "\n",
      "TA Study Hall<br>\n",
      "Wednesday 5:30-6:30pm // Rhodes 408 <br>\n",
      "Friday 2:30-3:30pm // Rhodes 597\n",
      "## \\*Revised\\* Course Schedule \n",
      "\n",
      "| Course Unit | Technical Lesson | Date | Technical Reading | Critical/Cultural Reading | Assignments |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| What Is Cultural Analytics? |  | Tues 1/21 |  |  |  |\n",
      "| The Command Line | [The Command Line](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Command-Line/The-Command-Line.html) | Th 1/23 | [The Command Line](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Command-Line/The-Command-Line.html) | [“The Yellow Wallpaper,”](https://www.nlm.nih.gov/exhibition/theliteratureofprescription/exhibitionAssets/digitalDocs/The-Yellow-Wall-Paper.pdf)**\\*** <br>Charlotte Perkins Gilman | [HW 1](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-1-Command-Line.html) <br> (Due Friday 5pm) |\n",
      "| Cultural Data + Python Fundamentals | [Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html) | Tues 1/28 | [How to Use Jupyter](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/How-to-Use-Jupyter.html) // <br> [The Life and Anatomy of a Python Script](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Life-Anatomy-Python-Script.html) | “What Gets Counted Counts,”**\\*** <br>*Data Feminism*, <br>Lauren Klein and Catherine D'Ignazio | [HW 1.5](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-1-5-Installation.html) <br> (Due Tuesday 9am) |\n",
      "|  | [Data Types](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Data-Types.html) & [String Methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html) | Th 1/30 | [Character Encoding](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Character-Encoding.html) | [“I Can Text You A Pile of Poo, But I Can’t Write My Name,”](https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name)**\\*** <br> Aditya Mukerjee | [HW 2](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-2-Variables-Data-Types.html) (Due Sunday 5pm) <br> |\n",
      "|  | [Conditionals & Comparisons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Conditionals-Comparisons.html) | Tues 2/4 |  | <a href=http://crdh.rrchnm.org/essays/v01-10-(re)-humanizing-data/>“(Re)Humanizing Data: Digitally Navigating the Bellevue Almshouse”<a>**\\*** <br> Anelise Hanson Shrout |  |\n",
      "|  | [Lists & Loops](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html) | Th 2/6 |  | [“Data Biographies,”](https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/) <br>Heather Krause | HW 3 ([Part I](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-3-Conditionals-Comparisons.html) & [II](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-3-Lists-Loops.html) Friday 5pm) |\n",
      "|  | [Lists & Loops Plus Modules](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html) | Tues 2/11 |  | \"Explore [*Slave Voyages*](https://www.slavevoyages.org/) // <br><a href=https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death>“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>**\\*** <br>Jessica Marie Johnson |  |\n",
      "|  | [Pandas](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Cultural-Data-Analysis/Pandas.html) | Th 2/13 | [Dictionaries](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Dictionaries.html) |  | [HW 4](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-4-Pandas.html) (Due Tuesday 9am) |\n",
      "|  | [Functions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Functions.html) & [Pandas (Exploratory Data Analysis)](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Cultural-Data-Analysis/Pandas-EDA.html) | Tues 2/18 |  | [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”,](https://pudding.cool/2017/03/film-dialogue/) <br>Hannah Anderson and Matt Daniels |  |\n",
      "|  |  | Th 2/20 | [Pandas (Targeted Analysis)](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Cultural-Data-Analysis/Pandas-Targeted.html) | [Film Dialogue FAQ](https://medium.com/@matthew_daniels/faq-for-the-film-dialogue-by-gender-project-40078209f751); Know data biography for The Pudding film dialogue data | [HW 5](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-5-Functions-Pandas.html) (Due following Thursday 9am) |\n",
      "| *February Break* |  | Tues 2/25 |  | *February Break* |  |\n",
      "| Collecting Cultural Data | [Web Scraping](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Web-Scraping.html) | Th 2/27 |  | [“The Largest Vocabulary in Hip-Hop,”](https://pudding.cool/projects/vocabulary/index.html) <br>Matt Daniels |  |\n",
      "|  | [Web Scraping Plus Regular Expressions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Web-Scraping-Plus-Regex.html) | Tues 3/3 |  | [“The Secretive Company That Might End Privacy as We Know It”](https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html)**\\*** <br>Kashmir Hill |  |\n",
      "|  | [APIs](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/APIs.html) & [GitHub](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Git-Github.html) | Th 3/5 | [Install Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git); [A Dead Simple Intro to GitHub for the Non-Technical](https://medium.com/crowdbotics/a-dead-simple-intro-to-github-for-the-non-technical-f9d56410a856) |  | [HW 6](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-6-Twitter-Setup.html) (Due Tuesday 9am) |\n",
      "|  | [Twitter Data Collection](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Collection.html) | Tues 3/10 |  | “#GirlsLikeUs: Trans advocacy and community building online,”**\\*** <br>Sarah J Jackson, Moya Bailey, and Brooke Foucault Welles |  |\n",
      "|  | [Twitter Data Analysis](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Analysis.html) | Th 3/12 |  | [“How China Unleashed Twitter Trolls to Discredit Hong Kong’s Protesters,”](https://www.nytimes.com/interactive/2019/09/18/world/asia/hk-twitter.html) <br>Raymond Zhong, Steven Lee Myers and Jin Wu | [HW 7](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-7-Twitter-Data.html) (Due Tuesday April 7 9am) |\n",
      "| No Class | | Tues 3/17 | |   |  |\n",
      "| No Class |  | Th 3/19 |  |   |  |\n",
      "| No Class |  | Tues 3/24 |  |  |  |\n",
      "| No Class |  | Th 3/26 |  |   |  |\n",
      "| *Spring Break* |  | Tues 3/31 |   | Recommended Reading: [“The Transformation of Gender in English-Language Fiction,”](https://culturalanalytics.org/article/11035) <br>Ted Underwood, David Bamman, and Sabrina Lee |  |\n",
      "| *Spring Break* |  | Th 4/2 |  |  |  |\n",
      "|  | Twitter Data Wrap-Up // The Path Forward | Tues 4/7 |  | Recommended Reading: <a href=\"https://cmci.colorado.edu/~cafi5706/ICWSM2020_datascraping.pdf\">No Robots, Spiders, or Scrapers: Legal and Ethical Regulation of Data Collection Methods in Social Media Terms of Service</a>, Casey Fiesler, Nathan Beard, Brian C. Keegan |  |\n",
      "| Text Analysis | [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/TF-IDF.html) | Th 4/9 |  | | [HW 8](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-8-TF-IDF.html)  (Due Friday) |\n",
      "|  | [Topic Modeling](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html) | Tues 4/14 |  | [“Narrative Paths and Negotiation of Power in Birth Stories,”](https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf)**\\*** <br>Maria Antoniak, David Mimno, and Karen Levy  |  |\n",
      "|  |  | Th 4/16 | |  | [HW 9](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-9-Topic-Modeling.html) + Discussion Post (Due Friday) |\n",
      "|  | [Named Entity Recognition](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Named-Entity-Recognition.html) | Tues 4/21 |  |  Excerpts, *Lost in the City* **\\***, <br> Edward P. Jones| |\n",
      "|  | [Part-of-Speech Tagging](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/POS-Keywords.html) | Th 4/23 |  |[\"Introduction,\"](https://iopn.library.illinois.edu/scalar/lost-in-the-city-a-exploration-of-edward-p-joness-short-fiction-/the-introduction-an-authors-note) [\"A Multimedia Literary Analysis,” (Sections 1-3)](https://iopn.library.illinois.edu/scalar/lost-in-the-city-a-exploration-of-edward-p-joness-short-fiction-/lost-in-the-city---section-2?path=chapter-3-lost-in-the-city) <br> *Lost in the City: An Exploration of Edward P. Jones's Short Fiction* <br>Kenton Rambsy and Peace Ossom-Williamson  | [HW 10](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-10-NER.html) + Discussion Post (Due Friday) |\n",
      "| Network Analysis |  | Tues 4/28 | [Scott Weingart, \"Demystifying Networks\"](http://journalofdigitalhumanities.org/1-1/demystifying-networks-by-scott-weingart/) |  [“Network of Thrones,”](https://www.maa.org/sites/default/files/pdf/Mathhorizons/NetworkofThrones%20%281%29.pdf) <br>Andrew Beveridge and Jie Shan // <br>[\"\"Mathematicians mapped out every Game of Thrones relationship to find the main character,”](https://qz.com/650796/mathematicians-mapped-out-every-game-of-thrones-relationship-to-find-the-main-character/) <br>Adam Epstein \" |  |\n",
      "| Mapping |  | Th 4/30 |  |  [Torn Apart / Separados](http://xpmethod.plaintext.in/torn-apart/volume/2/index) <br>Manan Ahmed et al // <br>['ICE Is Everywhere': Using Library Science to Map the Separation Crisis,](https://www.wired.com/story/ice-is-everywhere-using-library-science-to-map-child-separation/) <br>Emily Dreyfuss // [“How an internet mapping glitch turned a random Kansas farm into a digital hell”](https://splinternews.com/how-an-internet-mapping-glitch-turned-a-random-kansas-f-1793856052) <br>Kashmir Hill | Discussion Post (Due Friday)  |\n",
      "| Final Project |  | Tues 5/5 |  |  |  |\n",
      "| Final Project  |  | Thu 5/7 |  |  |  |\n",
      "|  Final Project | Conclusions | Tues 5/12 |  |  | |\n",
      "|  |  | Monday 5/18 |  |  | [Final Project Due](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Final-Project/Final-Project.html) |\n",
      "\n",
      "## Notable Events Calendar\n",
      "Find our [Notable Events calendar here](https://melaniewalsh.github.io/Intro-Cultural-Analytics/notable-events.html)\n",
      "# Edit Jupyter Notebook Files\n",
      "from pathlib import Path\n",
      "\n",
      "rootdir = Path('/Volumes/MelData/Intro-Cultural-Analytics/features/')\n",
      "# Return a list of regular files only, not directories\n",
      "file_list = [f for f in rootdir.glob('**/*.ipynb') if f.is_file()]\n",
      "file_list\n",
      "## Add border to images\n",
      "import json\n",
      "\n",
      "for file in file_list:\n",
      "    with open(file) as f:\n",
      "        data = json.load(f)\n",
      "        #print(data['source'])\n",
      "\n",
      "        for cell in data['cells']:\n",
      "            #if cell['cell_type'] == 'markdown':\n",
      "                for content in cell['source']:\n",
      "                    #print(cell['source'])\n",
      "                    content = [\"\".join(inner_content) for inner_content in content]\n",
      "                    print(content)\n",
      "                    #print(type(content))\n",
      "                            #content = content.replace('\", border=2>', '\" border=2>')\n",
      "                        #print(content)\n",
      "                        #cell['source'] = list(content)\n",
      "                        #ell['source']\n",
      "                        #print(content)\n",
      "                        #with open(file, 'w') as json_file:\n",
      "                          #  json.dump(data, json_file)\n",
      "## Change H1 headers to H2 headers\n",
      "import json\n",
      "\n",
      "for file in file_list:\n",
      "    with open(file) as f:\n",
      "        data = json.load(f)\n",
      "        #print(data['source'])\n",
      "\n",
      "        for cell in data['cells']:\n",
      "            if cell['cell_type'] == 'markdown':\n",
      "                for content in cell['source']:\n",
      "                    #print(cell['source'])\n",
      "                    if \"# \" in content:\n",
      "                        content = re.sub(r\"(?<!#)# \", \"## \", content)\n",
      "                        #print(content)\n",
      "                        #content = content.replace('>', ', border=2>')\n",
      "                        cell['source'] = list(content)\n",
      "                        #ell['source']\n",
      "                        #print(content)\n",
      "                        with open(file, 'w') as json_file:\n",
      "                            json.dump(data, json_file)\n",
      "import re\n",
      "sample = \"# header ## not header\"\n",
      "\n",
      "import json\n",
      "\n",
      "for file in file_list:\n",
      "    with open(file) as f:\n",
      "        data = json.load(f)\n",
      "        contents = [cell['source'] for cell in data['cells'] if cell['source'].contains('<img')]\n",
      "        print(contents)\n",
      "import json\n",
      "\n",
      "\n",
      "with open('Cultural-Data-Analysis/Pandas.ipynb') as f:\n",
      "    data = json.load(f)\n",
      "    #print(data['source'])\n",
      "\n",
      "    for cell in data['cells']:\n",
      "        if cell['cell_type'] == 'markdown':\n",
      "            for content in cell['source']:\n",
      "                #print(cell['source'])\n",
      "                if \"<img\" in content:\n",
      "                    content = content.replace('>', ', border=1>')\n",
      "                    cell['source'] = list(content)\n",
      "                    #ell['source']\n",
      "                    #print(content)\n",
      "                    with open('Cultural-Data-Analysis/Pandas.ipynb', 'w') as json_file:\n",
      "                        json.dump(data, json_file)\n",
      "\n",
      "## Mapping\n",
      "[Download relevant files](https://melaniewalsh.org/Mapping.zip)\n",
      "In this lesson, we're going to learn how to analyze and visualize geographic data.\n",
      "## Geocoding with GeoPy\n",
      "First, we're going to geocode data — aka get coordinates from addresses or place names — with the Python package [GeoPy](https://geopy.readthedocs.io/en/stable/#). GeoPy makes it easier to use a range of third-party [geocoding API services](https://geopy.readthedocs.io/en/stable/#), such as Google, Bing, ArcGIS, and OpenStreetMap.\n",
      "\n",
      "Though most of these services require an API key, Nominatim, which uses OpenStreetMap data, does not, which is why we're going to use it here.\n",
      "To install GeoPy, run this cell:\n",
      "!pip install geopy\n",
      "From GeoPy's list of possible geocoding services, we're going to import Nominatim:\n",
      "from geopy.geocoders import Nominatim\n",
      "## Nominatim & OpenStreetMap\n",
      "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/256px-Openstreetmap_logo.svg.png\", border=2>\n",
      "Nominatim (which means \"name\" in Latin) uses [OpenStreetMap data](https://www.openstreetmap.org/relation/174979) to match addresses with geopgraphic coordinates. Though we don't need an API key to use Nominatim, we do need to create a unique [application name](https://operations.osmfoundation.org/policies/nominatim/). \n",
      "Here we're initializing Nominatim as a variable called `geolocator`. Change the application name below to your own application name:\n",
      "geolocator = Nominatim(user_agent=\"YOUR NAME's mapping app\", timeout=2)\n",
      "To geocode an address or location, we simply use the `.geocode()` function:\n",
      "location = geolocator.geocode(\"South Cayuga Street\")\n",
      "location\n",
      "## Google Geocoding API\n",
      "The Google Geocoding API is superior to Nominatim, but it requires an API key and more set up. To enable the Google Geocoding API and get an API key, see [Get Started with Google Maps Platform](https://developers.google.com/maps/gmp-get-started) and [Get Started with Geocoding API](https://developers.google.com/maps/documentation/geocoding/start).\n",
      "#from geopy.geocoders import GoogleV3\n",
      "#google_geolocator = GoogleV3(api_key=\"YOUR-API-KEY HERE\")\n",
      "#google_geolocator.geocode(\"Cayuga Street\")\n",
      "## Get Address\n",
      "print(location.address)\n",
      "## Get Latitude and Longitude\n",
      "print(location.latitude, location.longitude)\n",
      "## Get \"Importance\" Score\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "## Get Class and Type\n",
      "print(f\"Class: {location.raw['class']} \\nType: {location.raw['type']}\")\n",
      "## Get Multiple Possible Matches\n",
      "possible_locations = geolocator.geocode(\"College Ave\", exactly_one=False)\n",
      "\n",
      "for location in possible_locations:\n",
      "    print(location.address)\n",
      "    print(location.latitude, location.longitude)\n",
      "    print(f\"Importance: {location.raw['importance']}\")\n",
      "location = geolocator.geocode(\"College Ave, Ithaca NY\")\n",
      "\n",
      "print(location.address)\n",
      "print(location.latitude, location.longitude)\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "## Geocode with Pandas\n",
      "To geocode every location in a CSV file, we can use Pandas, make a Python function, and `.apply()` it to every row in the CSV file.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "Here we make a function with `geolocator.geocode()` and ask it to return the address, lat/lon, and importance score:\n",
      "def find_location(row):\n",
      "    \n",
      "    place = row['place']\n",
      "    \n",
      "    location = geolocator.geocode(place)\n",
      "    \n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "To start exploring, let's read in a CSV file with a list of places in and around Ithaca.\n",
      "ithaca_df = pd.read_csv(\"../data/ithaca-places.csv\")\n",
      "ithaca_df\n",
      "Now let's `.apply()` our function to this Pandas dataframe and see what results Nominatim's geocoding service spits out.\n",
      "ithaca_df[['address', 'lat', 'lon', 'importance']] = ithaca_df.apply(find_location, axis=\"columns\", result_type=\"expand\")\n",
      "ithaca_df\n",
      "**What do you notice about these results?** ☝️☝️☝️\n",
      "<a href=\"https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc\", border=2><img src=\"https://photos.wikimapia.org/p/00/05/41/92/38_big.jpg\", border=2></a, border=2>\n",
      "**[Wharton Studio Inc.](https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc)** (1914-1919) — early 20th-century Ithaca movie studio, located in what is now Stewart Park  \n",
      "*To check out more historical photos of Wharton Studio Inc., see [the Cornell library](https://digital.library.cornell.edu/catalog/ss:550440).*\n",
      "## Mapping (Interactively) with Folium\n",
      "To map our geocoded coordinates, we're going to use the Python library [Folium](https://python-visualization.github.io/folium/). Folium is built on top of the popular JavaScript library [Leaflet](https://leafletjs.com/).\n",
      "To install and import Folium, run the cells below:\n",
      "!pip install folium\n",
      "import folium\n",
      "## Base Map\n",
      "First, we need to establish a base map. This is where we'll map our geocoded Ithaca locations. To do so, we're going to call `folium.Map()`and enter the general latitude/longitude coordinates of the Ithaca area at a particular zoom.\n",
      "\n",
      "(To find latitude/longitude coordintes for a particular location, you can use Google Maps, [as described here](https://support.google.com/maps/answer/18539?co=GENIE.Platform%3DDesktop&hl=en).)\n",
      "ithaca_map = folium.Map(location=[42.44, -76.5], zoom_start=14)\n",
      "ithaca_map\n",
      "## Add a Marker\n",
      "Adding a marker to a map is easy with Folium! We'll simply call `folium.Marker()` at a particular lat/lon, enter some text to display when the marker is clicked on, and then add it to our base map.\n",
      "folium.Marker(location=[42.444695, -76.482233], popup=\"Intro to Cultural Analytics\").add_to(ithaca_map)\n",
      "ithaca_map\n",
      "## Add Markers From Pandas Data\n",
      "To add markers for every location in our Pandas dataframe, we can make a Python function and `.apply()` it to every row in the dataframe.\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "Before we apply this function to our dataframe, we're going to drop any locations that were \"Not Found\" (which would cause `folium.Marker()` to return an error).\n",
      "found_ithaca_locations = ithaca_df[ithaca_df['address'] != \"Not Found\"]\n",
      "found_ithaca_locations.apply(create_map_markers, map_name=ithaca_map, axis='columns')\n",
      "ithaca_map\n",
      "## Save Map\n",
      "ithaca_map.save(\"Ithaca-map.html\")\n",
      "## Mapping Places From Texts — *Lost in the City*\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "from collections import Counter\n",
      "filepath = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "        if named_entity.label_ in [\"GPE\", \"FAC\"]:\n",
      "            places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "lost_df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "lost_df\n",
      "def find_location_with_help(row, helper_location=''):\n",
      "    \n",
      "    place = f\"{row['place']} {helper_location}\"\n",
      "    \n",
      "    location = geolocator.geocode(place, exactly_one=True)\n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "lost_df[['address', 'lat', 'lon', 'importance']] = lost_df.apply(find_location_with_help, helper_location=', Washington, DC', axis=\"columns\", result_type=\"expand\")\n",
      "lost_df\n",
      "found_lost_city_locations = lost_df[lost_df['address'] != \"Not Found\"]\n",
      "## Base Map\n",
      "Washington_map = folium.Map(location=[38.94, -77.03], zoom_start=10)\n",
      "Washington_map\n",
      "## Add Markers\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "found_lost_city_locations.apply(create_map_markers, map_name=Washington_map, axis='columns')\n",
      "Washington_map\n",
      "## Save Map\n",
      "#Washington_map.save(\"Lost-in-the-City-map.html\")\n",
      "## Mapping Systems & Power — Torn Apart / Separados\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). It maps the locations of Immigration and Customs Enforcement (ICE) detention facilities, as featured here http://xpmethod.plaintext.in/torn-apart/volume/1/.\n",
      "## Add a Circle Marker\n",
      "There are a few [different kinds of markers](https://python-visualization.github.io/folium/quickstart.html#Markers) that we can add to a Folium map, including circles. To make a circle, we can call `folium.CircleMarker()` with a particular radius and the option to fill in the circle. You can explore more customization options in the [Folium documentation](https://python-visualization.github.io/folium/modules.html#folium.vector_layers.CircleMarker). We're also going to add a hover `tooltip` in addition to a `popup`.\n",
      "def create_ICE_map_markers(row, map_name):\n",
      "    \n",
      "    folium.CircleMarker(location=[row['lat'], row['lon']], raidus=100, fill=True,\n",
      "                popup=folium.Popup(f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\", max_width=200),\n",
      "                  tooltip=f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\"\n",
      "                 ).add_to(map_name)\n",
      "ICE_df = pd.read_csv(\"../data/ICE-facilities.csv\")\n",
      "ICE_df\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "US_map\n",
      "ICE_df = ICE_df.dropna(subset=['lat', 'lon'])\n",
      "ICE_df.apply(create_ICE_map_markers, map_name=US_map, axis=\"columns\")\n",
      "US_map\n",
      "## Choropleth Maps\n",
      "> Choropleth map = a map where areas are shaded according to a value\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). This data maps the \"cumulative ICE awards since 2014 to contractors by congressional district,\" as featured here http://xpmethod.plaintext.in/torn-apart/volume/2/.\n",
      "To create a chropleth map with Folium, we need to pair a \"geo.json\" file (which indicates which parts of the map to shade) with a CSV file (which includes the variable that we want to shade by).\n",
      "The following data was drawn from [the Torn Apart / Separados project](https://github.com/xpmethod/torn-apart/tree/master/data/districts)\n",
      "US_districts_geo_json = \"../data/ICE_money_districts.geo.json\"\n",
      "US_districts_csv = pd.read_csv(\"../data/ICE_money_districts.csv\")\n",
      "US_districts_csv = US_districts_csv .dropna(subset=['districtName', 'representative'])\n",
      "US_districts_csv\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "\n",
      "folium.Choropleth(\n",
      "    geo_data = US_districts_geo_json,\n",
      "    name = 'choropleth',\n",
      "    data = US_districts_csv,\n",
      "    columns = ['districtName', 'total_awards'],\n",
      "    key_on = 'feature.properties.districtName',\n",
      "    fill_color = 'GnBu',\n",
      "    line_opacity = 0.2,\n",
      "    legend_name= 'Total ICE Money Received'\n",
      ").add_to(US_map)\n",
      "\n",
      "US_map\n",
      "## Add a Tooltip to Choropleth\n",
      "tooltip = folium.features.GeoJson(\n",
      "    US_districts_geo_json,\n",
      "    tooltip=folium.features.GeoJsonTooltip(fields=['representative', 'state', 'party', 'total_value'], localize=True)\n",
      "                                )\n",
      "US_map.add_child(tooltip)\n",
      "US_map\n",
      "## Custom Basemaps with Folium\n",
      "import folium\n",
      "You can add a custom base map by setting the `tiles` parameter to a custom map image and the `attr` parameter to the source of the the map.\n",
      "## *Game of Thrones*\n",
      "https://carto.com/blog/game-of-thrones-basemap/\n",
      "Also setting zoom and lat/lon bounds so the user can't navigate off the map.\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4, min_zoom=4, max_zoom=10,\n",
      "           max_bounds=True,\n",
      "           min_lon=0, max_lon=70, min_lat=-40, max_lat=40,\n",
      "           tiles='https://cartocdn-gusc.global.ssl.fastly.net//ramirocartodb/api/v1/map/named/tpl_756aec63_3adb_48b6_9d14_331c6cbc47cf/all/{z}/{x}/{y}.png',\n",
      "           attr='Textures and Icons from https://www.textures.com/ & https://thenounproject.com/')\n",
      "## Watercolor\n",
      "http://maps.stamen.com/#watercolor/12/37.7706/-122.3782\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=2,\n",
      "           tiles='http://c.tile.stamen.com/watercolor/{z}/{x}/{y}.jpg',\n",
      "           attr='Map tiles by <a href=\"http://stamen.com\">Stamen Design</a>, under <a href=\"http://creativecommons.org/licenses/by/3.0\">CC BY 3.0</a>. Data by <a href=\"http://openstreetmap.org\">OpenStreetMap</a>, under <a href=\"http://creativecommons.org/licenses/by-sa/3.0\">CC BY SA</a>.')\n",
      "## National Geographic\n",
      "https://www.arcgis.com/home/item.html?id=b9b1b422198944fbbd5250b3241691b6\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4,\n",
      "\n",
      "           tiles='http://services.arcgisonline.com/arcgis/rest/services/NatGeo_World_Map/MapServer/tile/{z}/{y}/{x}',\n",
      "           attr=\"Sources: National Geographic, Esri, Garmin, HERE, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, GEBCO, NOAA, INCREMENT P\")\n",
      "!pip install googlemaps\n",
      "# Parameters for geocoding clients\n",
      "# NOTE: Per-second query rates will quickly use up daily API quota.\n",
      "#   Need to be careful not to exceed daily quota\n",
      "\n",
      "import googlemaps\n",
      "gc_rate  =     50 # Geocoding queries per second\n",
      "pl_rate  =      5 # Places queries per second\n",
      "api_key = 'AIzaSyAiMSB2TyVfMxibvWXC-8GhS4VD1pj8egg'\n",
      "gc_client = googlemaps.Client(key=api_key, queries_per_second=gc_rate) # For Geocoding API\n",
      "pl_client = googlemaps.Client(key=api_key, queries_per_second=pl_rate) # For Places API\n",
      "We perform geocoding in two stages. First, we use the Places API to identify the location in question. The Places API is full of Google's best magic; it just *knows* which place you meant. But it doesn't return fully detailed geographic information. So we then use the Geocoding API to look up all the details for the location identified by Places.\n",
      "\n",
      "The glue between these two steps is the `placeid`, a unique identifier for a location that's shared between the two APIs.\n",
      "\n",
      "These APIs return JSON data, which looks a lot like a Python dictionary. The `googlemaps` client parses the incoming JSON into straight dictionaries, which can then be addressed as usual.\n",
      "\n",
      "The functions below extract only part of the full geo data returned by the API. It's enough to give us a feel for the thing, without being overwhelming. But it isn't fundamentally difficult to extract everything.\n",
      "\n",
      "In any case, we look up each unique string in our lookup set, return the full geocode as a dictionary, then save the output to a Pandas dataframe for easier manipulation.\n",
      "def get_placeid(string, api_client):\n",
      "    '''Takes a string and an established googlemaps places API client.\n",
      "       Returns first place_id associated with that string.\n",
      "       If no place_id found, returns \"ZERO_RESULTS\" or None, depending on result status code.'''\n",
      "    try:\n",
      "        place = api_client.places(string)\n",
      "        status = place['status']\n",
      "        if status == 'OK':\n",
      "            place_id = place['results'][0]['place_id']\n",
      "        elif status == 'ZERO_RESULTS':\n",
      "            place_id = None\n",
      "        else:\n",
      "            place_id = None\n",
      "    except:\n",
      "        place_id = None\n",
      "    return place_id\n",
      "def process_id(placeid, api_client):\n",
      "    '''Takes a Google place_id and an established googlemaps geocoding API client.\n",
      "        Looks up and parses geo data for placeid.\n",
      "        Returns int code on error, else dictionary of geo data.\n",
      "    '''\n",
      "    # Define all variables, initial to None\n",
      "    result = {\n",
      "    'formatted_address' : None,\n",
      "    'location_type' : None,\n",
      "    'country' : None,\n",
      "    'admin_1' : None,\n",
      "    'admin_2' : None,\n",
      "    'locality' : None,\n",
      "    'colloquial_area' : None,\n",
      "    'continent' : None,\n",
      "    'natural_feature' : None,\n",
      "    'point_of_interest' : None,\n",
      "    'lat' : None,\n",
      "    'lon' : None,\n",
      "    'partial' : None,\n",
      "    }\n",
      "    # Perform reverse geocode. Note this needs googlemaps v 2.4.3 or higher\n",
      "    try:\n",
      "        data = gc_client.reverse_geocode(placeid)\n",
      "    except:\n",
      "        return 1 # Problem with geocoding API call\n",
      "    \n",
      "    # Use the first result. Should only be one when reverse geocoding with place_id.\n",
      "    try:\n",
      "        data = data[0]\n",
      "        result['formatted_address'] = data['formatted_address']\n",
      "        result['location_type'] = data['types'][0]\n",
      "        result['lat'] = data['geometry']['location']['lat']\n",
      "        result['lon'] = data['geometry']['location']['lng']\n",
      "        try:\n",
      "            result['partial'] = result['partial_match']\n",
      "        except:\n",
      "            result['partial'] = False\n",
      "    except:\n",
      "        print(\"   Bad geocode for place_id %s\" % (placeid))\n",
      "        return 2 # Problem with basic geocode result\n",
      "    \n",
      "    try:\n",
      "        for addr_comp in data['address_components']:\n",
      "            comp_type = addr_comp['types'][0]\n",
      "            if comp_type == 'locality':\n",
      "                result['locality'] = addr_comp['long_name']\n",
      "            elif comp_type == 'country':\n",
      "                result['country'] = addr_comp['long_name']\n",
      "            elif comp_type == 'administrative_area_level_1':\n",
      "                result['admin_1'] = addr_comp['long_name']\n",
      "            elif comp_type == 'administrative_area_level_2':\n",
      "                result['admin_2'] = addr_comp['long_name']\n",
      "            elif comp_type == 'colloquial_area':\n",
      "                result['colloquial_area'] = addr_comp['long_name']\n",
      "            elif comp_type == 'natural_feature':\n",
      "                result['natural_feature'] = addr_comp['long_name']\n",
      "            elif comp_type == 'point_of_interest':\n",
      "                result['point_of_interest'] = addr_comp['long_name']\n",
      "            elif comp_type == 'continent':\n",
      "                result['continent'] = addr_comp['long_name']\n",
      "    except:\n",
      "        return 3 # Problem with address components\n",
      "    \n",
      "    return result\n",
      "Perform the placeid lookups and save results to a dataframe. Time the whole operation, just for reference, using the magic command `%%time` (FYI, `%%time` times the whole cell; use `%time` at the beginning of a line to time just that line).\n",
      "\n",
      "Note that the API can support much faster lookups than what we see here. We've deliberately throttled it (when we set up the API clients above) to avoid going over usage limits.\n",
      "place_id = get_placeid('Mount Vernon Square', pl_client)\n",
      "process_id(place_id, gc_client)\n",
      "process_id('ChIJN9YdOT64t4kRCI5wVsoip3c', gc_client)\n",
      "%%time\n",
      "\n",
      "placeids = {} # Store results in a dictionary keyed to placeid\n",
      "\n",
      "for loc in lookups.index:\n",
      "    plid = get_placeid(loc, pl_client)\n",
      "    placeids[loc] = plid\n",
      "    \n",
      "placeids = pd.DataFrame.from_dict(placeids, orient='index')\n",
      "placeids.columns = ['placeid']\n",
      "placeids.index.set_names('location', inplace=True)\n",
      "If you don't have an API key, hence didn't perform the actual placeid lookups, uncomment and run the line below to read stored data from disk. Note to self: My key is restricted to ND IP addresses.\n",
      "#placeids = pd.read_csv('../Results/placeids.csv', index_col=0)\n",
      "placeids.to_csv('../Results/placeids.csv')\n",
      "placeids.head()\n",
      "Now perform the second geocoding step, using placeids to retrieve detailed geo data. Again, this only works if you have an API key.\n",
      "%%time\n",
      "\n",
      "geodata = {} # Again, store results in a dict keyed to placeid\n",
      "\n",
      "for plid in placeids['placeid']:\n",
      "    if plid:\n",
      "        geodata[plid] = process_id(plid, gc_client)\n",
      "Complete the dataframe and examine the results ...\n",
      "geodata_df = pd.DataFrame.from_dict(geodata)\n",
      "geodata_df = geodata_df.transpose()\n",
      "geodata_df.index.set_names('placeid', inplace=True)\n",
      "geodata_df.head()\n",
      "And uncomment to read stored data if necessary ...\n",
      "#geodata_df = pd.read_csv('../Results/geodata_df.csv', index_col=0)\n",
      "# Write geo data to disk\n",
      "geodata_df.to_csv('../Results/geodata_df.csv')\n",
      "# Put all the data together\n",
      "placeids_geo = placeids.join(geodata_df, on='placeid')\n",
      "placeids_geo.head()\n",
      "Join the full geographic data to the corpus data in order to have everything in one place. Remove places not looked up and get rid of \"Charlotte\" (a notorious NER failure in literary texts).\n",
      "geo_all = geo.join(placeids_geo, on='location')\n",
      "geo_all = geo_all[geo_all.placeid.notnull()]\n",
      "geo_all = geo_all[geo_all['locality']!='Charlotte']\n",
      "print(geo_all.shape)\n",
      "geo_all.head()\n",
      "## Make maps\n",
      "# Setup\n",
      "import cartopy.crs as ccrs\n",
      "import cartopy.feature as cfeature\n",
      "from scipy import stats\n",
      "\n",
      "figDir = '../Results/'\n",
      "# Munge data\n",
      "\n",
      "# Just cities\n",
      "cities = geo_all[(geo_all['location_type'] == 'locality')].groupby('location')\n",
      "\n",
      "cities_tot = [int(i) for i in cities.occurs.sum()]\n",
      "cities_lon = [i for i in cities.lon.max()]\n",
      "cities_lat = [i for i in cities.lat.max()]\n",
      "def bubblemap(lats, lons, sizes, color, name):\n",
      "    import cartopy.crs as ccrs\n",
      "    import cartopy.feature as cfeature\n",
      "    import os\n",
      "\n",
      "    plt.figure(figsize=(12, 6))\n",
      "    ax = plt.axes(projection=ccrs.Robinson()) # projection controls shape on page\n",
      "    \n",
      "    ax.set_extent([-170, 170, -60, 80])\n",
      "    #sizes      = [i/50 for i in sizes]\n",
      "\n",
      "    plt.scatter(lons, lats, s=sizes, linewidths=0,\n",
      "                color=color, marker='o', alpha=0.4, transform=ccrs.Geodetic()) # geodetic = lat/lon\n",
      "\n",
      "    ax.add_feature(cfeature.COASTLINE, linewidth=1.5, alpha=1.0)\n",
      "    ax.add_feature(cfeature.BORDERS, edgecolor='lightgray')\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.savefig(os.path.join(figDir, name)+'.png', dpi=300)\n",
      "bubblemap(cities_lat, cities_lon, cities_tot, 'darkblue', 'map-cities-corpus')\n",
      "cities.occurs.sum().sort_values(ascending=False).head(10)\n",
      "geo_all[geo_all['location']=='Leicester']\n",
      "# Aggregated places by nation\n",
      "nations = geo_all[(geo_all['country'].notnull())].groupby('country')\n",
      "\n",
      "countries_tot = [int(i) for i in nations.occurs.sum()]\n",
      "countries_lon = [i for i in nations.lon.apply(np.mean)]\n",
      "countries_lat = [i for i in nations.lat.apply(np.mean)]\n",
      "# Create labels for country counts (used later)\n",
      "countries_lab = [i for i in nations.country.groups]\n",
      "countries_lab = [str(i) for i in countries_lab]\n",
      "countries_lab = sorted(countries_lab)\n",
      "for i in range(len(countries_lab)):\n",
      "    countries_lab[i] = countries_lab[i] + ': ' + str(countries_tot[i])\n",
      "bubblemap(countries_lat, countries_lon, countries_tot, 'darkred', 'map-nations-corpus')\n",
      "# UK vs US nation-level attention\n",
      "brit = geo_all[(geo_all['nation'] == 'B')].groupby('country')\n",
      "amer = geo_all[(geo_all['nation'] == 'A')].groupby('country')\n",
      "\n",
      "brit_tot = [int(i) for i in brit.occurs.sum()]\n",
      "brit_lon = [i for i in brit.lon.apply(np.mean)]\n",
      "brit_lat = [i for i in brit.lat.apply(np.mean)]\n",
      "\n",
      "amer_tot = [int(i) for i in amer.occurs.sum()]\n",
      "amer_lon = [i for i in amer.lon.apply(np.mean)]\n",
      "amer_lat = [i for i in amer.lat.apply(np.mean)]\n",
      "bubblemap(brit_lat, brit_lon, brit_tot, 'purple', 'map-nations-brit')\n",
      "bubblemap(amer_lat, amer_lon, amer_tot, 'darkgreen', 'map-nations-amer')\n",
      "## Interactive\n",
      "# Note that Folium needs to *serve* the output files, not just open them in a browser\n",
      "#  Start a python web server with \n",
      "#   python3 -m http.server 8080 --bind localhost\n",
      "#  in the figure directory. The load output page in browser like\n",
      "#   http://localhost:8080/map-nations.html\n",
      "\n",
      "import folium\n",
      "\n",
      "map = folium.Map(location=[0, 0], zoom_start=2)\n",
      "marker_size = 1\n",
      "for i in range(len(countries_lab)):\n",
      "    folium.CircleMarker(location=[countries_lat[i], countries_lon[i]],\n",
      "                      radius=np.sqrt(countries_tot[i]*marker_size),\n",
      "                      popup=countries_lab[i]).add_to(map)\n",
      "\n",
      "map.save(os.path.join(figDir, 'map-nations.html'))\n",
      "map\n",
      "We can display this map in a full browser window separate from any notebook by running a lightweight web server from the command line in whatever directory contains the output map HTML file:\n",
      "\n",
      "```\n",
      "python3 -m http.server 8080 --bind localhost\n",
      "```\n",
      "\n",
      "Then we point out browser to [http://localhost:8080/map-nations.html](http://localhost:8080/map-nations.html)\n",
      "\n",
      "Finally, what's up with that dot in the Atlantic Ocean off the northwest coast of Africa?\n",
      "geo_all[geo_all['country']=='Portugal']\n",
      "## Hand review\n",
      "\n",
      "review = geo_all.groupby('location')\n",
      "review.occurs.sum().sort_values(ascending=False).head(20)\n",
      "geo_all[geo_all.location=='Leicester']\n",
      "geo_all[geo_all.location=='Wellington']\n",
      "geo_all[geo_all.country=='Svalbard and Jan Mayen']\n",
      "geo_all.shape\n",
      "strike = [\n",
      "    ['Leicester', 'B-Dickens-Bleak_House-1853-M'],\n",
      "    ['Wellington'],\n",
      "]\n",
      "for i in strike:\n",
      "    if len(i)==1:\n",
      "        geo_all = geo_all[geo_all.location!=i[0]]\n",
      "    else:\n",
      "        geo_all = geo_all[~((geo_all.location==i[0]) & (geo_all.file==i[1]))]\n",
      "geo_all.shape\n",
      "geo_all[geo_all.location=='Leicester']\n",
      "geo_all[geo_all.location=='Wellington']\n",
      "\n",
      "## Publish Your Map on the Web\n",
      "Here are two (free!) options for publishing your Folium map on the web.\n",
      "## Netlify\n",
      "One of the easiest ways to publish your map is to drag and drop your map into [Netlify](https://app.netlify.com/drop).\n",
      "<img src=\"../images/netlify-drag-drop.png\", border=2>  \n",
      "\n",
      "<img src=\"../images/web-map-folder.png\", border=2>  \n",
      "\n",
      "<img src=\"../images/live-site.png\", border=2>  \n",
      "Netlify will create a live site with your map (at a random URL), which will stay online for 24 hours. To make this site permanent and/or change the site URL, you can sign up for a free Netlify account. To change the site's name, go to Settings -> Site Details -> \"Change site name\"\n",
      "<img src=\"../images/change-site-name.png\", border=2>  \n",
      "\n",
      "You can check out a sample map published with Netlify here: https://ithaca-map.netlify.app/\n",
      "## GitHub Pages\n",
      "<a href=\"https://melaniewalsh.github.io/Sample-Map-Website/\", border=2><img src=\"../images/sample-map-website.png\", border=2></a, border=2>\n",
      "https://melaniewalsh.github.io/Sample-Map-Website/\n",
      "You can also publish your map as a simple website with GitHub Pages (as featured in the example above). Below I'm going to walk through the key points of the [GitHub Pages tutorial](https://pages.github.com/) for creating a \"Project site.\"\n",
      "\n",
      "Though you don't need to be a GitHub expert to make a GitHub Pages website, understanding the following premises will be helpful:\n",
      "1) GitHub consists of \"repositories\" where you can store files and code for your projects  \n",
      "2) Git (the backbone of GitHub) is a version-control system that helps you keep track of the changes in your projects. Whereas a platform like Google Docs automatically keeps track of changes that you make in a document, Git needs you to record and track your own changes in a more explicit way. You will need to \"commit\" any changes that you make (e.g., adding your map file) and include a message that describes this change (e.g., \"added a map file\").\n",
      "\n",
      "*For more information, refer back to our course materials [Git & GitHub](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Git-Github.html) or see [GitHub's official guides](https://guides.github.com/).*\n",
      "## Create a New Repository\n",
      "If you don't have one already, [create a GitHub account](https://github.com/join). Then create a \"New repository\".\n",
      "\n",
      "NOTE: Your free GitHub Pages website will be hosted at a URL that combines your GitHub user name and your repository name, like so https://user-name.github.io/repo-name/ So be sure to choose your names wisely!\n",
      "<img src=\"../images/new-repo-name.png\", border=2>  \n",
      "## Choose a GitHub Pages Theme\n",
      "\n",
      "<img src=\"../images/github-pages-theme.png\", border=2>  \n",
      "Once you've selected a theme, you can delete the text that's included in the default \"index.md\" file. Whatever you put in this Markdown (.md) file will be featured on your web site's home page. For now, you can put something simple in the file, add a \"commit\" message (such as \"added website content\"), and then \"Commit changes\" (you can \"Commit directly to the master branch\").\n",
      "<img src=\"../images/sample-home-page.png\", border=2>  \n",
      "\n",
      "NOTE: Markdown is a special markup language, which allows you to format and style text in a very simple way. For example, a hash mark `#` will format your text as a header and a pair of asterisks `**` will format your text in *italics*. To learn more about Markdown and how you can style the home page of your web site, see https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
      "## Upload Map and Logo \n",
      "To upload your map HTML file and (optionally) an image for a web site logo to your GitHub repository, click \"Upload files.\" \n",
      "<img src=\"../images/upload-files.png\", border=2> \n",
      "<img src=\"../images/add-map-and-logo.png\", border=2> \n",
      "## Embed and Link to Map\n",
      "### Embed\n",
      "To embed your map directly into your home page, return to your \"index.md\" file, click the \"Edit\" button (the little pencil icon), and add the following [\"iframe\" code](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe) with your map html file inside:\n",
      "`<iframe src=\"Ithaca-map.html\" height=\"500\" width=\"500\"></iframe>`\n",
      "NOTE: HTML (like the iframe code above) is conveniently compatible with Markdown.\n",
      "### Link\n",
      "To link to your map from your home page, add your HTML file inside a [Markdown-formatted link](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links), as below:\n",
      "`You can explore this map [as its own web page here](Ithaca-map.html)`   \n",
      "👇👇👇  \n",
      "You can explore this map [as its own web page here](Ithaca-map.html)\n",
      "## Customize Map\n",
      "This GitHub Pages site is meant to be minimal (hence the name), but there are a few ways that you can customize it.\n",
      "\n",
      "You can change the title of your website, provide a description, and add a logo by editing the \"_config.yml\" file, as below:\n",
      "```theme: jekyll-theme-minimal\n",
      "title: Sample Map Website\n",
      "description: This website demonstrates how to easily publish and display an interactive map made with Folium. It features some hot spots around Ithaca, NY.\n",
      "logo: Taughannock.jpg\n",
      "**#**Note: You can also add a logo from an image URL, as below\n",
      "**#**logo: https://www.lawschool.cornell.edu/_cs_apps/pt_photo_gallery/uploads/mainphotogallery/fullsizeimage/Photogallery_Ithacapage_8.png```\n",
      "You can read more about configuration options at the [Minimal theme's GitHub page](https://github.com/pages-themes/minimal).\n",
      "## Look at Sample Website\n",
      "You can look at the GitHub repository for this sample website here: https://github.com/melaniewalsh/Sample-Map-Website\n",
      "<a href=\"../images/sample-map-website-diagram.png\", border=2><img src=\"../images/sample-map-website-diagram.png\", border=2></a, border=2>\n",
      "## Custom Basemaps with Folium\n",
      "import folium\n",
      "You can add a custom base map by setting the `tiles` parameter to a custom map image and the `attr` parameter to the source of the the map.\n",
      "## Game of Thrones\n",
      "Also setting zoom and lat/lon bounds so the user can't navigate off the map.\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4, min_zoom=4, max_zoom=10,\n",
      "           max_bounds=True,\n",
      "           min_lon=0, max_lon=70, min_lat=-40, max_lat=40,\n",
      "           tiles='https://cartocdn-gusc.global.ssl.fastly.net//ramirocartodb/api/v1/map/named/tpl_756aec63_3adb_48b6_9d14_331c6cbc47cf/all/{z}/{x}/{y}.png',\n",
      "           attr='Textures and Icons from https://www.textures.com/ & https://thenounproject.com/')\n",
      "## Watercolor\n",
      "http://maps.stamen.com/#watercolor/12/37.7706/-122.3782\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=2,\n",
      "           tiles='http://c.tile.stamen.com/watercolor/{z}/{x}/{y}.jpg',\n",
      "           attr='Map tiles by <a href=\"http://stamen.com\">Stamen Design</a>, under <a href=\"http://creativecommons.org/licenses/by/3.0\">CC BY 3.0</a>. Data by <a href=\"http://openstreetmap.org\">OpenStreetMap</a>, under <a href=\"http://creativecommons.org/licenses/by-sa/3.0\">CC BY SA</a>.')\n",
      "## National Geographic\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4,\n",
      "\n",
      "           tiles='http://services.arcgisonline.com/arcgis/rest/services/NatGeo_World_Map/MapServer/tile/{z}/{y}/{x}',\n",
      "           attr=\"Content may not reflect National Geographic's current map policy. Sources: National Geographic, Esri, Garmin, HERE, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, GEBCO, NOAA, INCREMENT P\")\n",
      "'http://services.arcgisonline.com/arcgis/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}\n",
      "https://basemaps.arcgis.com/v1/arcgis/rest/services/World_Basemap/VectorTileServer/tile/{z}/{y}/{x}\n",
      "width, height = 650, 450\n",
      "\n",
      "m = Map(width=width, height=height, location=[-10, -20], zoom_start=4)\n",
      "\n",
      "add = '/MapServer/tile/{z}/{y}/{x}'\n",
      "ESRI = dict(World_Ocean_Base='http://services.arcgisonline.com/arcgis/rest/services/Ocean/World_Ocean_Base',\n",
      "            World_Navigation_Charts='http://services.arcgisonline.com/ArcGIS/rest/services/Specialty/World_Navigation_Charts',\n",
      "            World_Ocean_Reference='http://services.arcgisonline.com/arcgis/rest/services/Ocean/World_Ocean_Reference',\n",
      "            NatGeo_World_Map='http://services.arcgisonline.com/arcgis/rest/services/NatGeo_World_Map/MapServer',\n",
      "            World_Imagery='http://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer',\n",
      "            World_Physical_Map='http://services.arcgisonline.com/arcgis/rest/services/World_Physical_Map/MapServer',\n",
      "            World_Shaded_Relief='http://services.arcgisonline.com/arcgis/rest/services/World_Shaded_Relief/MapServer',\n",
      "            World_Street_Map='http://services.arcgisonline.com/arcgis/rest/services/World_Street_Map/MapServer',\n",
      "            World_Terrain_Base='http://services.arcgisonline.com/arcgis/rest/services/World_Terrain_Base/MapServer',\n",
      "            World_Topo_Map='http://services.arcgisonline.com/arcgis/rest/services/World_Topo_Map/MapServer')\n",
      "\n",
      "for tile_name, tile_url in ESRI.items():\n",
      "    tile_url += add\n",
      "    folium.LayerControladd_tile_layer(tile_name=tile_name,\n",
      "                     tile_url=tile_url)\n",
      "\n",
      "m.add_layers_to_map()\n",
      "inline_map(m)\n",
      "folium.Map(location=[0, 1],\n",
      "           zoom_start=1000,\n",
      "           max_bounds=True,\n",
      "           tiles='https://www.cayugagenealogy.org/maps/1866tompkins/16307.jpg',\n",
      "           attr='Textures and Icons from https://www.textures.com/ & https://thenounproject.com/')\n",
      "\n",
      "# Mapping\n",
      "In this series of lessons, \n",
      "- Geocoding\n",
      "- Interactive Maps\n",
      "- Publishing Your Map\n",
      "\n",
      "# Mapping\n",
      "In this lesson, we're going to learn how to analyze and visualize geographic data.\n",
      "## Geocoding with GeoPy\n",
      "First, we're going to geocode data — aka get coordinates from addresses or place names — with the Python package [GeoPy](https://geopy.readthedocs.io/en/stable/#). GeoPy makes it easier to use a range of third-party [geocoding API services](https://geopy.readthedocs.io/en/stable/#), such as Google, Bing, ArcGIS, and OpenStreetMap.\n",
      "\n",
      "Though most of these services require an API key, Nominatim, which uses OpenStreetMap data, does not, which is why we're going to use it here.\n",
      "To install GeoPy, run this cell:\n",
      "!pip install geopy\n",
      "From GeoPy's list of possible geocoding services, we're going to import Nominatim:\n",
      "from geopy.geocoders import Nominatim\n",
      "### Nominatim & OpenStreetMap\n",
      "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/256px-Openstreetmap_logo.svg.png\" border=2 >\n",
      "Nominatim (which means \"name\" in Latin) uses [OpenStreetMap data](https://www.openstreetmap.org/relation/174979) to match addresses with geopgraphic coordinates. Though we don't need an API key to use Nominatim, we do need to create a unique [application name](https://operations.osmfoundation.org/policies/nominatim/). \n",
      "Here we're initializing Nominatim as a variable called `geolocator`. Change the application name below to your own application name:\n",
      "geolocator = Nominatim(user_agent=\"YOUR NAME's mapping app\", timeout=2)\n",
      "To geocode an address or location, we simply use the `.geocode()` function:\n",
      "location = geolocator.geocode(\"South Cayuga Street\")\n",
      "location\n",
      "### Google Geocoding API\n",
      "The Google Geocoding API is superior to Nominatim, but it requires an API key and more set up. To enable the Google Geocoding API and get an API key, see [Get Started with Google Maps Platform](https://developers.google.com/maps/gmp-get-started) and [Get Started with Geocoding API](https://developers.google.com/maps/documentation/geocoding/start).\n",
      "#from geopy.geocoders import GoogleV3\n",
      "#google_geolocator = GoogleV3(api_key=\"YOUR-API-KEY HERE\")\n",
      "#google_geolocator.geocode(\"Cayuga Street\")\n",
      "### Get Address\n",
      "print(location.address)\n",
      "### Get Latitude and Longitude\n",
      "print(location.latitude, location.longitude)\n",
      "### Get \"Importance\" Score\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "### Get Class and Type\n",
      "print(f\"Class: {location.raw['class']} \\nType: {location.raw['type']}\")\n",
      "### Get Multiple Possible Matches\n",
      "possible_locations = geolocator.geocode(\"College Ave\", exactly_one=False)\n",
      "\n",
      "for location in possible_locations:\n",
      "    print(location.address)\n",
      "    print(location.latitude, location.longitude)\n",
      "    print(f\"Importance: {location.raw['importance']}\")\n",
      "location = geolocator.geocode(\"College Ave, Ithaca NY\")\n",
      "\n",
      "print(location.address)\n",
      "print(location.latitude, location.longitude)\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "## Geocode with Pandas\n",
      "To geocode every location in a CSV file, we can use Pandas, make a Python function, and `.apply()` it to every row in the CSV file.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "Here we make a function with `geolocator.geocode()` and ask it to return the address, lat/lon, and importance score:\n",
      "def find_location(row):\n",
      "    \n",
      "    place = row['place']\n",
      "    \n",
      "    location = geolocator.geocode(place)\n",
      "    \n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "To start exploring, let's read in a CSV file with a list of places in and around Ithaca.\n",
      "ithaca_df = pd.read_csv(\"../data/ithaca-places.csv\")\n",
      "ithaca_df\n",
      "Now let's `.apply()` our function to this Pandas dataframe and see what results Nominatim's geocoding service spits out.\n",
      "ithaca_df[['address', 'lat', 'lon', 'importance']] = ithaca_df.apply(find_location, axis=\"columns\", result_type=\"expand\")\n",
      "ithaca_df\n",
      "**What do you notice about these results?** ☝️☝️☝️\n",
      "<a href=\"https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc\"><img src=\"https://photos.wikimapia.org/p/00/05/41/92/38_big.jpg\" border=2></a>\n",
      "**[Wharton Studio Inc.](https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc)** (1914-1919) — early 20th-century Ithaca movie studio, located in what is now Stewart Park  \n",
      "*To check out more historical photos of Wharton Studio Inc., see [the Cornell library](https://digital.library.cornell.edu/catalog/ss:550440).*\n",
      "## Mapping (Interactively) with Folium\n",
      "To map our geocoded coordinates, we're going to use the Python library [Folium](https://python-visualization.github.io/folium/). Folium is built on top of the popular JavaScript library [Leaflet](https://leafletjs.com/).\n",
      "To install and import Folium, run the cells below:\n",
      "!pip install folium\n",
      "import folium\n",
      "### Base Map\n",
      "First, we need to establish a base map. This is where we'll map our geocoded Ithaca locations. To do so, we're going to call `folium.Map()`and enter the general latitude/longitude coordinates of the Ithaca area at a particular zoom.\n",
      "\n",
      "(To find latitude/longitude coordintes for a particular location, you can use Google Maps, [as described here](https://support.google.com/maps/answer/18539?co=GENIE.Platform%3DDesktop&hl=en).)\n",
      "ithaca_map = folium.Map(location=[42.44, -76.5], zoom_start=14)\n",
      "ithaca_map\n",
      "### Add a Marker\n",
      "Adding a marker to a map is easy with Folium! We'll simply call `folium.Marker()` at a particular lat/lon, enter some text to display when the marker is clicked on, and then add it to our base map.\n",
      "folium.Marker(location=[42.444695, -76.482233], popup=\"Intro to Cultural Analytics\").add_to(ithaca_map)\n",
      "ithaca_map\n",
      "### Add Markers From Pandas Data\n",
      "To add markers for every location in our Pandas dataframe, we can make a Python function and `.apply()` it to every row in the dataframe.\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "Before we apply this function to our dataframe, we're going to drop any locations that were \"Not Found\" (which would cause `folium.Marker()` to return an error).\n",
      "found_ithaca_locations = ithaca_df[ithaca_df['address'] != \"Not Found\"]\n",
      "found_ithaca_locations.apply(create_map_markers, map_name=ithaca_map, axis='columns')\n",
      "ithaca_map\n",
      "### Save Map\n",
      "ithaca_map.save(\"Ithaca-map.html\")\n",
      "## Mapping Places From Texts — *Lost in the City*\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "from collections import Counter\n",
      "filepath = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "        if named_entity.label_ in [\"GPE\", \"FAC\"]:\n",
      "            places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "lost_df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "lost_df\n",
      "def find_location_with_help(row, helper_location=''):\n",
      "    \n",
      "    place = f\"{row['place']} {helper_location}\"\n",
      "    \n",
      "    location = geolocator.geocode(place, exactly_one=True)\n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "lost_df[['address', 'lat', 'lon', 'importance']] = lost_df.apply(find_location_with_help, helper_location=', Washington, DC', axis=\"columns\", result_type=\"expand\")\n",
      "lost_df\n",
      "found_lost_city_locations = lost_df[lost_df['address'] != \"Not Found\"]\n",
      "### Base Map\n",
      "Washington_map = folium.Map(location=[38.94, -77.03], zoom_start=10)\n",
      "Washington_map\n",
      "### Add Markers\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "found_lost_city_locations.apply(create_map_markers, map_name=Washington_map, axis='columns')\n",
      "Washington_map\n",
      "### Save Map\n",
      "#Washington_map.save(\"Lost-in-the-City-map.html\")\n",
      "## Mapping Systems & Power — Torn Apart / Separados\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). It maps the locations of Immigration and Customs Enforcement (ICE) detention facilities, as featured here http://xpmethod.plaintext.in/torn-apart/volume/1/.\n",
      "### Add a Circle Marker\n",
      "There are a few [different kinds of markers](https://python-visualization.github.io/folium/quickstart.html#Markers) that we can add to a Folium map, including circles. To make a circle, we can call `folium.CircleMarker()` with a particular radius and the option to fill in the circle. You can explore more customization options in the [Folium documentation](https://python-visualization.github.io/folium/modules.html#folium.vector_layers.CircleMarker). We're also going to add a hover `tooltip` in addition to a `popup`.\n",
      "def create_ICE_map_markers(row, map_name):\n",
      "    \n",
      "    folium.CircleMarker(location=[row['lat'], row['lon']], raidus=100, fill=True,\n",
      "                popup=folium.Popup(f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\", max_width=200),\n",
      "                  tooltip=f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\"\n",
      "                 ).add_to(map_name)\n",
      "ICE_df = pd.read_csv(\"../data/ICE-facilities.csv\")\n",
      "ICE_df\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "US_map\n",
      "ICE_df = ICE_df.dropna(subset=['lat', 'lon'])\n",
      "ICE_df.apply(create_ICE_map_markers, map_name=US_map, axis=\"columns\")\n",
      "US_map\n",
      "Intro-Cultural-Analytics/Website-Content/Mapping/Mapping.ipynb\n",
      "## Choropleth Maps\n",
      "> Choropleth map = a map where areas are shaded according to a value\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). This data maps the \"cumulative ICE awards since 2014 to contractors by congressional district,\" as featured here http://xpmethod.plaintext.in/torn-apart/volume/2/.\n",
      "To create a chropleth map with Folium, we need to pair a \"geo.json\" file (which indicates which parts of the map to shade) with a CSV file (which includes the variable that we want to shade by).\n",
      "The following data was drawn from [the Torn Apart / Separados project](https://github.com/xpmethod/torn-apart/tree/master/data/districts)\n",
      "US_districts_geo_json = \"../data/ICE_money_districts.geo.json\"\n",
      "US_districts_csv = pd.read_csv(\"../data/ICE_money_districts.csv\")\n",
      "US_districts_csv = US_districts_csv .dropna(subset=['districtName', 'representative'])\n",
      "US_districts_csv\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "\n",
      "folium.Choropleth(\n",
      "    geo_data = US_districts_geo_json,\n",
      "    name = 'choropleth',\n",
      "    data = US_districts_csv,\n",
      "    columns = ['districtName', 'total_awards'],\n",
      "    key_on = 'feature.properties.districtName',\n",
      "    fill_color = 'GnBu',\n",
      "    line_opacity = 0.2,\n",
      "    legend_name= 'Total ICE Money Received'\n",
      ").add_to(US_map)\n",
      "\n",
      "US_map\n",
      "### Add a Tooltip to Choropleth\n",
      "tooltip = folium.features.GeoJson(\n",
      "    US_districts_geo_json,\n",
      "    tooltip=folium.features.GeoJsonTooltip(fields=['representative', 'state', 'party', 'total_value'], localize=True)\n",
      "                                )\n",
      "US_map.add_child(tooltip)\n",
      "US_map\n",
      "## Publish Your Map on the Web\n",
      "Here are two (free!) options for publishing your Folium map on the web.\n",
      "## Netlify\n",
      "One of the easiest ways to publish your map is to drag and drop your map into [Netlify](https://app.netlify.com/drop).\n",
      "<img src=\"../images/netlify-drag-drop.png\", border=2>  \n",
      "\n",
      "<img src=\"../images/web-map-folder.png\", border=2>  \n",
      "\n",
      "<img src=\"../images/live-site.png\", border=2>  \n",
      "Netlify will create a live site with your map (at a random URL), which will stay online for 24 hours. To make this site permanent and/or change the site URL, you can sign up for a free Netlify account. To change the site's name, go to Settings -> Site Details -> \"Change site name\"\n",
      "<img src=\"../images/change-site-name.png\", border=2>  \n",
      "\n",
      "You can check out a sample map published with Netlify here: https://ithaca-map.netlify.app/\n",
      "## GitHub Pages\n",
      "<a href=\"https://melaniewalsh.github.io/Sample-Map-Website/\", border=2><img src=\"../images/sample-map-website.png\", border=2></a, border=2>\n",
      "https://melaniewalsh.github.io/Sample-Map-Website/\n",
      "You can also publish your map as a simple website with GitHub Pages (as featured in the example above). Below I'm going to walk through the key points of the [GitHub Pages tutorial](https://pages.github.com/) for creating a \"Project site.\"\n",
      "\n",
      "Though you don't need to be a GitHub expert to make a GitHub Pages website, understanding the following premises will be helpful:\n",
      "1) GitHub consists of \"repositories\" where you can store files and code for your projects  \n",
      "2) Git (the backbone of GitHub) is a version-control system that helps you keep track of the changes in your projects. Whereas a platform like Google Docs automatically keeps track of changes that you make in a document, Git needs you to record and track your own changes in a more explicit way. You will need to \"commit\" any changes that you make (e.g., adding your map file) and include a message that describes this change (e.g., \"added a map file\").\n",
      "\n",
      "*For more information, refer back to our course materials [Git & GitHub](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Git-Github.html) or see [GitHub's official guides](https://guides.github.com/).*\n",
      "## Create a New Repository\n",
      "If you don't have one already, [create a GitHub account](https://github.com/join). Then create a \"New repository\".\n",
      "\n",
      "NOTE: Your free GitHub Pages website will be hosted at a URL that combines your GitHub user name and your repository name, like so https://user-name.github.io/repo-name/ So be sure to choose your names wisely!\n",
      "<img src=\"../images/new-repo-name.png\", border=2>  \n",
      "## Choose a GitHub Pages Theme\n",
      "\n",
      "<img src=\"../images/github-pages-theme.png\", border=2>  \n",
      "Once you've selected a theme, you can delete the text that's included in the default \"index.md\" file. Whatever you put in this Markdown (.md) file will be featured on your web site's home page. For now, you can put something simple in the file, add a \"commit\" message (such as \"added website content\"), and then \"Commit changes\" (you can \"Commit directly to the master branch\").\n",
      "<img src=\"../images/sample-home-page.png\", border=2>  \n",
      "\n",
      "NOTE: Markdown is a special markup language, which allows you to format and style text in a very simple way. For example, a hash mark `#` will format your text as a header and a pair of asterisks `**` will format your text in *italics*. To learn more about Markdown and how you can style the home page of your web site, see https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
      "## Upload Map and Logo \n",
      "To upload your map HTML file and (optionally) an image for a web site logo to your GitHub repository, click \"Upload files.\" \n",
      "<img src=\"../images/upload-files.png\", border=2> \n",
      "<img src=\"../images/add-map-and-logo.png\", border=2> \n",
      "## Embed and Link to Map\n",
      "### Embed\n",
      "To embed your map directly into your home page, return to your \"index.md\" file, click the \"Edit\" button (the little pencil icon), and add the following [\"iframe\" code](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe) with your map html file inside:\n",
      "`<iframe src=\"Ithaca-map.html\" height=\"500\" width=\"500\"></iframe>`\n",
      "NOTE: HTML (like the iframe code above) is conveniently compatible with Markdown.\n",
      "### Link\n",
      "To link to your map from your home page, add your HTML file inside a [Markdown-formatted link](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links), as below:\n",
      "`You can explore this map [as its own web page here](Ithaca-map.html)`   \n",
      "👇👇👇  \n",
      "You can explore this map [as its own web page here](Ithaca-map.html)\n",
      "## Customize Map\n",
      "This GitHub Pages site is meant to be minimal (hence the name), but there are a few ways that you can customize it.\n",
      "\n",
      "You can change the title of your website, provide a description, and add a logo by editing the \"_config.yml\" file, as below:\n",
      "```theme: jekyll-theme-minimal\n",
      "title: Sample Map Website\n",
      "description: This website demonstrates how to easily publish and display an interactive map made with Folium. It features some hot spots around Ithaca, NY.\n",
      "logo: Taughannock.jpg\n",
      "**#**Note: You can also add a logo from an image URL, as below\n",
      "**#**logo: https://www.lawschool.cornell.edu/_cs_apps/pt_photo_gallery/uploads/mainphotogallery/fullsizeimage/Photogallery_Ithacapage_8.png```\n",
      "You can read more about configuration options at the [Minimal theme's GitHub page](https://github.com/pages-themes/minimal).\n",
      "## Look at Sample Website\n",
      "You can look at the GitHub repository for this sample website here: https://github.com/melaniewalsh/Sample-Map-Website\n",
      "<a href=\"../images/sample-map-website-diagram.png\", border=2><img src=\"../images/sample-map-website-diagram.png\", border=2></a, border=2>\n",
      "## Custom Basemaps with Folium\n",
      "import folium\n",
      "You can add a custom base map by setting the `tiles` parameter to a custom map image and the `attr` parameter to the source of the the map.\n",
      "## Game of Thrones\n",
      "Also setting zoom and lat/lon bounds so the user can't navigate off the map.\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4, min_zoom=4, max_zoom=10,\n",
      "           max_bounds=True,\n",
      "           min_lon=0, max_lon=70, min_lat=-40, max_lat=40,\n",
      "           tiles='https://cartocdn-gusc.global.ssl.fastly.net//ramirocartodb/api/v1/map/named/tpl_756aec63_3adb_48b6_9d14_331c6cbc47cf/all/{z}/{x}/{y}.png',\n",
      "           attr='Textures and Icons from https://www.textures.com/ & https://thenounproject.com/')\n",
      "## Watercolor\n",
      "http://maps.stamen.com/#watercolor/12/37.7706/-122.3782\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=2,\n",
      "           tiles='http://c.tile.stamen.com/watercolor/{z}/{x}/{y}.jpg',\n",
      "           attr='Map tiles by <a href=\"http://stamen.com\">Stamen Design</a>, under <a href=\"http://creativecommons.org/licenses/by/3.0\">CC BY 3.0</a>. Data by <a href=\"http://openstreetmap.org\">OpenStreetMap</a>, under <a href=\"http://creativecommons.org/licenses/by-sa/3.0\">CC BY SA</a>.')\n",
      "## National Geographic\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4,\n",
      "\n",
      "           tiles='http://services.arcgisonline.com/arcgis/rest/services/NatGeo_World_Map/MapServer/tile/{z}/{y}/{x}',\n",
      "           attr=\"Content may not reflect National Geographic's current map policy. Sources: National Geographic, Esri, Garmin, HERE, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, GEBCO, NOAA, INCREMENT P\")\n",
      "'http://services.arcgisonline.com/arcgis/rest/services/World_Topo_Map/MapServer/tile/{z}/{y}/{x}\n",
      "https://basemaps.arcgis.com/v1/arcgis/rest/services/World_Basemap/VectorTileServer/tile/{z}/{y}/{x}\n",
      "width, height = 650, 450\n",
      "\n",
      "m = Map(width=width, height=height, location=[-10, -20], zoom_start=4)\n",
      "\n",
      "add = '/MapServer/tile/{z}/{y}/{x}'\n",
      "ESRI = dict(World_Ocean_Base='http://services.arcgisonline.com/arcgis/rest/services/Ocean/World_Ocean_Base',\n",
      "            World_Navigation_Charts='http://services.arcgisonline.com/ArcGIS/rest/services/Specialty/World_Navigation_Charts',\n",
      "            World_Ocean_Reference='http://services.arcgisonline.com/arcgis/rest/services/Ocean/World_Ocean_Reference',\n",
      "            NatGeo_World_Map='http://services.arcgisonline.com/arcgis/rest/services/NatGeo_World_Map/MapServer',\n",
      "            World_Imagery='http://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer',\n",
      "            World_Physical_Map='http://services.arcgisonline.com/arcgis/rest/services/World_Physical_Map/MapServer',\n",
      "            World_Shaded_Relief='http://services.arcgisonline.com/arcgis/rest/services/World_Shaded_Relief/MapServer',\n",
      "            World_Street_Map='http://services.arcgisonline.com/arcgis/rest/services/World_Street_Map/MapServer',\n",
      "            World_Terrain_Base='http://services.arcgisonline.com/arcgis/rest/services/World_Terrain_Base/MapServer',\n",
      "            World_Topo_Map='http://services.arcgisonline.com/arcgis/rest/services/World_Topo_Map/MapServer')\n",
      "\n",
      "for tile_name, tile_url in ESRI.items():\n",
      "    tile_url += add\n",
      "    folium.LayerControladd_tile_layer(tile_name=tile_name,\n",
      "                     tile_url=tile_url)\n",
      "\n",
      "m.add_layers_to_map()\n",
      "inline_map(m)\n",
      "folium.Map(location=[0, 1],\n",
      "           zoom_start=1000,\n",
      "           max_bounds=True,\n",
      "           tiles='https://www.cayugagenealogy.org/maps/1866tompkins/16307.jpg',\n",
      "           attr='Textures and Icons from https://www.textures.com/ & https://thenounproject.com/')\n",
      "\n",
      "## Custom Basemaps with Folium\n",
      "import folium\n",
      "You can add a custom base map by setting the `tiles` parameter to a custom map image and the `attr` parameter to the source of the the map.\n",
      "## *Game of Thrones*\n",
      "https://carto.com/blog/game-of-thrones-basemap/\n",
      "Also setting zoom and lat/lon bounds so the user can't navigate off the map.\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4, min_zoom=4, max_zoom=10,\n",
      "           max_bounds=True,\n",
      "           min_lon=0, max_lon=70, min_lat=-40, max_lat=40,\n",
      "           tiles='https://cartocdn-gusc.global.ssl.fastly.net//ramirocartodb/api/v1/map/named/tpl_756aec63_3adb_48b6_9d14_331c6cbc47cf/all/{z}/{x}/{y}.png',\n",
      "           attr='Textures and Icons from https://www.textures.com/ & https://thenounproject.com/')\n",
      "## Watercolor\n",
      "http://maps.stamen.com/#watercolor/12/37.7706/-122.3782\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=2,\n",
      "           tiles='http://c.tile.stamen.com/watercolor/{z}/{x}/{y}.jpg',\n",
      "           attr='Map tiles by <a href=\"http://stamen.com\">Stamen Design</a>, under <a href=\"http://creativecommons.org/licenses/by/3.0\">CC BY 3.0</a>. Data by <a href=\"http://openstreetmap.org\">OpenStreetMap</a>, under <a href=\"http://creativecommons.org/licenses/by-sa/3.0\">CC BY SA</a>.')\n",
      "## National Geographic\n",
      "https://www.arcgis.com/home/item.html?id=b9b1b422198944fbbd5250b3241691b6\n",
      "folium.Map(location=[0, 30],\n",
      "           zoom_start=4,\n",
      "\n",
      "           tiles='http://services.arcgisonline.com/arcgis/rest/services/NatGeo_World_Map/MapServer/tile/{z}/{y}/{x}',\n",
      "           attr=\"Sources: National Geographic, Esri, Garmin, HERE, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, GEBCO, NOAA, INCREMENT P\")\n",
      "# Mapping\n",
      "In this lesson, we're going to learn how to analyze and visualize geographic data.\n",
      "## Geocoding with GeoPy\n",
      "First, we're going to geocode data — aka get coordinates from addresses or place names — with the Python package [GeoPy](https://geopy.readthedocs.io/en/stable/#). GeoPy makes it easier to use a range of third-party [geocoding API services](https://geopy.readthedocs.io/en/stable/#), such as Google, Bing, ArcGIS, and OpenStreetMap.\n",
      "\n",
      "Though most of these services require an API key, Nominatim, which uses OpenStreetMap data, does not, which is why we're going to use it here.\n",
      "To install GeoPy, run this cell:\n",
      "!pip install geopy\n",
      "From GeoPy's list of possible geocoding services, we're going to import Nominatim:\n",
      "from geopy.geocoders import Nominatim\n",
      "### Nominatim & OpenStreetMap\n",
      "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/256px-Openstreetmap_logo.svg.png\" border=2 >\n",
      "Nominatim (which means \"name\" in Latin) uses [OpenStreetMap data](https://www.openstreetmap.org/relation/174979) to match addresses with geopgraphic coordinates. Though we don't need an API key to use Nominatim, we do need to create a unique [application name](https://operations.osmfoundation.org/policies/nominatim/). \n",
      "Here we're initializing Nominatim as a variable called `geolocator`. Change the application name below to your own application name:\n",
      "geolocator = Nominatim(user_agent=\"YOUR NAME's mapping app\", timeout=2)\n",
      "To geocode an address or location, we simply use the `.geocode()` function:\n",
      "location = geolocator.geocode(\"South Cayuga Street\")\n",
      "location\n",
      "### Google Geocoding API\n",
      "The Google Geocoding API is superior to Nominatim, but it requires an API key and more set up. To enable the Google Geocoding API and get an API key, see [Get Started with Google Maps Platform](https://developers.google.com/maps/gmp-get-started) and [Get Started with Geocoding API](https://developers.google.com/maps/documentation/geocoding/start).\n",
      "#from geopy.geocoders import GoogleV3\n",
      "#google_geolocator = GoogleV3(api_key=\"YOUR-API-KEY HERE\")\n",
      "#google_geolocator.geocode(\"Cayuga Street\")\n",
      "### Get Address\n",
      "print(location.address)\n",
      "### Get Latitude and Longitude\n",
      "print(location.latitude, location.longitude)\n",
      "### Get \"Importance\" Score\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "### Get Class and Type\n",
      "print(f\"Class: {location.raw['class']} \\nType: {location.raw['type']}\")\n",
      "### Get Multiple Possible Matches\n",
      "possible_locations = geolocator.geocode(\"College Ave\", exactly_one=False)\n",
      "\n",
      "for location in possible_locations:\n",
      "    print(location.address)\n",
      "    print(location.latitude, location.longitude)\n",
      "    print(f\"Importance: {location.raw['importance']}\")\n",
      "location = geolocator.geocode(\"College Ave, Ithaca NY\")\n",
      "\n",
      "print(location.address)\n",
      "print(location.latitude, location.longitude)\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "## Geocode with Pandas\n",
      "To geocode every location in a CSV file, we can use Pandas, make a Python function, and `.apply()` it to every row in the CSV file.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "Here we make a function with `geolocator.geocode()` and ask it to return the address, lat/lon, and importance score:\n",
      "def find_location(row):\n",
      "    \n",
      "    place = row['place']\n",
      "    \n",
      "    location = geolocator.geocode(place)\n",
      "    \n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "To start exploring, let's read in a CSV file with a list of places in and around Ithaca.\n",
      "ithaca_df = pd.read_csv(\"../data/ithaca-places.csv\")\n",
      "ithaca_df\n",
      "Now let's `.apply()` our function to this Pandas dataframe and see what results Nominatim's geocoding service spits out.\n",
      "ithaca_df[['address', 'lat', 'lon', 'importance']] = ithaca_df.apply(find_location, axis=\"columns\", result_type=\"expand\")\n",
      "ithaca_df\n",
      "**What do you notice about these results?** ☝️☝️☝️\n",
      "<a href=\"https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc\"><img src=\"https://photos.wikimapia.org/p/00/05/41/92/38_big.jpg\" border=2></a>\n",
      "**[Wharton Studio Inc.](https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc)** (1914-1919) — early 20th-century Ithaca movie studio, located in what is now Stewart Park  \n",
      "*To check out more historical photos of Wharton Studio Inc., see [the Cornell library](https://digital.library.cornell.edu/catalog/ss:550440).*\n",
      "## Mapping (Interactively) with Folium\n",
      "To map our geocoded coordinates, we're going to use the Python library [Folium](https://python-visualization.github.io/folium/). Folium is built on top of the popular JavaScript library [Leaflet](https://leafletjs.com/).\n",
      "To install and import Folium, run the cells below:\n",
      "!pip install folium\n",
      "import folium\n",
      "### Base Map\n",
      "First, we need to establish a base map. This is where we'll map our geocoded Ithaca locations. To do so, we're going to call `folium.Map()`and enter the general latitude/longitude coordinates of the Ithaca area at a particular zoom.\n",
      "\n",
      "(To find latitude/longitude coordintes for a particular location, you can use Google Maps, [as described here](https://support.google.com/maps/answer/18539?co=GENIE.Platform%3DDesktop&hl=en).)\n",
      "ithaca_map = folium.Map(location=[42.44, -76.5], zoom_start=14)\n",
      "ithaca_map\n",
      "### Add a Marker\n",
      "Adding a marker to a map is easy with Folium! We'll simply call `folium.Marker()` at a particular lat/lon, enter some text to display when the marker is clicked on, and then add it to our base map.\n",
      "folium.Marker(location=[42.444695, -76.482233], popup=\"Intro to Cultural Analytics\").add_to(ithaca_map)\n",
      "ithaca_map\n",
      "### Add Markers From Pandas Data\n",
      "To add markers for every location in our Pandas dataframe, we can make a Python function and `.apply()` it to every row in the dataframe.\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "Before we apply this function to our dataframe, we're going to drop any locations that were \"Not Found\" (which would cause `folium.Marker()` to return an error).\n",
      "found_ithaca_locations = ithaca_df[ithaca_df['address'] != \"Not Found\"]\n",
      "found_ithaca_locations.apply(create_map_markers, map_name=ithaca_map, axis='columns')\n",
      "ithaca_map\n",
      "### Save Map\n",
      "ithaca_map.save(\"Ithaca-map.html\")\n",
      "## Mapping Places From Texts — *Lost in the City*\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "from collections import Counter\n",
      "filepath = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "        if named_entity.label_ in [\"GPE\", \"FAC\"]:\n",
      "            places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "lost_df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "lost_df\n",
      "def find_location_with_help(row, helper_location=''):\n",
      "    \n",
      "    place = f\"{row['place']} {helper_location}\"\n",
      "    \n",
      "    location = geolocator.geocode(place, exactly_one=True)\n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "lost_df[['address', 'lat', 'lon', 'importance']] = lost_df.apply(find_location_with_help, helper_location=', Washington, DC', axis=\"columns\", result_type=\"expand\")\n",
      "lost_df\n",
      "found_lost_city_locations = lost_df[lost_df['address'] != \"Not Found\"]\n",
      "### Base Map\n",
      "Washington_map = folium.Map(location=[38.94, -77.03], zoom_start=10)\n",
      "Washington_map\n",
      "### Add Markers\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "found_lost_city_locations.apply(create_map_markers, map_name=Washington_map, axis='columns')\n",
      "Washington_map\n",
      "### Save Map\n",
      "#Washington_map.save(\"Lost-in-the-City-map.html\")\n",
      "## Mapping Systems & Power — Torn Apart / Separados\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). It maps the locations of Immigration and Customs Enforcement (ICE) detention facilities, as featured here http://xpmethod.plaintext.in/torn-apart/volume/1/.\n",
      "### Add a Circle Marker\n",
      "There are a few [different kinds of markers](https://python-visualization.github.io/folium/quickstart.html#Markers) that we can add to a Folium map, including circles. To make a circle, we can call `folium.CircleMarker()` with a particular radius and the option to fill in the circle. You can explore more customization options in the [Folium documentation](https://python-visualization.github.io/folium/modules.html#folium.vector_layers.CircleMarker). We're also going to add a hover `tooltip` in addition to a `popup`.\n",
      "def create_ICE_map_markers(row, map_name):\n",
      "    \n",
      "    folium.CircleMarker(location=[row['lat'], row['lon']], raidus=100, fill=True,\n",
      "                popup=folium.Popup(f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\", max_width=200),\n",
      "                  tooltip=f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\"\n",
      "                 ).add_to(map_name)\n",
      "ICE_df = pd.read_csv(\"../data/ICE-facilities.csv\")\n",
      "ICE_df\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "US_map\n",
      "ICE_df = ICE_df.dropna(subset=['lat', 'lon'])\n",
      "ICE_df.apply(create_ICE_map_markers, map_name=US_map, axis=\"columns\")\n",
      "US_map\n",
      "Intro-Cultural-Analytics/Website-Content/Mapping/Mapping.ipynb\n",
      "## Choropleth Maps\n",
      "> Choropleth map = a map where areas are shaded according to a value\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). This data maps the \"cumulative ICE awards since 2014 to contractors by congressional district,\" as featured here http://xpmethod.plaintext.in/torn-apart/volume/2/.\n",
      "To create a chropleth map with Folium, we need to pair a \"geo.json\" file (which indicates which parts of the map to shade) with a CSV file (which includes the variable that we want to shade by).\n",
      "The following data was drawn from [the Torn Apart / Separados project](https://github.com/xpmethod/torn-apart/tree/master/data/districts)\n",
      "US_districts_geo_json = \"../data/ICE_money_districts.geo.json\"\n",
      "US_districts_csv = pd.read_csv(\"../data/ICE_money_districts.csv\")\n",
      "US_districts_csv = US_districts_csv .dropna(subset=['districtName', 'representative'])\n",
      "US_districts_csv\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "\n",
      "folium.Choropleth(\n",
      "    geo_data = US_districts_geo_json,\n",
      "    name = 'choropleth',\n",
      "    data = US_districts_csv,\n",
      "    columns = ['districtName', 'total_awards'],\n",
      "    key_on = 'feature.properties.districtName',\n",
      "    fill_color = 'GnBu',\n",
      "    line_opacity = 0.2,\n",
      "    legend_name= 'Total ICE Money Received'\n",
      ").add_to(US_map)\n",
      "\n",
      "US_map\n",
      "### Add a Tooltip to Choropleth\n",
      "tooltip = folium.features.GeoJson(\n",
      "    US_districts_geo_json,\n",
      "    tooltip=folium.features.GeoJsonTooltip(fields=['representative', 'state', 'party', 'total_value'], localize=True)\n",
      "                                )\n",
      "US_map.add_child(tooltip)\n",
      "US_map\n",
      "# Mapping\n",
      "In this series of lessons, \n",
      "- Geocoding\n",
      "- Interactive Maps\n",
      "- Publishing Your Map\n",
      "\n",
      "## Mapping\n",
      "[Download relevant files](https://melaniewalsh.org/Mapping.zip)\n",
      "In this lesson, we're going to learn how to analyze and visualize geographic data.\n",
      "## Geocoding with GeoPy\n",
      "First, we're going to geocode data — aka get coordinates from addresses or place names — with the Python package [GeoPy](https://geopy.readthedocs.io/en/stable/#). GeoPy makes it easier to use a range of third-party [geocoding API services](https://geopy.readthedocs.io/en/stable/#), such as Google, Bing, ArcGIS, and OpenStreetMap.\n",
      "\n",
      "Though most of these services require an API key, Nominatim, which uses OpenStreetMap data, does not, which is why we're going to use it here.\n",
      "To install GeoPy, run this cell:\n",
      "!pip install geopy\n",
      "From GeoPy's list of possible geocoding services, we're going to import Nominatim:\n",
      "from geopy.geocoders import Nominatim\n",
      "## Nominatim & OpenStreetMap\n",
      "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Openstreetmap_logo.svg/256px-Openstreetmap_logo.svg.png\", border=2>\n",
      "Nominatim (which means \"name\" in Latin) uses [OpenStreetMap data](https://www.openstreetmap.org/relation/174979) to match addresses with geopgraphic coordinates. Though we don't need an API key to use Nominatim, we do need to create a unique [application name](https://operations.osmfoundation.org/policies/nominatim/). \n",
      "Here we're initializing Nominatim as a variable called `geolocator`. Change the application name below to your own application name:\n",
      "geolocator = Nominatim(user_agent=\"YOUR NAME's mapping app\", timeout=2)\n",
      "To geocode an address or location, we simply use the `.geocode()` function:\n",
      "location = geolocator.geocode(\"South Cayuga Street\")\n",
      "location\n",
      "## Google Geocoding API\n",
      "The Google Geocoding API is superior to Nominatim, but it requires an API key and more set up. To enable the Google Geocoding API and get an API key, see [Get Started with Google Maps Platform](https://developers.google.com/maps/gmp-get-started) and [Get Started with Geocoding API](https://developers.google.com/maps/documentation/geocoding/start).\n",
      "#from geopy.geocoders import GoogleV3\n",
      "#google_geolocator = GoogleV3(api_key=\"YOUR-API-KEY HERE\")\n",
      "#google_geolocator.geocode(\"Cayuga Street\")\n",
      "## Get Address\n",
      "print(location.address)\n",
      "## Get Latitude and Longitude\n",
      "print(location.latitude, location.longitude)\n",
      "## Get \"Importance\" Score\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "## Get Class and Type\n",
      "print(f\"Class: {location.raw['class']} \\nType: {location.raw['type']}\")\n",
      "## Get Multiple Possible Matches\n",
      "possible_locations = geolocator.geocode(\"College Ave\", exactly_one=False)\n",
      "\n",
      "for location in possible_locations:\n",
      "    print(location.address)\n",
      "    print(location.latitude, location.longitude)\n",
      "    print(f\"Importance: {location.raw['importance']}\")\n",
      "location = geolocator.geocode(\"College Ave, Ithaca NY\")\n",
      "\n",
      "print(location.address)\n",
      "print(location.latitude, location.longitude)\n",
      "print(f\"Importance: {location.raw['importance']}\")\n",
      "## Geocode with Pandas\n",
      "To geocode every location in a CSV file, we can use Pandas, make a Python function, and `.apply()` it to every row in the CSV file.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "Here we make a function with `geolocator.geocode()` and ask it to return the address, lat/lon, and importance score:\n",
      "def find_location(row):\n",
      "    \n",
      "    place = row['place']\n",
      "    \n",
      "    location = geolocator.geocode(place)\n",
      "    \n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "To start exploring, let's read in a CSV file with a list of places in and around Ithaca.\n",
      "ithaca_df = pd.read_csv(\"../data/ithaca-places.csv\")\n",
      "ithaca_df\n",
      "Now let's `.apply()` our function to this Pandas dataframe and see what results Nominatim's geocoding service spits out.\n",
      "ithaca_df[['address', 'lat', 'lon', 'importance']] = ithaca_df.apply(find_location, axis=\"columns\", result_type=\"expand\")\n",
      "ithaca_df\n",
      "**What do you notice about these results?** ☝️☝️☝️\n",
      "<a href=\"https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc\", border=2><img src=\"https://photos.wikimapia.org/p/00/05/41/92/38_big.jpg\", border=2></a, border=2>\n",
      "**[Wharton Studio Inc.](https://exhibits.library.cornell.edu/biggest-little-fashion-city/feature/wharton-studio-inc)** (1914-1919) — early 20th-century Ithaca movie studio, located in what is now Stewart Park  \n",
      "*To check out more historical photos of Wharton Studio Inc., see [the Cornell library](https://digital.library.cornell.edu/catalog/ss:550440).*\n",
      "## Mapping (Interactively) with Folium\n",
      "To map our geocoded coordinates, we're going to use the Python library [Folium](https://python-visualization.github.io/folium/). Folium is built on top of the popular JavaScript library [Leaflet](https://leafletjs.com/).\n",
      "To install and import Folium, run the cells below:\n",
      "!pip install folium\n",
      "import folium\n",
      "## Base Map\n",
      "First, we need to establish a base map. This is where we'll map our geocoded Ithaca locations. To do so, we're going to call `folium.Map()`and enter the general latitude/longitude coordinates of the Ithaca area at a particular zoom.\n",
      "\n",
      "(To find latitude/longitude coordintes for a particular location, you can use Google Maps, [as described here](https://support.google.com/maps/answer/18539?co=GENIE.Platform%3DDesktop&hl=en).)\n",
      "ithaca_map = folium.Map(location=[42.44, -76.5], zoom_start=14)\n",
      "ithaca_map\n",
      "## Add a Marker\n",
      "Adding a marker to a map is easy with Folium! We'll simply call `folium.Marker()` at a particular lat/lon, enter some text to display when the marker is clicked on, and then add it to our base map.\n",
      "folium.Marker(location=[42.444695, -76.482233], popup=\"Intro to Cultural Analytics\").add_to(ithaca_map)\n",
      "ithaca_map\n",
      "## Add Markers From Pandas Data\n",
      "To add markers for every location in our Pandas dataframe, we can make a Python function and `.apply()` it to every row in the dataframe.\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "Before we apply this function to our dataframe, we're going to drop any locations that were \"Not Found\" (which would cause `folium.Marker()` to return an error).\n",
      "found_ithaca_locations = ithaca_df[ithaca_df['address'] != \"Not Found\"]\n",
      "found_ithaca_locations.apply(create_map_markers, map_name=ithaca_map, axis='columns')\n",
      "ithaca_map\n",
      "## Save Map\n",
      "ithaca_map.save(\"Ithaca-map.html\")\n",
      "## Mapping Places From Texts — *Lost in the City*\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "from collections import Counter\n",
      "filepath = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "        if named_entity.label_ in [\"GPE\", \"FAC\"]:\n",
      "            places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "lost_df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "lost_df\n",
      "def find_location_with_help(row, helper_location=''):\n",
      "    \n",
      "    place = f\"{row['place']} {helper_location}\"\n",
      "    \n",
      "    location = geolocator.geocode(place, exactly_one=True)\n",
      "    if location != None:\n",
      "        return location.address, location.latitude, location.longitude, location.raw['importance']\n",
      "    else:\n",
      "        return \"Not Found\", \"Not Found\", \"Not Found\", \"Not Found\"\n",
      "lost_df[['address', 'lat', 'lon', 'importance']] = lost_df.apply(find_location_with_help, helper_location=', Washington, DC', axis=\"columns\", result_type=\"expand\")\n",
      "lost_df\n",
      "found_lost_city_locations = lost_df[lost_df['address'] != \"Not Found\"]\n",
      "## Base Map\n",
      "Washington_map = folium.Map(location=[38.94, -77.03], zoom_start=10)\n",
      "Washington_map\n",
      "## Add Markers\n",
      "def create_map_markers(row, map_name):\n",
      "    folium.Marker(location=[row['lat'], row['lon']], popup=row['place']).add_to(map_name)\n",
      "found_lost_city_locations.apply(create_map_markers, map_name=Washington_map, axis='columns')\n",
      "Washington_map\n",
      "## Save Map\n",
      "#Washington_map.save(\"Lost-in-the-City-map.html\")\n",
      "## Mapping Systems & Power — Torn Apart / Separados\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). It maps the locations of Immigration and Customs Enforcement (ICE) detention facilities, as featured here http://xpmethod.plaintext.in/torn-apart/volume/1/.\n",
      "## Add a Circle Marker\n",
      "There are a few [different kinds of markers](https://python-visualization.github.io/folium/quickstart.html#Markers) that we can add to a Folium map, including circles. To make a circle, we can call `folium.CircleMarker()` with a particular radius and the option to fill in the circle. You can explore more customization options in the [Folium documentation](https://python-visualization.github.io/folium/modules.html#folium.vector_layers.CircleMarker). We're also going to add a hover `tooltip` in addition to a `popup`.\n",
      "def create_ICE_map_markers(row, map_name):\n",
      "    \n",
      "    folium.CircleMarker(location=[row['lat'], row['lon']], raidus=100, fill=True,\n",
      "                popup=folium.Popup(f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\", max_width=200),\n",
      "                  tooltip=f\"{row['Name'].title()} <br> {row['City'].title()}, {row['State']}\"\n",
      "                 ).add_to(map_name)\n",
      "ICE_df = pd.read_csv(\"../data/ICE-facilities.csv\")\n",
      "ICE_df\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "US_map\n",
      "ICE_df = ICE_df.dropna(subset=['lat', 'lon'])\n",
      "ICE_df.apply(create_ICE_map_markers, map_name=US_map, axis=\"columns\")\n",
      "US_map\n",
      "## Choropleth Maps\n",
      "> Choropleth map = a map where areas are shaded according to a value\n",
      "The data in this section was drawn from [Torn Apart / Separados Project](https://github.com/xpmethod/torn-apart-open-data). This data maps the \"cumulative ICE awards since 2014 to contractors by congressional district,\" as featured here http://xpmethod.plaintext.in/torn-apart/volume/2/.\n",
      "To create a chropleth map with Folium, we need to pair a \"geo.json\" file (which indicates which parts of the map to shade) with a CSV file (which includes the variable that we want to shade by).\n",
      "The following data was drawn from [the Torn Apart / Separados project](https://github.com/xpmethod/torn-apart/tree/master/data/districts)\n",
      "US_districts_geo_json = \"../data/ICE_money_districts.geo.json\"\n",
      "US_districts_csv = pd.read_csv(\"../data/ICE_money_districts.csv\")\n",
      "US_districts_csv = US_districts_csv .dropna(subset=['districtName', 'representative'])\n",
      "US_districts_csv\n",
      "US_map = folium.Map(location=[42, -102], zoom_start=4)\n",
      "\n",
      "folium.Choropleth(\n",
      "    geo_data = US_districts_geo_json,\n",
      "    name = 'choropleth',\n",
      "    data = US_districts_csv,\n",
      "    columns = ['districtName', 'total_awards'],\n",
      "    key_on = 'feature.properties.districtName',\n",
      "    fill_color = 'GnBu',\n",
      "    line_opacity = 0.2,\n",
      "    legend_name= 'Total ICE Money Received'\n",
      ").add_to(US_map)\n",
      "\n",
      "US_map\n",
      "## Add a Tooltip to Choropleth\n",
      "tooltip = folium.features.GeoJson(\n",
      "    US_districts_geo_json,\n",
      "    tooltip=folium.features.GeoJsonTooltip(fields=['representative', 'state', 'party', 'total_value'], localize=True)\n",
      "                                )\n",
      "US_map.add_child(tooltip)\n",
      "US_map\n",
      "## Topic Modeling — Code\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "This notebook is a streamlined version of a previous lesson on **topic modeling**. It is primarily intended for those who want to reuse the code without the previous lesson's overview and explanations.\n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [\"Topic Modeling-Set Up\"](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed.\n",
      "## Set MALLET Path\n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory. If MALLET is located somewhere else, change the directory path:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "## Import Libraries\n",
      "import little_mallet_wrapper\n",
      "import seabornimport glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "## Process Texts\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "## Get Training Data Stats\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "## Training the Topic Model\n",
      "## Set Number of Topics\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "training_data = training_data\n",
      "## Set Topic Model Output Files\n",
      "If you'd like to change this output location, simply change `output_directory_path` below.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/NYT-Obits'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Display Topics and Top Words\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "## Explore Heatmap of Topics and Texts\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "## Display Top Titles Per Topic\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "## Display Topic Words in Context of Original Text\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "## Topic Modeling — Text Files\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model 378 obituaries published by *The New York Times*. \n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Note: \"uncomment\" means delete the initial `#`*\n",
      "*Note: the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "Before we topic model the *NYT* obituaries, we need to process the text files and prepare them for analysis. The steps below demonstrate how to process texts if your corpus is a collection of separate text files. In the next lesson, we'll demonstrate how to process texts that come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "files\n",
      "## Process Texts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual text file, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "```{admonition} Python Review!\n",
      ":class: python_review\n",
      "Take a moment to study this code and reflect about what's happening here. This is a very common Python pattern! We make an empty list `training_data = []`, then we use a `for` loop to iterate through every file path in the list of file paths, then we `open()` and `.read()` each text file associated with that file path, then we processes `little_mallet_wrapper.process_string()` the text, and finally we `.append()` the processed text to our master list.\n",
      "```\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "Here we extract the relevant part of each file name by using [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem), which conveniently extracts just the last part of the file path without the \".txt\" file extension. Because each file name includes the obituary subject's name as well as the year that the subject died, we're going to use this information as a title or label for each obituary.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very simple list comprehension!*\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "obit_titles\n",
      "## Get Training Data Stats\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "According to this little report, we have 378 documents (or obituaries) that average 1345 words in length.\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed obituary texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Topic Model Output Files\n",
      "Finally, we need to tell Little MALLET Wrapper where to find and output all of our topic modeling results. The code below will set Little MALLET Wrapper up to output your results inside a directory called \"topic-model-output\" and a subdirectory called \"NYT-Obits\", all of which will be inside your current directory.\n",
      "\n",
      "If you'd like to change this output location, simply change `output_directory_path` below.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/NYT-Obits'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "Now we import our training data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Finally, we train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 45 seconds to 1 minute to fully train and complete. If you want, you can look at your Terminal or PowerShell while it's running and see what the model looks like as it trains.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "When the topic model finishes, it will output your results to your `output_directory_path`.\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the *NYT* obituaries, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "\n",
      ">*Take a minute to read through every topic. Reflect on what each topic seems to capture as well as how well you think the topics capture the broad themes of the entire collection. Note any oddities, outliers, or inconsistencies.*\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single obituary in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific obituaries.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "If we look at the 32nd topic distribution in this list of `topic_distributions`, which corresponds to Marilyn Monroe's obituary, we will see a list of 15 probabilities. This  list corresponds to the likelihood that each of the 15 topics exists in Marilyn Monroe's obituary.\n",
      "topic_distributions[32]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 0 \"miss film theater movie broadway films\" has a relatively high probability of existing in Marilyn Monroe's obituary `.202` while Topic 5 \"soviet hitler german germany stalin union\" has a relatively low probability `.002`. This seems to comport with what we know about Marilyn Monroe.\n",
      "obituary_to_check = \"1962-Marilyn-Monroe\"\n",
      "\n",
      "obit_number = obit_titles.index(obituary_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {obit_titles[obit_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[obit_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "The darker squares in this heatmap represent a high probability for the corresponding topic (compared to everyone else in the heatmap) and the lighter squares in the heatmap represent a low probability for the corresponding topic. For example, if you scan across the row of Marilyn Monroe, you can see a dark square for the topic \"miss film theater movie theater broadway\". If you scan across the row of Ada Lovelace, an English mathematician who is now recognized as the first computer programmer, according to her [NYT obituary](https://www.nytimes.com/interactive/2018/obituaries/overlooked-ada-lovelace.html), you can see a dark square for \"university professor research science also\".\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the obituaries that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "\n",
      "Because most of the obituaries in our corpus are pretty long, however, it will be more useful for us to simply display the title of each obituary, rather than the entire document—at least as a first step. To do so, we'll first need to make two dictionaries, which will allow us to find the corresponding obituary title and the original text from a given training document.\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "Then we'll make our own function `display_top_titles_per_topic()` that will display the top text titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "**Topic 0 Label**: Hollywood\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Hollywood.\"\n",
      "**Topic 9**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "**Topic 9 Label**: Global Affairs\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Global Affairs.\"\n",
      "**Topic 8**\n",
      "To display the top 7 obituaries with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=8, number_of_documents=7)\n",
      "**Topic 8 Label**: Authors\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Authors.\"\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original obituary texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "**Topic 8**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=8, number_of_documents=3)\n",
      "## Your Turn!\n",
      "Choose a topic from the results above and write down its corresponding topic number below.\n",
      "**Topic: *Your Number Choice Here***\n",
      "**1.** Display the top 6 obituary titles for this topic.\n",
      "#Your Code Here\n",
      "**2.** Display the topic words in the context of the original obituary for these 6 top titles.\n",
      "#Your Code Here\n",
      "**3.** Come up with a label for your topic and write it below:\n",
      "**Topic Label: *Your Label Here***\n",
      "**Reflection**\n",
      "**4.** Why did you label your topic the way you did? What do you think this topic means in the context of all the *NYT* obituaries?\n",
      "**#**Your answer here\n",
      "**5.** What's another collection of texts that you think might be interesting to topic model? Why?\n",
      "**#**Your answer here\n",
      "## Cluster Characters\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "!pip install fuzzywuzzy\n",
      "!pip install python-Levenshtein\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "import en_core_web_sm\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import networkx \n",
      "from networkx.algorithms.components.connected import connected_components, node_connected_component\n",
      "import itertools\n",
      "from fuzzywuzzy import fuzz\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "nlp = en_core_web_sm.load()\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "document = nlp(text)\n",
      "## Get People (More Accurately)\n",
      "Here's a common spaCy NER scenario:\n",
      "\n",
      "You process your text with spaCy, and you find that the model has correctly tagged a person as a \"PERSON.\" Nice! 🎉\n",
      "\n",
      "But then, paragraphs later, you notice that spaCy has tagged the exact same person as a different entity — perhaps an organization (\"ORG\") or a place (\"GPE\"). Ugh 😫 \n",
      "\n",
      "To get a more accurate character/people count, we're going to extract all the named entities that spaCy identified as a \"PERSON\" and then count *any* instance of that entitiy, regardless of its NER label.\n",
      "\n",
      "Additionally, we're going to output this character list to a CSV file, so we can clean and edit the list by hand (if we wish).\n",
      "Extract list of all named entities labeled \"PERSON\":\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "After manual editing, re-upload CSV file for accurate list of people:\n",
      "spacy_identified_people = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "Count any entity that matches a person in our cleaned list of people. Also extract the [index number](https://spacy.io/usage/linguistic-features#named-entities-101) where the person appears in the document.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in spacy_identified_people:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe 's from character name\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        #Get the character index number from the text\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "people_tally = Counter(all_people_matches)\n",
      "character_df = pd.DataFrame(people_tally.most_common())\n",
      "character_df.columns = ['character', 'count']\n",
      "\n",
      "character_df\n",
      "## Cluster Characters By Name Similarity and Distance\n",
      "\n",
      "spaCy doesn't know that \"Betsy Ann Morgan\" and \"Betsy Ann\" should be the same person. So we're also going to pair two character names if they're an extremely close match and they occur within 750 characters of one another.\n",
      "aggregated_people = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                if fuzz.partial_ratio(person[0], another_person[0]) == 100:\n",
      "                    aggregated_people.append((person[0], another_person[0]))\n",
      "G=networkx.Graph()\n",
      "G.add_edges_from(aggregated_people)\n",
      "people_clusters  = list(connected_components(G))\n",
      "people_clusters = [sorted(cluster, key=len, reverse=True) for cluster in people_clusters]\n",
      "people_clusters\n",
      "def add_clustered_characters(row):\n",
      "    character = row\n",
      "    if any(character in cluster for cluster in people_clusters):\n",
      "        for cluster in people_clusters:\n",
      "            if character in cluster:\n",
      "                return \" // \".join(cluster)\n",
      "    else:\n",
      "        return character\n",
      "character_df['clustered_characters'] = character_df['character'].apply(add_clustered_characters)\n",
      "character_df\n",
      "Manually edit\n",
      "#character_df.to_csv('clustered_characters_draft_Lost.csv')\n",
      "#character_df = pd.read_csv('clustered_characters_edited.csv')\n",
      "character_df.groupby('clustered_characters')[['count']].sum().sort_values(by='count', ascending=False).reset_index()\n",
      "## Make a Network of Characters\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 50\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'edge_weight'])\n",
      "character_df['character1']=character_df['character_pair'].str[0]\n",
      "character_df['character2']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['character1', 'character2', 'edge_weight']]\n",
      "character_network[:100]\n",
      "def add_clustered_characters(row):\n",
      "    character = row\n",
      "    if any(character in cluster for cluster in people_clusters):\n",
      "        for cluster in people_clusters:\n",
      "            if character in cluster:\n",
      "                return \" // \".join(cluster)\n",
      "    else:\n",
      "        return character\n",
      "character_network['character1'] = character_network['character1'].apply(add_clustered_characters)\n",
      "character_network['character2'] = character_network['character2'].apply(add_clustered_characters)\n",
      "character_network.to_csv('lost-in-the-city-network.csv', index=False, encoding='utf-8')\n",
      "character_network.groupby(['character1', 'character2'])[['edge_weight']].sum().sort_values(by='edge_weight', ascending=False).reset_index()\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath).read()\n",
      "chunked_text= text.split('\\n')\n",
      "chunked_text[2]\n",
      "number_of_chunks = 5000\n",
      "chunked_text = [text[i:i+number_of_chunks] for i in range(0, len(text), number_of_chunks)]\n",
      "len(chunked_text)\n",
      "\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "## Get People (More Accurately)\n",
      "Extract list of all named entities labeled \"PERSON\":\n",
      "spacy_identified_people = []\n",
      "\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "#pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "After manual editing, re-upload CSV file for accurate list of people:\n",
      "#spacy_identified_people = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "document_length = 0\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for document in chunked_documents:\n",
      "    document_length += len(document.text)\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.text in spacy_identified_people:\n",
      "            person = named_entity.text\n",
      "\n",
      "            #Remove apostrophe 's from character name\n",
      "            person = person.replace(\"’s\", \"\").strip()\n",
      "            #Get the character index number from the text\n",
      "            \n",
      "            person_index =  (document_length - named_entity.start_char)\n",
      "\n",
      "            all_people_matches.append(person)\n",
      "            all_people_matches_plus_ids.append([person, person_index])\n",
      "people_tally = Counter(all_people_matches)\n",
      "character_df = pd.DataFrame(people_tally.most_common())\n",
      "character_df.columns = ['character', 'count']\n",
      "\n",
      "character_df\n",
      "## Make a network!\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 100\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'edge_weight'])\n",
      "character_df['character1']=character_df['character_pair'].str[0]\n",
      "character_df['character2']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['character1', 'character2', 'edge_weight']]\n",
      "character_network[character_network['edge_weight'] > 2]\n",
      "character_network[character_network['edge_weight'] > 2].to_csv('Little-Women-character-network.csv')\n",
      "\n",
      "## Topic Modeling — CSV Files\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model a CSV file with 5,000 Reddit posts from the subreddit [r/AmITheAsshole](https://www.reddit.com/r/AmItheAsshole/). This is an online forum where people share their personal conflicts and ask the community to judge who's the a**hole in the story.\n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Remember that \"uncomment\" means delete the initial `#`*\n",
      "*Remember that the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import the `random` module for generating random numbers; the `pandas` library for reading CSV data (we're also changing its default column width display setting); and  [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import random\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_colwidth\", 500)\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From CSV File\n",
      "Before we topic model the Reddit posts, we need to process the posts and prepare them for analysis. The steps below demonstrate how to process texts if they come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## Reddit — Am I The Asshole?\n",
      "This dataset of Reddit posts is a sample of a larger dataset published by [Elle O'Brien](https://dvc.org/blog/a-public-reddit-dataset) and [Iterative](https://github.com/iterative/aita_dataset). To read in the CSV file, we're going to use Pandas.\n",
      "reddit_df = pd.read_csv(\"../texts/social-media/reddit-aita-sample.csv\")\n",
      "reddit_df.head()\n",
      "reddit_df['body'] = reddit_df['body'].astype(str)\n",
      "### Process Reddit Posts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we're going to process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual post, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "training_data = [little_mallet_wrapper.process_string(text, numbers='remove') for text in reddit_df['body']]\n",
      "original_texts = [text for text in reddit_df['body']]\n",
      "### Process Reddit Post Titles\n",
      "We're also going to extract the file name for each Reddit post.\n",
      "reddit_titles = [title for title in reddit_df['title']]\n",
      "### Get Dataset Statistics\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed Reddit post texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Other MALLET File Paths\n",
      "Then we're going to set a file path where we want all our MALLET topic modeling data to be dumped. I'm going to output everything onto my Desktop inside a folder called \"topic-model-output\" and a subfolder specific to the Reddit posts called \"Reddit.\"\n",
      "\n",
      "All the other necessary variables below `output_directory_path` will be automatically created inside this directory.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/reddit'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "We're going to import the data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Then we're going to train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 1-1.5 minutes to train and complete. If you want it, you can look at your Terminal or PowerShell and see what the model looks like as it's training.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the Reddit posts, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single Reddit post in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific posts.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "topic_distributions[0]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 6 \"family wedding party want birthday would\" has a relatively high probability of existing in the Reddit post \"AITA for not attending holiday gatherings?\" `.124` while Topic 14 \"dog cats dog house would take\" has a relatively low probability `.006`.\n",
      "reddit_post_to_check = \"AITA for not attending holiday gatherings?\"\n",
      "\n",
      "reddit_post_number = reddit_titles.index(reddit_post_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {reddit_titles[reddit_post_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[reddit_post_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = [\"AITA For putting a dog poop bag sidewalk and picking it up on my way home?\",\n",
      " \"AITA for telling my friend that she shouldn't get a tattoo?\",\n",
      " \"AITA for cutting off all contact with my dad?\",\n",
      " \"AITA for being upset/disappointed my bf went and worked out when we had arranged to go to the cinema for a date without letting me know he was going to be late?\",\n",
      " \"WIBTA if I refused to pay for half of the groceries when my boyfriend eats all of the things I get for myself?\",\n",
      " \"WIBTA if I got students from my school to sign a petition to have a teacher 'talked' too?\",\n",
      " \"AITA if I don't invite my aunt to my birthday\"]\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#target_labels = random.sample(reddit_titles, 7)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(reddit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      #output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (25, 8)\n",
      "                                     )\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the Reddit posts and titles that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "training_data_reddit_titles = dict(zip(training_data, reddit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "We'll make our own function `display_top_titles_per_topic()` that will display the top Reddit post titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_reddit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 Reddit post titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "**Topic 9**\n",
      "To display the top 5 Reddit post titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "**Topic 8**\n",
      "To display the top 7 Reddit posts with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=8, number_of_documents=7)\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original Reddit post texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        reddit_title = f\"**{training_data_reddit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(reddit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original Reddit posts with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "**Topic 8**\n",
      "To display the top 3 original Reddit posts with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=14, number_of_documents=3)\n",
      "## Term Frequency–Inverse Document Frequency\n",
      "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "In this lesson, we're going to learn about a text analysis method called **term frequency–inverse document frequency** (tf–idf). This method will help us identify the most unique words in a document from a given corpus. \n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "## Why is tf–idf Useful?\n",
      "\n",
      "## The Basic Math\n",
      "> `term_frequency * inverse_document_frequency`\n",
      "## Breaking Down the Formula\n",
      "\n",
      "> `term_frequency = number of times a given word appears in story or text`\n",
      "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n",
      "\n",
      "> `total_number_of_documents / number_of_documents_with_term`\n",
      "\n",
      "...the result of which we're going to take the logarithm of and then add 1\n",
      "\n",
      "> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n",
      "\n",
      "Do you see how if we flipped the fraction — making it `number_of_documents_with_term /  total_number_of_documents`— that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\"\n",
      "## The Formula in Action\n",
      "**\"said\" vs \"pigeons\"**\n",
      "Using this formula, we're going to calculate and compare the tf–idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*.\n",
      "We need the log() function for our calculation, so we're going to import it from the `math` package.\n",
      "from math import log\n",
      "**\"said\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 13 #number of short stories the contain the word \"said\"\n",
      "term_frequency = 47 #number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**\"pigeons\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 2 #number of short stories the contain the word \"pigeons\"\n",
      "term_frequency = 30 #number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**tf–idf Scores**\n",
      "\n",
      "\"said\" = 50.48<br>\n",
      "\"pigeons\" = 88.38\n",
      "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf–idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them.\n",
      "## tf–idf with scikit-learn\n",
      "## Import Libraries\n",
      "We could continue calculating tf–idf scores in this manner — by doing all the math with Python — but conveniently there's a Python library that can calculate tf–idf scores in just a few lines of code.\n",
      "\n",
      "This library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`.\n",
      "!pip install sklearn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 200)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*.\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_files\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "Path(\"../texts/literature/Lost-in-the-City_Stories/04-Young-Lions.txt\").stem\n",
      "text_titles\n",
      "Let's display them to make sure they're correct:\n",
      "text_files, text_titles\n",
      "## Calculate Word Frequency (Optional Step)\n",
      "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf–idf scores, we can compare these two methods and see how tf–idf helps us find more unique words.\n",
      "\n",
      "(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)\n",
      "#Initialize CountVectorizer with desired parameters\n",
      "count_vectorizer= CountVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\n",
      "word_count_vector = count_vectorizer.fit_transform(text_files)\n",
      "Check the sciki-learn stop words\n",
      "count_vectorizer.get_stop_words()\n",
      "#Make a DataFrame out of the word count vector and sort by title\n",
      "word_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\n",
      "word_count_df = word_count_df.sort_index()\n",
      "\n",
      "#Add column for number of times each word appears in all the documents\n",
      "word_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()\n",
      "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\").\n",
      "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words.\n",
      "word_count_df.sample(10, axis='columns')\n",
      "Let's zoom in on some specific words.\n",
      "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`\n",
      "def get_top_n_counts(dataframe, top_n=10):\n",
      "    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words.\n",
      "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')\n",
      "top_word_freq = get_top_n_counts(word_count_df)\n",
      "top_word_freq\n",
      "## Calculate tf–idf\n",
      "To calculate tf–idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
      "\n",
      "When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
      "### Without Smoothing or Normalization (Not Recommended)\n",
      "Remember how we calculated the tf–idf score for the word \"pigeons\" above?\n",
      "total_number_of_documents = 14 \n",
      "number_of_documents_with_term = 2\n",
      "term_frequency = 30\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "\n",
      "term_frequency * inverse_document_frequency\n",
      "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf–idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn.\n",
      "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. \n",
      "\n",
      "Smoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all.\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Compare Word Frequency and tf–idf Scores\n",
      "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*.\n",
      "First, we're going to merge the top raw word frequency ranks into our top tf–idf dataframe.\n",
      "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')\n",
      "Then we're going to add a column that calculates the change in rank—that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\n",
      "tfidf_compare = tfidf_compare.fillna(\"*new top word*\")\n",
      "Finally, we're going to make some functions that will alter the style of our Pandas dataframe—such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign.\n",
      "tfidf_compare.dtypes\n",
      "type(\"hello\")\n",
      "def make_positive(value):\n",
      "    if value != '*new top word*':\n",
      "        if value > 0:\n",
      "            value = f'+{round(value)}'\n",
      "        else:\n",
      "            value = round(value)\n",
      "    return str(value)\n",
      "\n",
      "def make_bold(value):\n",
      "    return 'font-weight: bold'\n",
      "\n",
      "def color_df(value):\n",
      "    if value == '*new top word*':\n",
      "        color = 'green'    \n",
      "    else:\n",
      "        if value.startswith('-'):\n",
      "            color = 'red'\n",
      "        elif value.startswith('+'):\n",
      "            color = 'green'\n",
      "        else:\n",
      "             color = 'black'        \n",
      "    df_style = f'color: {color}; font-weight: bold'\n",
      "    return df_style\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Now let's display the dataframe and explore which words have become more significant and which words have become less so.\n",
      "tfidf_compare.dtypes\n",
      "for row in tfidf_compare['changed_rank']:\n",
      "    print(row, type(row))\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\n",
      "tfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\n",
      "\n",
      "tfidf_compare_styled\n",
      "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n",
      "\n",
      "*Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below*\n",
      "tfidf_compare[tfidf_compare['word'] == 'said']\n",
      "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer.\n",
      "tfidf_compare[tfidf_compare['word'] == 'pigeons']\n",
      "Words that were not frequent enough to make the top 10 for raw word frequency — such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks — now suddenly show up in the top 10 for tf-idf scores.\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreaming']\n",
      "tfidf_compare[tfidf_compare['word'] == 'gospelteers']\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']\n",
      "## Your Turn!\n",
      "Take a few minutes to explore the dataframe below and then answer the following questions.\n",
      "tfidf_compare\n",
      "**1.** What is the difference between a tf-idf score and raw word frequency?\n",
      "**#** Your answer here\n",
      "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?\n",
      "**#** Your answer here\n",
      "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**#** Your answer here\n",
      "## TF-IDF — Code\n",
      "This notebook is a streamlined version of the previous lesson on **term frequency–inverse document frequency** (tf–idf). It is primarily intended for those who want to reuse the code without the previous lesson's overview and explanations.\n",
      "## Import Libraries\n",
      "To calculate tf-idf scores, we're going to use a Python library called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. \n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 500)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html).\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're using `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "## Calculate tf–idf\n",
      "We need to initialize [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) with our desired parameters. Then we need to plug in the list of text file paths that we want to be calculated with `.fit_transform`.\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "Then we make a dataframe of every word in the collection and its corresponding tf-idf score.\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=10)\n",
      "top_tfidf\n",
      "If you want to change how many top tf-idf scores to show for every text, simply change the `top_n` value.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=20)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "\n",
      "import spacy\n",
      "import re\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import networkx as nx\n",
      "pd.set_option(\"display.max_rows\",1000)\n",
      "pd.set_option(\"display.max_columns\",1000)\n",
      "!python -m spacy download es_core_news_md\n",
      "#get multilingual support for Spanish and English\n",
      "!python -m spacy download xx_ent_wiki_sm\n",
      "!python -m spacy download en_core_web_sm\n",
      "Eliminate blank lines\n",
      "with open('The-House-on-Mango-Street-No-Blank-Lines.txt', 'w') as file_write:\n",
      "    with open('The-House-on-Mango-Street.txt','r') as file_read:\n",
      "        for line in file_read:\n",
      "            if not line.isspace():\n",
      "                file_write.write(line)\n",
      "file = open('The-House-on-Mango-Street.txt', 'r')\n",
      "file.readlines()\n",
      "\n",
      "with open('The-House-on-Mango-Street.txt') as file_object:\n",
      "    file = file_object.read()\n",
      "    file = file.split('\\n')\n",
      "file = [line for line in file if line.strip() != '']\n",
      "with open('The-House-on-Mango-Street-No-Blank-Lines.txt', 'w') as file_object:\n",
      "    for line in file:\n",
      "        file_object.writelines(line)\n",
      "for line in file:\n",
      "    print(line)\n",
      "nlp = spacy.load('en')\n",
      "nlp_mango_model = spacy.load('en-mango')\n",
      "nlp_full_mango_model = spacy.load('en-full-mango')\n",
      "doc = nlp(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "doc_mango = nlp_mango_model(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "doc_full_mango = nlp_full_mango_model(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "from spacy import displacy\n",
      "from spacy.tokenizer import Tokenizer\n",
      "from spacy.lang.en import English\n",
      "nlp = English()\n",
      "tokenizer = Tokenizer(nlp.vocab)\n",
      "tokens = tokenizer(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "tokens\n",
      "displacy.render(doc, style=\"ent\")\n",
      "displacy.render(doc, style=\"ent\")\n",
      "displacy.render(doc_full_mango, style=\"ent\")\n",
      "displacy.render(doc_mango_model, style=\"ent\")\n",
      "displacy.render(doc, style=\"ent\")\n",
      "doc\n",
      "people = [(named_entity.text, named_entity.label_) for named_entity in doc.ents if named_entity.label_ == 'PERSON']\n",
      "for named_entity in doc.ents:\n",
      "    print(named_entity.text, named_entity.label_)\n",
      "people\n",
      "for tokens in doc.ents:\n",
      "    print(tokens.text[:10])\n",
      "\"\"\"Notes to myself\n",
      "\n",
      ".ents retruns entities, which comes with the attributes .text and .label_\"\n",
      "\n",
      "\"\"\"\n",
      "def remove_whitespace_entities(doc):\n",
      "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
      "    return doc\n",
      "nlp.add_pipe(remove_whitespace_entities, after='ner')\n",
      "#count characters and organizations and include label\n",
      "def count_characters(filepath):\n",
      "            #open and read file with spacy\n",
      "            tokens = nlp(open(filepath).read())\n",
      "            #get the file name\n",
      "            filename = os.path.split(filepath)[-1].replace(\".txt\",\"\")\n",
      "            filename = filename.replace(\"-\",\" \")\n",
      "            #get a list of tuples with people/organizations and entity label\n",
      "            people = [(item.text, item.label_) for item in tokens.ents if item.label_ == 'ORG' or item.label_ == 'PERSON']\n",
      "            #clean up the people/organization names by getting rid of plurals, linebreaks, and some punctuation\n",
      "            if len(people) > 0:\n",
      "                people = [((re.sub('([0-9])|’s|’|—|\\.|\\n*', '', person[0])), person[1]) for person in people]\n",
      "                #get rid of people/organizations that begin with lowecase letters unless they start with 'the' or 'and'\n",
      "                people = [(person[0], person[1]) for person in people if re.match('(^[a-z])', person[0]) == None or re.match('^(the)', person[0]) != None or re.match('^(and)', person[0]) != None]\n",
      "                #change people/organizations names to title case\n",
      "                people = [(person[0].title(), person[1]) for person in people]\n",
      "                #count tuples\n",
      "                people_counts = Counter(people)\n",
      "                    # make datalist for pandas dataframe\n",
      "                #datalist = [(filename, people[0], people[1], people_counts) \n",
      "                 #           for ((people[0], people[1]), people_counts) in people_counts.items()]\n",
      "                datalist = [(filename, people_counts[0][0], people_counts[1]) for people_counts in people_counts.items()]\n",
      "                tmp = pd.DataFrame(datalist)\n",
      "                tmp.columns = ['vignette','person','weight'] \n",
      "                return tmp\n",
      "            else:\n",
      "                return \n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "characters_df = pd.DataFrame()\n",
      "for filepath in filepaths:\n",
      "    characters_df = characters_df.append(count_characters(filepath))\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "for filepath in filepaths:\n",
      "    filename = os.path.split(filepath)\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "for filepath in filepaths:\n",
      "    filename = os.path.split(filepath)[-1].replace(\".txt\",\"\").strip()\n",
      "    print(filename)\n",
      "characters_df\n",
      "characters_df = characters_df.replace('I', 'Esperanza')\n",
      "characters_df = characters_df.replace('Meme', 'Meme Ortiz')\n",
      "characters_df_by_match.groupby(['person']).sum().sort_values(by='weight', ascending=False).to_csv('Mango-Characters-2.csv')\n",
      "characters_df.groupby(['person']).sum().sort_values(by='weight', ascending=False)\n",
      "characters_df_by_match.groupby(['person', 'vignette']).sum().sort_values(by=\"weight\", ascending=False).reset_index()\n",
      "characters_df.groupby(['person', 'vignette']).sum().sort_values(by=\"weight\", ascending=False).reset_index()\n",
      "G = nx.from_pandas_edgelist(characters_df_by_match, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df_by_match['person'], bimodal='character', category='category')\n",
      "G.add_nodes_from(characters_df_by_match['vignette'], bimodal='vignette')\n",
      "#import matplotlib as plt\n",
      "# fig, ax = plt.subplots(1, 1, figsize=(8, 6));\n",
      "# nx.draw_networkx(G, ax=ax)\n",
      "nx.write_gexf(G, '2020-by-match-mango-street-character-network.gexf')\n",
      "G = nx.from_pandas_edgelist(characters_df, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df['person'], bimodal='character', category='category')\n",
      "G.add_nodes_from(characters_df['vignette'], bimodal='vignette')\n",
      "#import matplotlib as plt\n",
      "# fig, ax = plt.subplots(1, 1, figsize=(8, 6));\n",
      "# nx.draw_networkx(G, ax=ax)\n",
      "nx.write_gexf(G, '2020-mango-street-character-network.gexf')\n",
      "mango = pd.read_csv('Mango-Characters.csv', delimiter='\\t')\n",
      "mango2 = pd.read_csv('Mango-Characters-2.csv', delimiter='\\t')\n",
      "tokens = nlp(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "    #get the file name\n",
      "sample = '12 fjakdlaf;j'\n",
      "re.match('(^[0-9]+)', sample).group()\n",
      "#count characters and organizations and include label\n",
      "def count_characters_by_match(filepath):\n",
      "    #open and read file with spacy\n",
      "    tokens = nlp(open(filepath).read())\n",
      "    #get the file name\n",
      "    filename = os.path.split(filepath)[-1].replace(\".txt\",\"\")\n",
      "    filename = filename.replace(\"-\",\" \")\n",
      "    story_number = re.match('(^[0-9]+)', filename).group()\n",
      "    filename = re.sub('(^[0-9]+)', '', filename)\n",
      "    filename = f'{filename} ({story_number})'\n",
      "    #get a list of tuples with people/organizations and entity label\n",
      "    character_list = [person for person in mango2['person']]\n",
      "    character_list.append('I')\n",
      "    character_list.append('me')\n",
      "    people = [item.text for item in tokens if item.text in character_list]\n",
      "    #clean up the people/organization names by getting rid of plurals, linebreaks, and some punctuation\n",
      "    if len(people) > 0:\n",
      "        people_counts = Counter(people)\n",
      "\n",
      "                # make datalist for pandas dataframe\n",
      "            #datalist = [(filename, people[0], people[1], people_counts) \n",
      "                     #           for ((people[0], people[1]), people_counts) in people_counts.items()]\n",
      "        datalist = [(filename, people_counts[0], people_counts[1], story_number) for people_counts in people_counts.items()]\n",
      "        tmp = pd.DataFrame(datalist)\n",
      "        tmp.columns = ['vignette','person','weight', 'story_number'] \n",
      "        return tmp\n",
      "    else:\n",
      "        return\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "characters_df_by_match = pd.DataFrame()\n",
      "for filepath in filepaths:\n",
      "    characters_df_by_match = characters_df_by_match.append(count_characters_by_match(filepath))\n",
      "characters_df_by_match = characters_df_by_match.replace('I', 'Esperanza')\n",
      "characters_df_by_match = characters_df_by_match.replace('me', 'Esperanza')\n",
      "characters_df_by_match.sample(40)\n",
      "G = nx.from_pandas_edgelist(characters_df_by_match, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df_by_match['person'], bimodal='character')\n",
      "G.add_nodes_from(characters_df_by_match['vignette'], bimodal='vignette')\n",
      "#nx.set_node_attributes(G, pd.Series(nodes.story_number, index=nodes.node).to_dict(), 'story_number')\n",
      "G.remove_node('People')\n",
      "#import matplotlib as plt\n",
      "# fig, ax = plt.subplots(1, 1, figsize=(8, 6));\n",
      "# nx.draw_networkx(G, ax=ax)\n",
      "nx.write_gexf(G, '2020-by-match-mango-street-character-network.gexf')\n",
      "\n",
      "\n",
      "for item in count.items():\n",
      "    print(item[1])\n",
      "characters_df_by_match\n",
      "\n",
      "character_list = [person for person in mango['person']]\n",
      "\n",
      "character_list.append('I')\n",
      "\n",
      "character_list.append('me')\n",
      "characters_df_by_match.to_csv('Mango-Character-By-Match.csv')\n",
      "\n",
      "# Topic Modeling — Time Series\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\" border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model a CSV file of Donald Trump's tweets and plot the fluctuation of topics over time.\n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Remember that \"uncomment\" means delete the initial `#`*\n",
      "*Remember that the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import the `random` module for generating random numbers; the `pandas` library for reading CSV data (we're also changing its default column width display setting); and  [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import random\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_columns\", 50)\n",
      "pd.set_option(\"max_colwidth\", 200)\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From CSV File\n",
      "Before we topic model Donald Trump's tweets, we need to process the tweets and prepare them for analysis. The steps below demonstrate how to process texts if they come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## Trump Tweets\n",
      "This dataset of Donald Trump's tweets is taken from [Trump Twitter Archive](http://www.trumptwitterarchive.com/). To read in the CSV file, we're going to use Pandas.\n",
      "trump_df = pd.read_csv(\"../texts/politics/Trump-Tweets.csv\", encoding='utf-8')\n",
      "trump_df.head()\n",
      "trump_df['text'] = trump_df['text'].astype(str)\n",
      "### Process Trump Tweets\n",
      "training_data = [little_mallet_wrapper.process_string(text, numbers='remove') for text in trump_df['text']]\n",
      "### Get Original Trump Tweets\n",
      "original_trump_tweets = [title for title in trump_df['text']]\n",
      "### Get Dataset Statistics\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 35\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed Reddit post texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Other MALLET File Paths\n",
      "Then we're going to set a file path where we want all our MALLET topic modeling data to be dumped. I'm going to output everything onto my Desktop inside a folder called \"topic-model-output\" and a subfolder specific to the Reddit posts called \"Reddit.\"\n",
      "\n",
      "All the other necessary variables below `output_directory_path` will be automatically created inside this directory.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/Trump-Tweets'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "We're going to import the data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Then we're going to train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 1-1.5 minutes to train and complete. If you want it, you can look at your Terminal or PowerShell and see what the model looks like as it's training.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Display Topics and Top Words\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Display Top Tweets For Topic\n",
      "## Load Topic Distributions\n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "tweet_dict = dict(zip(training_data, original_trump_tweets))\n",
      "def display_top_tweets_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), tweet_dict[document] + \"\\n\")\n",
      "    return\n",
      "display_top_tweets_per_topic(topic_number=0, number_of_documents=5)\n",
      "## Show Topic Words in Context of Full Tweet\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "\n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        original_text = tweet_dict[document]\n",
      "        original_text_lowered = original_text.lower()\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text_lowered:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word.upper()}**\", original_text, flags=re.I)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(original_text))\n",
      "    return\n",
      "display_bolded_topic_words_in_context(topic_number=0, number_of_documents=4)\n",
      "## Plot Topics Over Time\n",
      "## Load Topic Distributions\n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "## Add Topic Distribution Columns\n",
      "Add column with all topic distributions for every tweet\n",
      "trump_df['topic_distributions'] = pd.Series(topic_distributions)\n",
      "Make a separate dataframe with each topic distribution as a separate column\n",
      "topic_distributions_df = trump_df['topic_distributions'].apply(pd.Series)\n",
      "Rename each of those columns with the first four words from the topic\n",
      "topic_distributions_df.columns = [\" \".join(topic[:4]) for topic in topics]\n",
      "Merge that column into the dataframe\n",
      "trump_df = pd.concat([trump_df, topic_distributions_df], axis=1)\n",
      "## Date Formatting For Time Series Plot\n",
      "Convert to datetime\n",
      "trump_df['date'] = pd.to_datetime(trump_df['created_at'])\n",
      "Extract year\n",
      "trump_df['year'] = pd.to_datetime(trump_df['date'].dt.year, format='%Y')\n",
      "Extract year and month\n",
      "trump_df['year-month'] = trump_df['date'].dt.to_period('M')\n",
      "trump_df['Date (by month)'] = [month.to_timestamp() for month in trump_df['year-month']]\n",
      "Set year and month as index\n",
      "trump_df = trump_df.set_index('Date (by month)')\n",
      "## Plot All Topics as Times Series\n",
      "#Ignore warning about opening too many plots\n",
      "import matplotlib as plt\n",
      "plt.rcParams.update({'figure.max_open_warning': 0})\n",
      "\n",
      "for number in range(0,33):\n",
      "    topic_number = number\n",
      "    topic_label = \" \".join(topics[topic_number][:4])\n",
      "    trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title=f'Trump Tweets \\n Topic {topic_number}: {topic_label}\\n ', linewidth=2)\n",
      "## Plot Individual Topics as Time Series\n",
      "**Topic 10**: Hillary Clinton\n",
      "For every Trump tweet, plot the probability of Topic 10 \"hillary crooked clinton court\"\n",
      "topic_number = 10\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df[topic_label].plot(style='.', title='Trump Tweets By Topic')\n",
      "For every month of Trump tweets, plot the average probability of Topic 10 \"hillary crooked clinton court\"\n",
      "topic_number = 10\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 18** Fake News\n",
      "For every month of Trump tweets, plot the average probability of Topic 18 \"news fake media story\"\n",
      "topic_number = 18\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 9** Border Wall\n",
      "For every month of Trump tweets, plot the average probability of Topic 9 \"border wall security country\"\n",
      "topic_number = 9\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 19** Make America Great Again\n",
      "For every month of Trump tweets, plot the average probability of Topic 19 \"great america make https\"\n",
      "topic_number = 19\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 13** The Apprentice\n",
      "For every month of Trump tweets, plot the average probability of Topic 13 \"last night celebapprentice http\"\n",
      "topic_number = 13\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "## Topic Modeling — Overview\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In the next lessons, we're going to learn about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "When we calculated term frequency-inverse document frequency (tf-idf) scores, we identified individual words that were statistically meaningful for certain documents (i.e., they were more likely to show up in certain documents rather than in other ones). When we topic model, we're going to identify *clusters of words* that show up together in statistically meaningful ways throughout the corpus.\n",
      "## Why Are Topic Models Useful?\n",
      "Topic models are useful for understanding collections of texts in their broadest outlines and themes. If you wanted to get a sense of the primary subjects discussed in thousands of journal articles published over multiple decades, then topic modeling might be a good choice. Topic models can also be helpful for looking at the fluctuation of topics and themes over time (this is a time series approach that we will discuss in the next lesson) or finding clusters of texts that contain the same or similar topics. \n",
      "In these lessons, we will train a topic model on a collection of 378 obituaries published by *The New York Times*. As we'll find out, the topics—or clusters of words—that emerge from this analysis are broadly related to art, literature, sports, the military, and politics, among other subjects. These results are pretty satisfying! They seem to capture what the obituaries are \"about.\" Frida Kahlo's obituary contains a significant discussion of her art, while Nella Larsen's obituary discusses her novels, Jackie Robinson's obituary discusses his role in sports, and Ulysses S. Grant's obituary discusses his military career. How does the topic model \"know\" or \"discover\" what these *NYT* obituaries are about?\n",
      "## How LDA Topic Models Work\n",
      "There are numerous kinds of topic models, but the most popular and widely-used kind is latent Dirichlet allocation (LDA). It's so popular, in fact, that \"LDA\" and \"topic model\" are sometimes used interchangeably, even though LDA is only one type.\n",
      "\n",
      "LDA math is pretty complicated. We're not going to get very deep into the math here. But we are going to introduce two important concepts that will help us conceptually understand how LDA topic models work.\n",
      "## 1) LDA is an Unsupervised Algorithm\n",
      "\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "\n",
      "## Topics and Labels\n",
      "What we call a \"topic\" is really just a list of the most probable words for that topic, which are sorted in descending order of probability. The most probable word for the topic is the first word. Here are some sample topics from a previous run on the *NYT* obituaries:\n",
      "\n",
      "✨Topic 3✨\n",
      "\n",
      "['book', 'new', 'wrote', 'work', 'published', 'art', 'years', 'writing', 'world', 'writer', 'books', 'novel', 'paris', 'life', 'story']\n",
      "\n",
      "✨Topic 6✨\n",
      "\n",
      "['president', 'state', 'court', 'roosevelt', 'justice', 'house', 'years', 'law', 'party', 'political', 'republican', 'senator', 'governor', 'democratic', 'campaign']\n",
      "\n",
      "✨Topic 10✨\n",
      "\n",
      "['miss', 'film', 'years', 'theater', 'broadway', 'movie', 'films', 'hollywood', 'stage', 'movies', 'actor', 'new', 'director', 'york', 'show']\n",
      "\n",
      "Topic models start to get more powerful when we, as human researchers, analyze the most probable words for every topic and summarize what these words have in common. This summary can then be used as a descriptive label for the topic. Remember, since an LDA topic model is an unsupervised algorithm, it doesn't know what these words mean in relationship to one another. It's up to us, as the human researchers, to make meaning out of the topics.\n",
      "\n",
      "For example, we might label the topics as follows:\n",
      "\n",
      "✨Topic 3: **Literature**✨\n",
      "\n",
      "['book', 'new', 'wrote', 'work', 'published', 'art', 'years', 'writing', 'world', 'writer', 'books', 'novel', 'paris', 'life', 'story']\n",
      "\n",
      "✨Topic 6: **Politics**✨\n",
      "\n",
      "['president', 'state', 'court', 'roosevelt', 'justice', 'house', 'years', 'law', 'party', 'political', 'republican', 'senator', 'governor', 'democratic', 'campaign']\n",
      "\n",
      "✨Topic 10: **Hollywood**✨\n",
      "\n",
      "['miss', 'film', 'years', 'theater', 'broadway', 'movie', 'films', 'hollywood', 'stage', 'movies', 'actor', 'new', 'director', 'york', 'show']\n",
      "\n",
      "However, even when the topics are relatively straightforward, as these topics seem to be, they're still open to interpretation. For instance, we could just as easily label Topic 3 \"Writing & Art,\" Topic 6 \"Government,\" and Topic 10 \"Film & Theater.\" These subtle changes might shape the direction of our analysis and eventual argument in substantial ways. Topics can be far more ambiguous than the above examples, as well, which makes the business of interpretation even more significant. We'll discuss the ambiguity of topics and topic labels in more depth in the next lessons.\n",
      "\n",
      "Press `Tab` to autocomplete file path\n",
      "pd.read_csv(\"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Text-Analysis/\")\n",
      "^ The tilde `~` is an alias for your home folder\n",
      "import pandas as pd\n",
      "cmv_data = pd.read_json(\"/Users/melaniewalsh/Downloads/cmv_20161111.jsonlist\", lines=True)\n",
      "cmv_data.head()\n",
      "!pip install convokit\n",
      "import convokit\n",
      "convokit.download(\"subreddit-changemyview\")\n",
      "from convokit import Corpus, download\n",
      "corpus = Corpus(filename=download(\"subreddit-Cornell\"))\n",
      "convokit.download(\"subreddit-Cornell\", data_dir = \"/Users/melaniewalsh/Downloads/\")\n",
      "convokit.download(\"subreddit-AskReddit\", data_dir = \"/Users/melaniewalsh/Downloads/\")\n",
      "\n",
      "## Named Entity Recognition\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "In this lesson, we're going to learn about a text analysis method called **Named Entity Recognition** (NER). This method will help us computationally identify people, places, and things (of various kinds) in a text or collection of texts.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\" width=\"100%\", border=2>\n",
      "## Why is NER Useful?\n",
      "Named Entity Recognition is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters (something we'll do in a later lesson!). Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping the locations (something we'll also do in a later lesson!).\n",
      "## Natural Language Processing (NLP)\n",
      "Named Entity Recognition is a fundamental task in the field of *natural language processing* (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called *spellcheck*? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!\n",
      "\n",
      "Thanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts are now getting eerily good. (If you don't believe me, check out [this app that will autocomplete your sentences](https://transformer.huggingface.co/doc/gpt2-large/qCNMTfzephfZMBkryTNvSRKQ/edit) with GPT-2, a state-of-the-art text generation model. When I ran it, the model generated a mini-lecture from a \"university professor\" that sounds spookily close to home...)\n",
      "<img src=\"../images/GPT-2.png\", border=2>\n",
      "Open-source NLP tools are getting very good, too. We're going to use one of these open-source tools, the Python library `spaCy`, for our Named Entity Recognition tasks in this lesson.\n",
      "## How spaCy Works\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "The screenshot above shows spaCy correctly identifying named entities in Ada Lovelace's *New York Times* obituary (something that we'll test out for ourselves below). How does spaCy know that \"Ada Lovelace\" is a person and that \"1843\" is a date?\n",
      "Well, spaCy doesn't *know*, not for sure anyway. Instead, spaCy is making a very educated guess. This \"guess\" is based on what spaCy has learned about the English language after seeing lots of other examples.\n",
      "That's a colloquial way of saying: spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand). This is similar to our <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html#1)-LDA-is-an-Unsupervised-Algorithm\">topic modeling work</a> from the previous lesson, except our topic model wasn't using labeled data.\n",
      "\n",
      "The English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n",
      "\n",
      "When spaCy identifies people and places in Ada Lovelace's obituary, in other words, the NLP model is actually making *predictions* about the text based on what it has learned about how people and places function in English-language sentences.\n",
      "## NER with spaCy\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things, and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it. There are two ways to load a spaCy language model.\n",
      "**1.** We can import it as a module and then load it from the module, as below:\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "**2.** Or we can load it by name:\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "If you just downloaded the model for the first time, it's advisable to use Option 1. Then you can use the model immediately. Otherwise, you'll likely need to restart your Jupyter kernel (which you can do by clicking Kernel -> Restart Kernel.. in the Jupyter Lab menu).\n",
      "## Create a Processed spaCy Document\n",
      "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
      "\n",
      "In the cell below, we `open()` and `.read()` Ada Lovelace's obituary. Then we run`nlp()` on the `text` and create our `document`.\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "## spaCy Named Entities\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "Above is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels. To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):\n",
      "displacy.render(document, style=\"ent\")\n",
      "From a quick glance at the text above, we can see that spaCy is doing quite well with NER. But it's definitely not perfect.\n",
      "\n",
      "Though spaCy correctly identifies \"Ada Lovelace\" as a `PERSON` in the first sentence, just a few sentences later it labels her as a `WORK_OF_ART`. Though spaCy correctly identifies \"London\" as a place `GPE` a few paragraphs down, it incorrectly identifies \"Jacquard\" as a place `GPE`, too (when really \"Jacquard\" is a type of loom, named after [Marie Jacquard](https://en.wikipedia.org/wiki/Jacquard_machine)). \n",
      "\n",
      "This inconsistency is very important to note and keep in mind. If we wanted to use spaCy's NER for a project, it would almost certainly require manual correction and cleaning. And even then it wouldn't be perfect. That's why understanding the limitations of this tool is so crucial. While spaCy's NER can be very good for identifying entities in broad strokes, it can't be relied upon for anything exact and fine-grained — not out of the box anyway.\n",
      "## Get Named Entities\n",
      "All the named entities in our `document` can be found in the `document.ents` property. If we check out `document.ents`, we can see all the entities from Ada Lovelace's obituary.\n",
      "document.ents\n",
      "Each of the named entities in `document.ents` contains [more information about itself](https://spacy.io/usage/linguistic-features#accessing), which we can access by iterating through the `document.ents` with a simple `for` loop. `For` each `named_entity` in `document.ents`, we will extract the `named_entity` and its corresponding `named_entity.label_`.\n",
      "for named_entity in document.ents:\n",
      "    print(named_entity, named_entity.label_)\n",
      "To extract just the named entities that have been identified as `PERSON`, we can add a simple `if` statement into the mix:\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        print(named_entity)\n",
      "## Practicing with *Lost in the City*\n",
      "For the rest of this lesson, we're going to work with Edward P. Jones's short story collection *Lost in the City*, specifically the first story, \"The Girl Who Raised Pigeons.\"\n",
      "filepath = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "text = open(filepath, encoding=\"utf-8\").read()\n",
      "document = nlp(text)\n",
      "## Get People\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "To extract and count the people identified in \"The Girl Who Raised Pigeons,\" we will follow the same model as above, using an `if` statement that will pull out words only if their \"ent\" label matches \"PERSON.\"\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">*While we demonstrate how to extract named entities in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.* \n",
      "Here's the code all together:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "        \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "Here's the code broken up. We make a list of all the people identified in *Lost in the City*:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "people\n",
      "Then we count the unique people in this list with the `Counter()` module:\n",
      "people_tally = Counter(people)\n",
      "people_tally.most_common()\n",
      "Then we make a dataframe from this list with `pd.DataFrame()`:\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "To write this dataframe (or any dataframe!) to a CSV file, we can use `df.to_csv()`. To create a CSV file of character counts, uncomment the cell below:\n",
      "#df.to_csv(\"Lost-in-the-City-characters.csv\", encoding='utf-8', index=False)\n",
      "## Get Places\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "To extract and count places, we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"GPE\" or \"LOC.\" These are the type labels for \"counties cities, states\" and \"locations, mountain ranges, bodies of water.\"\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"GPE\" or named_entity.label_ == \"LOC\":\n",
      "        places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "## Get Streets & Parks\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "To extract and count streets and parks (which show up a lot in *Lost in the City*!), we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"FAC.\" This is the type label for \"buildings, airports, highways, bridges, etc.\"\n",
      "streets = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"FAC\":\n",
      "        streets.append(named_entity.text)\n",
      "\n",
      "streets_tally = Counter(streets)\n",
      "\n",
      "df = pd.DataFrame(streets_tally.most_common(), columns = ['street', 'count'])\n",
      "df\n",
      "## Get Works of Art\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the \"ent\" label \"WORK_OF_ART\").\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of named entities in a single line of code! Closely examine the first line of code below:\n",
      "works_of_art = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"WORK_OF_ART\"]\n",
      "\n",
      "art_tally = Counter(works_of_art)\n",
      "\n",
      "df = pd.DataFrame(art_tally.most_common(), columns = ['work_of_art', 'count'])\n",
      "df\n",
      "## Working with Longer Texts (or Many Texts) 📚\n",
      "On most computers, spaCy should be able to read and process Jones's entire short story collection at once, as we did above:\n",
      "\n",
      "`filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"  \n",
      "text = open(filepath, encoding='utf-8').read()  \n",
      "document = nlp(text)`\n",
      "However, processing an entire book at once is computationally intensive. Because it takes up so much memory, this method won't work in Binder, for example, since Binder only lets us use a certain amount.\n",
      "\n",
      "What's more, spaCy won't be able to process longer texts, such as long novels, with this method. Even on a super fast computer, you'd get an error. (Though you can manually [increase the amount of memory spaCy uses](https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit), it's not advisable).\n",
      "If we want to process a long document, or many documents at once, with spaCy, then here's what we'll do. Rather than creating a single processed `document` with `nlp()`, we're going to create a bunch of smaller spaCy `documents` with `nlp.pipe()`. The [`nlp.pipe()`](https://spacy.io/usage/processing-pipelines#processing) method is faster and more efficient when we're processing many documents.\n",
      "We `open()` and `.read()` our *Lost in the City* text file as we did with the first short story.\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath, encoding=\"utf-8\").read()\n",
      "\n",
      "But then we `.split()` the short story collection on every line break `\\n` and process each chunk of the text as its own document, returning a list of `chunked_documents`.\n",
      "#Split text on line breaks \n",
      "chunked_text = text.split('\\n')\n",
      "#Process each chunk of text and return a list of processed documents\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "To extract people from all the `chunked_documents`, all we need to do is add one more `for` loop to our code and iterate through every document in `chunked_documents`.\n",
      "people = []\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            people.append(named_entity.text)\n",
      "            \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "places = [named_entity.text  for document in chunked_documents for named_entity in document.ents if named_entity.label_ == \"GPE\"]\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "## Get NER in Context\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def get_ner_in_context(keyword, document, desired_ner_labels= False):\n",
      "    \n",
      "    if desired_ner_labels != False:\n",
      "        desired_ner_labels = desired_ner_labels\n",
      "    else:\n",
      "        desired_ner_labels = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']  \n",
      "        \n",
      "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
      "    for sentence in document.sents:\n",
      "        #process each sentence\n",
      "        sentence_doc = nlp(sentence.text)\n",
      "        for named_entity in sentence_doc.ents:\n",
      "            #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
      "            if keyword.lower() in named_entity.text.lower()  and named_entity.label_ in desired_ner_labels:\n",
      "                #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
      "                #sentence_text = sentence.text\n",
      "            \n",
      "                sentence_text = re.sub('\\n', ' ', sentence.text)\n",
      "                sentence_text = re.sub(f\"{named_entity.text}\", f\"**{named_entity.text}**\", sentence_text, flags=re.IGNORECASE)\n",
      "\n",
      "                display(Markdown('---'))\n",
      "                display(Markdown(f\"**{named_entity.label_}**\"))\n",
      "                display(Markdown(sentence_text))\n",
      "for document in chunked_documents:\n",
      "    get_ner_in_context('Cassandra', document)\n",
      "## Your Turn!\n",
      "Now it's your turn to take a crack at NER with a whole new text!\n",
      "\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "In this section, you're going to extract and count named entities from Barack Obama's memoir *The Audacity of Hope*. We're exploring Obama's memoir because it's chock full of named entities.\n",
      "Open and read the text file\n",
      "filepath = \"../texts/literature/Obama-The-Audacity-of-Hope.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "To process *The Audacity of Hope* in smaller chunks (if working in Binder or on a computer with memory constraints):\n",
      "chunked_text = text.split('\\n')\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "To process *The Audacity of Hope* all at once (if working on a computer with a larger amount of memory):\n",
      "document = nlp(text)\n",
      "**1.** Choose a named entity from the possible spaCy named entities listed above. Extract, count, and make a dataframe from the most frequent named entities (of the type that you've chosen) in *The Audacity of Hope*. If you need help, study the examples above.\n",
      "#Your Code Here 👇 \n",
      "\n",
      "**2.** What is a result from this NER extraction that conformed to your expectations, that you find obvious or predictable? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**3.** What is a result from this NER extraction that defied your expectations, that you find curious or counterintuitive? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**4.** What's an insight that you might be able to glean about *The Audacity of Hope* based on your NER extraction?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "## Topic Modeling — Set Up\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "\n",
      "This page describes how to set up the packages and programs that you'll need if you want to start topic modeling on your own computer. If you want to topic model without installing anything, however, you can skip ahead and explore these Jupyter notebook topic modeling lessons in the cloud. They notebooks already have the necessary requirements installed:\n",
      "- [Topic Modeling - Text Files](https://mybinder.org/v2/gh/melaniewalsh/Intro-Cultural-Analytics/master?filepath=Website-Content%2FText-Analysis%2FTopic-Modeling-Text-Files.ipynb)\n",
      "- [Topic Modeling - CSV Files](https://mybinder.org/v2/gh/melaniewalsh/Intro-Cultural-Analytics/master?filepath=Website-Content%2FText-Analysis%2FTopic-Modeling-CSV.ipynb)\n",
      "- [Topic Modeling - Time-Series](https://mybinder.org/v2/gh/melaniewalsh/Intro-Cultural-Analytics/master?filepath=Website-Content%2FText-Analysis%2FTopic-Modeling-Time-Series.ipynb)\n",
      "## MALLET & Little MALLET Wrapper\n",
      "For our topic modeling analysis, we're going to use a tool called [MALLET](http://mallet.cs.umass.edu/topics.php). MALLET, short for **MA**chine **L**earning for **L**anguag**E** **T**oolkit, is a software package for  topic modeling and other natural language processing techniques. It's maintained by David Mimno, a Cornell professor in Information Science. Go Big Red!\n",
      "\n",
      "MALLET is great, but it's written in Java, another programming language, which means that we have to install Java before we can use it. It also means that MALLET isn't typically ideal for Python and Jupyter notebooks.\n",
      "\n",
      "Luckily, another Cornellian, Maria Antoniak, a PhD student in Information Science, has written a conveinet Python package that will allow us to use MALLET in this Jupyter notebook after we download and install Java. This package is called [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper).\n",
      "\n",
      "Note: A \"wrapper\" is a Python package that makes complicated code easier to use and/or makes code from a different programming language accessible in Python.\n",
      "## Download and Install Java Development Kit\n",
      "But first, we have to install Java, specifically the Java Development Kit.\n",
      "\n",
      "Go to the Java Development Kit download page, find your operating system, and click on the corresponding download link: https://www.oracle.com/java/technologies/javase-jdk14-downloads.html\n",
      "\n",
      "- Linux -> Linux Compressed Archive\n",
      "- Mac -> macOS Installer\n",
      "- Windows -> Windowsx64 Installer\n",
      "\n",
      "Then open or unzip the file and follow all the prompts. You can use all the suggested defaults.\n",
      "## Tell Your Computer Where to Find Java\n",
      "Now that we have the JDK downloaded, we have to tell our computers where to find it. For Mac/Chrome/Linux users, we have to set up a special [\"environment\" variable](https://launchschool.com/books/command_line/read/environment#environmentvariables) called `JAVA_HOME` and give it the file path where we just downloaded our Java Development Kit. For Windows users, we have to edit the special environmental variable called `PATH` and add the file path of the JDK.\n",
      "\n",
      "Note: \"Environment\" variables are kind of like Python variables, except they exist in your whole computer environment. The Launch School has a helpful chapter on [environment variables](https://launchschool.com/books/command_line/read/environment#environmentvariables) and the [PATH](https://launchschool.com/books/command_line/read/environment#path) variable.\n",
      "###  Mac\n",
      "To set up the `JAVA_HOME` environment variable on a Mac, you can run the following on the command line. The line of code adds your `JAVA_HOME` variable to a file called \"bash_profile\", which is where environment variables are stored.\n",
      "!echo \"export JAVA_HOME=$(/usr/libexec/java_home)\" >> ~/.bash_profile\n",
      "To immediately update your \"bash_profile,\" run:\n",
      "!source ~/.bash_profile\n",
      "Then, to test whether Java installed correctly, run `javac` on the command line. If you get a list of options, as below, then you've installed the JDK properly. If it says the command is not recognized, then you don't have JDK set up yet.\n",
      "!javac\n",
      "### <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "To edit the `PATH` variable on a Windows computer, follow the instructions below:\n",
      "- Open Search and type \"advanced system settings\"\n",
      "- In the shown options, select the \"View advanced system settings\" link\n",
      "- Under the Advanced tab, click \"Environment Variables\"\n",
      "- Under \"System variables,\" click the variable \"PATH\" and then click \"Edit\"\n",
      "- Click \"New\" and add the file path to the JDK (e.g. `C:\\Program Files\\Java\\jdk13.0.2\\bin`)\n",
      "For more Windows installation help, see Prof. Paul Vierthaler's video tutorial [\"Practical Python for DH: Topic Modeling Software Install\"](https://youtu.be/2C3cDEd7h4o?t=224).\n",
      "Now restart your PowerShell. To test whether java is installed, run `javac` in the PowerShell. If you get a list of options, then you've installed the JDK properly. If it says the command is not recognized, then you don't have it yet.\n",
      "!javac\n",
      "### Chrome / Linux\n",
      "To set up the `JAVA_HOME` environment variable on a Linux machine or a Chrome computer running Linux, you can run the following on the command line. The line of code adds your `JAVA_HOME` variable to a file called \"bashrc\", which is where environment variables are stored.\n",
      "Make sure to change `/fill-in-the-path/to/your-java_installation` to the file path where your JDK actually exists below:\n",
      "!echo \"export JAVA_HOME=/fill-in-the-path/to/your-java_installation/bin\" >> ~/.bashrc\n",
      "To immediately update your \"bash_profile,\" run:\n",
      "!source ~/.bashrc\n",
      "To test whether java is installed, run `javac` on the command line. If you get a list of options, as below, then you've installed the JDK properly. If it says the command is not recognized, then you don't have it yet.\n",
      "!javac\n",
      "## Download and Unzip MALLET\n",
      "Now we need to download the MALLET package. To download MALLET, click the following link http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip or find the link on the [MALLET home page](http://mallet.cs.umass.edu/download.php). Once the zip file downloads, unzip it.\n",
      "If you're using a Mac, move the \"mallet-2.0.8\" directory into your home folder.\n",
      "\n",
      "*Note: To open your \"home\" folder, open \"Finder\" and type `Cmd` + `Shift` + `H`. To move one directory up, type `Cmd` + `↑`. Now, if you want to bookmark your home folder so you can find it more easily in the future, simply drag and drop your home folder to the sidebar.*\n",
      "\n",
      "If you're using a Windows computer, move the \"mallet-2.0.8\" directory int your `C:\\` drive. \n",
      "### <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Heads Up Windows Users! \n",
      "You need to complete one more step. You need to once again tell your computer where MALLET is located:\n",
      "- Open Search and type \"advanced system settings\"\n",
      "- In the shown options, select the View advanced system settings link\n",
      "- Under the Advanced tab, click \"Environment Variables\"\n",
      "- In the User variables section, click \"New\"\n",
      "- For the Variable name, type `MALLET_HOME`. For the Value, type the path to your MALLET: `C:\\mallet-2.0.8`. Then click OK\n",
      "- Click OK and click Apply to apply the changes\n",
      "For more Windows help, see Prof. Paul Vierthaler's [topic modeling tutorial](https://youtu.be/2C3cDEd7h4o?t=107).\n",
      "To test whether MALLET works on your computer, type in the file path for MALLET on the command line and `import-file`.\n",
      "\n",
      "If it's working, then you'll get a message that says \"A tool for creating instance lists of feature vectors from comma-separated-values\" and a list of options.\n",
      "!~/mallet-2.0.8/bin/mallet import-file\n",
      "## Install Little MALLET Wrapper\n",
      "Finally, we're going to install the Python package [little_mallet_wrapper](https://github.com/maria-antoniak/little-mallet-wrapper). To install it, run `pip install little_mallet_wrapper`, as below.\n",
      "!pip install little_mallet_wrapper\n",
      "Since Little MALLET Wrapper also uses the data visualization library `seaborn`, we're also going to `pip install seaborn`:\n",
      "!pip install seaborn\n",
      "## You're Ready! 🥳\n",
      "\n",
      "## Make a Character Network\n",
      "This notebook demonstrates how to create a social network of people mentioned in a text based on how often people appear within a certain distance of one another.\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import networkx \n",
      "import itertools\n",
      "## Load spaCy Language Model\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "nlp = en_core_web_sm.load()\n",
      "## Character Networks For Shorter Texts\n",
      "This section demonstrates how to create a character network from a text if you can process the entire text with spaCy at one time (mostly shorter texts).\n",
      "## Read in Text File\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "## Process Text\n",
      "document = nlp(text)\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Then output this list of spaCy's identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in my_list_of_characters:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe 's from character name\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        #Get the character index number from the text\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 50 characters of one another, add them to the `edge_list`. To change the number of characters, simply change the `threshold_distance` variable below:\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 50\n",
      "\n",
      "#If two people fall within 50 characters of one another, add them to the edge list\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        \n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Lost-in-the-City-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Lost-in-the-City-network.csv\")\n",
      "## Character Networks For Longer Texts\n",
      "This section demonstrates how to create a character network from a text if you cannot process the entire text with spaCy at one time and need to chunk it into smaller documents (mostly longer texts).\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath).read()\n",
      "## Chunk Text By Number of Chunks\n",
      "To chunk text by a specific number of chunks, choose a `number_of_chunks` value and run the cell below. The current default value is 80 chunks. \n",
      "import math\n",
      "number_of_chunks = 80\n",
      "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
      "chunked_text = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
      "The code above is dividing the total number of characters in the text `len(text)` by the number of chunks you want, then rounding up `math.ceil()` to a whole number. This is calculating the necessary chunk size for the number of chunks you want. The final line iterates through the text and creates slices at the necessary chunk size.\n",
      "## Or Chunk Text By Line Breaks\n",
      "#chunked_text= text.split('\\n')\n",
      "## Process Chunked Text\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document(s)\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "document_length = 0\n",
      "\n",
      "for document in chunked_documents:\n",
      "    document_length += len(document.text)\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.text in my_list_of_characters:\n",
      "            person = named_entity.text\n",
      "\n",
      "            #Remove apostrophe 's from character name\n",
      "            person = person.replace(\"’s\", \"\").strip()\n",
      "            \n",
      "            #Get the character index number from the text\n",
      "            person_index =  (document_length - named_entity.start_char)\n",
      "\n",
      "            all_people_matches.append(person)\n",
      "            all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 100 characters of one another, add them to the `edge_list`.\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 100\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Filter Network By Edge Weights\n",
      "character_network[character_network['Weight'] > 2]\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Little-Women-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Little-Women-network.csv\")\n",
      "## Part-of-Speech Tagging & Keyword Extraction — Code\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "This notebook is a streamlined version of a previous lesson on **part-of-speech tagging**. It is primarily intended for those who want to reuse the code without the previous lessons' overview and explanations.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things later on; the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "`document = nlp(open(filepath, , encoding='utf-8').read())`\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "\n",
      "document = nlp(open(filepath, encoding='utf-8').read())\n",
      "## Get Part-Of-Speech Tags\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
      "| ADP   | adposition                | in, to, during                                |\n",
      "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
      "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
      "| CONJ  | conjunction               | and, or, but                                  |\n",
      "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
      "| DET   | determiner                | a, an, the                                    |\n",
      "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
      "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
      "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
      "| PART  | particle                  | ’s, not,                                      |\n",
      "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
      "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
      "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
      "| SCONJ | subordinating conjunction | if, while, that                               |\n",
      "| SYM   | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :), 😝             |\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "| X     | other                     | sfpksdpsxmsa                                  |\n",
      "| SPACE | space                     |                                               |\n",
      "\n",
      "To get part of speech tags for every word in a document, we have to iterate through all the tokens in the document and pull out the `.pos_` attribute for each token. We can get even finer-grained dependency information with the attribute `.dep_`.\n",
      "\n",
      "for token in document:\n",
      "    print(token.text, token.pos_, token.dep_)\n",
      "## Get Verbs\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "verbs = [token.text for token in document if token.pos_ == 'VERB']\n",
      "verbs_tally = Counter(verbs)\n",
      "df = pd.DataFrame(verbs_tally.most_common(), columns=['verb', 'count'])\n",
      "df[:100]\n",
      "To write this dataframe to a CSV file, we can use `df.to_csv()`:\n",
      "#df.to_csv(\"Lovelace-verbs.csv\", encoding='utf-8', index=False)\n",
      "## Get Keyword in Context\n",
      "#Make a list of tokens and POS labels from document if the token is a word \n",
      "tokens_and_labels = [(token.text, token.pos_) for token in document if token.is_alpha]\n",
      "def get_keyword_in_context(keyword, word_list, number_surrounding_words, pos_label=None):\n",
      "    \n",
      "    ngrams = []\n",
      "    words_around_keyword = []\n",
      "    adj_length_of_word_list = len(word_list) - (number_surrounding_words)\n",
      "    \n",
      "    keyword = keyword.lower()\n",
      "    \n",
      "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
      "    for word_index in range(adj_length_of_word_list):\n",
      "        \n",
      "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
      "        ngram = word_list[word_index : word_index + (number_surrounding_words + 1)]\n",
      "        \n",
      "        #Append this word combo to the master list \"ngrams\"\n",
      "        ngrams.append(ngram)\n",
      "    \n",
      "    \n",
      "    for word_label_pair in ngrams:\n",
      "    \n",
      "        words = [word.lower() for word, label in word_label_pair]\n",
      "        labels = [label for word, label in word_label_pair if word != keyword]\n",
      "\n",
      "        if keyword in words:\n",
      "\n",
      "            if pos_label in labels or pos_label == None:\n",
      "                if keyword in words[0]:\n",
      "                    words_around_keyword.append(\" \".join(words[1:]))\n",
      "\n",
      "                elif keyword in words[number_surrounding_words]:\n",
      "                    words_around_keyword.append(\" \".join(words[:number_surrounding_words]))\n",
      "    \n",
      "    words_around_keyword = [word.lower() for word in words_around_keyword]\n",
      "    \n",
      "    return Counter(words_around_keyword).most_common()\n",
      "get_keyword_in_context(\"computer\", tokens_and_labels, number_surrounding_words=2)\n",
      "get_keyword_in_context(\"computer\", tokens_and_labels, number_surrounding_words=2, pos_label=\"ADJ\")\n",
      "get_keyword_in_context(\"women\", tokens_and_labels, number_surrounding_words=2)\n",
      "## Term Frequency–Inverse Document Frequency\n",
      "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "In this lesson, we're going to learn about a text analysis method called **term frequency–inverse document frequency** (tf–idf). This method will help us identify the most unique words in a document from a given corpus. \n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "### Why is tf–idf Useful?\n",
      "\n",
      "### The Basic Math\n",
      "> `term_frequency * inverse_document_frequency`\n",
      "#### Breaking Down the Formula\n",
      "\n",
      "> `term_frequency = number of times a given word appears in story or text`\n",
      "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n",
      "\n",
      "> `total_number_of_documents / number_of_documents_with_term`\n",
      "\n",
      "...the result of which we're going to take the logarithm of and then add 1\n",
      "\n",
      "> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n",
      "\n",
      "Do you see how if we flipped the fraction — making it `number_of_documents_with_term /  total_number_of_documents`— that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\"\n",
      "### The Formula in Action\n",
      "**\"said\" vs \"pigeons\"**\n",
      "Using this formula, we're going to calculate and compare the tf–idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*.\n",
      "We need the log() function for our calculation, so we're going to import it from the `math` package.\n",
      "from math import log\n",
      "**\"said\"**\n",
      "total_number_of_documents = 14 ##total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 13 ##number of short stories the contain the word \"said\"\n",
      "term_frequency = 47 ##number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**\"pigeons\"**\n",
      "total_number_of_documents = 14 ##total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 2 ##number of short stories the contain the word \"pigeons\"\n",
      "term_frequency = 30 ##number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**tf–idf Scores**\n",
      "\n",
      "\"said\" = 50.48<br>\n",
      "\"pigeons\" = 88.38\n",
      "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf–idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them.\n",
      "## tf–idf with scikit-learn\n",
      "#### Import Libraries\n",
      "We could continue calculating tf–idf scores in this manner — by doing all the math with Python — but conveniently there's a Python library that can calculate tf–idf scores in just a few lines of code.\n",
      "\n",
      "This library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`.\n",
      "!pip install sklearn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 200)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html##basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*.\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "#### Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_files\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "Path(\"../texts/literature/Lost-in-the-City_Stories/04-Young-Lions.txt\").stem\n",
      "text_titles\n",
      "Let's display them to make sure they're correct:\n",
      "text_files, text_titles\n",
      "#### Calculate Word Frequency (Optional Step)\n",
      "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf–idf scores, we can compare these two methods and see how tf–idf helps us find more unique words.\n",
      "\n",
      "(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)\n",
      "##Initialize CountVectorizer with desired parameters\n",
      "count_vectorizer= CountVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "##Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\n",
      "word_count_vector = count_vectorizer.fit_transform(text_files)\n",
      "Check the sciki-learn stop words\n",
      "count_vectorizer.get_stop_words()\n",
      "#Make a DataFrame out of the word count vector and sort by title\n",
      "word_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\n",
      "word_count_df = word_count_df.sort_index()\n",
      "\n",
      "#Add column for number of times each word appears in all the documents\n",
      "word_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()\n",
      "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\").\n",
      "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words.\n",
      "word_count_df.sample(10, axis='columns')\n",
      "Let's zoom in on some specific words.\n",
      "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`\n",
      "def get_top_n_counts(dataframe, top_n=10):\n",
      "    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words.\n",
      "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')\n",
      "top_word_freq = get_top_n_counts(word_count_df)\n",
      "top_word_freq\n",
      "## Calculate tf–idf\n",
      "To calculate tf–idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
      "\n",
      "When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
      "#### Without Smoothing or Normalization (Not Recommended)\n",
      "Remember how we calculated the tf–idf score for the word \"pigeons\" above?\n",
      "total_number_of_documents = 14 \n",
      "number_of_documents_with_term = 2\n",
      "term_frequency = 30\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "\n",
      "term_frequency * inverse_document_frequency\n",
      "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf–idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn.\n",
      "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "#### With Smoothing and Normalization (Defaults/Recommended)\n",
      "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. \n",
      "\n",
      "Smoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all.\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df)\n",
      "top_tfidf\n",
      "#### Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Compare Word Frequency and tf–idf Scores\n",
      "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*.\n",
      "First, we're going to merge the top raw word frequency ranks into our top tf–idf dataframe.\n",
      "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')\n",
      "Then we're going to add a column that calculates the change in rank—that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\n",
      "tfidf_compare = tfidf_compare.fillna(\"*new top word*\")\n",
      "Finally, we're going to make some functions that will alter the style of our Pandas dataframe—such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign.\n",
      "def make_positive(value):\n",
      "    if value != '*new top word*':\n",
      "        if float(value) > 0:\n",
      "            value = f'+{round(value)}'\n",
      "    return value\n",
      "\n",
      "def make_bold(value):\n",
      "    return 'font-weight: bold'\n",
      "\n",
      "def color_df(value):\n",
      "    if value == '*new top word*':\n",
      "        color = 'green'    \n",
      "    else:\n",
      "        value = str(value).replace('+', '')\n",
      "        value = float(value)\n",
      "        \n",
      "        if value < 0:\n",
      "            color = 'red'\n",
      "        elif value > 0:\n",
      "            color = 'green'\n",
      "        else:\n",
      "             color = 'black'        \n",
      "    df_style = f'color: {color}; font-weight: bold'\n",
      "    return df_style\n",
      "Now let's display the dataframe and explore which words have become more significant and which words have become less so.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\n",
      "tfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\n",
      "tfidf_compare_styled\n",
      "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n",
      "\n",
      "```{note}\n",
      "Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below\n",
      "```\n",
      "tfidf_compare[tfidf_compare['word'] == 'said']\n",
      "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer.\n",
      "tfidf_compare[tfidf_compare['word'] == 'pigeons']\n",
      "Words that were not frequent enough to make the top 10 for raw word frequency — such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks — now suddenly show up in the top 10 for tf-idf scores.\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreaming']\n",
      "tfidf_compare[tfidf_compare['word'] == 'gospelteers']\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']\n",
      "## Your Turn!\n",
      "Take a few minutes to explore the dataframe below and then answer the following questions.\n",
      "tfidf_compare\n",
      "**1.** What is the difference between a tf-idf score and raw word frequency?\n",
      "**Your answer here**\n",
      "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?\n",
      "**Your answer here**\n",
      "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**Your answer here**\n",
      "## Keyword in Context\n",
      "#Make a list of tokens and POS labels from document if the token is a word \n",
      "tokens_and_labels = [(token.text, token.pos_) for token in document if token.is_alpha]\n",
      "def get_keyword_in_context(keyword, word_list, number_surrounding_words, pos_label=None):\n",
      "    \n",
      "    ngrams = []\n",
      "    words_around_keyword = []\n",
      "    adj_length_of_word_list = len(word_list) - (number_surrounding_words)\n",
      "    \n",
      "    keyword = keyword.lower()\n",
      "    \n",
      "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
      "    for word_index in range(adj_length_of_word_list):\n",
      "        \n",
      "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
      "        ngram = word_list[word_index : word_index + (number_surrounding_words + 1)]\n",
      "        \n",
      "        #Append this word combo to the master list \"ngrams\"\n",
      "        ngrams.append(ngram)\n",
      "    \n",
      "    \n",
      "    for word_label_pair in ngrams:\n",
      "    \n",
      "        words = [word.lower() for word, label in word_label_pair]\n",
      "        labels = [label for word, label in word_label_pair if word != keyword]\n",
      "\n",
      "        if keyword in words:\n",
      "\n",
      "            if pos_label in labels or pos_label == None:\n",
      "                if keyword in words[0]:\n",
      "                    words_around_keyword.append(\" \".join(words[1:]))\n",
      "\n",
      "                elif keyword in words[number_surrounding_words]:\n",
      "                    words_around_keyword.append(\" \".join(words[:number_surrounding_words]))\n",
      "    \n",
      "    words_around_keyword = [word.lower() for word in words_around_keyword]\n",
      "    \n",
      "    return Counter(words_around_keyword).most_common()\n",
      "get_keyword_in_context(\"mother\", tokens_and_labels, number_surrounding_words=2, pos_label='VERB')\n",
      "## Named Entity Recognition — Code\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "This notebook is a streamlined version of a previous lesson on **Named Entity Recognition**. It is primarily intended for those who want to reuse the code without the previous lessons' overview and explanations.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things later on; the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "## Get Named Entities\n",
      "|Type Label|Description|\n",
      "|--- |--- |\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "All the named entities in our `document` can be found in the `document.ents` property. We can access the entity labels by iterating through the `document.ents` with a simple `for` loop and pulling out the `.label_` attribute.\n",
      "for named_entity in document.ents:\n",
      "    print(named_entity, named_entity.label_)\n",
      "## Get People\n",
      "|Type Label|Description|\n",
      "|--- |--- |\n",
      "|PERSON|People, including fictional.|\n",
      "people = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"PERSON\"]\n",
      "people_tally = Counter(people)\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "## Process Long Documents (or Many Documents)\n",
      "Rather than creating a single processed `document` with `nlp()`, we're going to create a bunch of smaller spaCy `documents` with `nlp.pipe()`. The [`nlp.pipe()`](https://spacy.io/usage/processing-pipelines#processing) method is faster and more efficient when we're processing many documents.\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath, encoding=\"utf-8\").read()\n",
      "\n",
      "#Split text on line breaks \n",
      "chunked_text = text.split('\\n')\n",
      "#Process each chunk of text and return a list of processed documents\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "We `open()` and `.read()` our text file, then `.split()` the text on every line break `\\n` and process each chunk of the text as its own document, returning a list of `chunked_documents`.\n",
      "To extract people from all the `chunked_documents`, all we need to do is add one more `for` loop to our code and iterate through every document in `chunked_documents`.\n",
      "people = []\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            people.append(named_entity.text)\n",
      "            \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "places = [named_entity.text  for document in chunked_documents for named_entity in document.ents if named_entity.label_ == \"GPE\"]\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "To write these dataframe to a CSV file, we can use `df.to_csv()`:\n",
      "#df.to_csv(\"people.csv\", encoding='utf-8', index=False)\n",
      "## Named Entity Recognition — Cluster Characters\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "!pip install fuzzywuzzy\n",
      "!pip install python-Levenshtein\n",
      "import spacy\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import networkx \n",
      "from networkx.algorithms.components.connected import connected_components, node_connected_component\n",
      "import itertools\n",
      "from fuzzywuzzy import fuzz\n",
      "import glob\n",
      "from pathlib import Path\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "document = nlp(text)\n",
      "displacy.render(document, style=\"ent\")\n",
      "## Get People (More Accurately)\n",
      "Sometimes spaCy will correctly tag a person as a \"PERSON\" but then later tag the same person as a different entity as an organization (\"ORG\") or a place (\"GPE\").\n",
      "\n",
      "So, to get a more accurate character count, we're going to extract all the named entities that spaCy identified as a \"PERSON\" and then count *any* instance of that entitiy, regardless of its NER label.\n",
      "Extract list of all named entities labeled \"PERSON\":\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified to a CSV file and manually edit by hand\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Re-upload CSV file for accurate list of people\n",
      "spacy_identified_people = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in spacy_identified_people:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe s\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "people_tally = Counter(all_people_matches)\n",
      "character_df = pd.DataFrame(people_tally.most_common())\n",
      "character_df.columns = ['character', 'count']\n",
      "\n",
      "character_df\n",
      "## Cluster By Name and Distance\n",
      "\n",
      "spaCy doesn't know that \"Betsy Ann Morgan\" and \"Betsy Ann\" should be the same person. So we're also going to pair two character names if they're an extremely close match and they occur within 800 characters of one another.\n",
      "from datetime import datetime\n",
      "startTime = datetime.now()\n",
      "\n",
      "aggregated_people = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, person_index in all_people_matches_plus_ids:\n",
      "    for another_person, another_person_index in all_people_matches_plus_ids:\n",
      "        distance = abs(person_index - another_person_index)\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person != another_person:\n",
      "                \n",
      "                if fuzz.partial_ratio(person, another_person) == 100:\n",
      "                    aggregated_people.append((person, another_person))\n",
      "                    \n",
      "print(datetime.now() - startTime)\n",
      "With itertools (slightly faster)\n",
      "from datetime import datetime\n",
      "startTime = datetime.now()\n",
      "\n",
      "aggregated_people = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                if fuzz.partial_ratio(person[0], another_person[0]) == 100:\n",
      "                    aggregated_people.append((person[0], another_person[0]))\n",
      "                    \n",
      "print(datetime.now() - startTime)\n",
      "## Make a network!\n",
      "from datetime import datetime\n",
      "startTime = datetime.now()\n",
      "\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 400\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "                    \n",
      "print(datetime.now() - startTime)\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'edge_weight'])\n",
      "character_df['character1']=character_df['character_pair'].str[0]\n",
      "character_df['character2']=character_df['character_pair'].str[1]\n",
      "character_df\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "G=networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "#G.add_weighted_edges_from(character_df)\n",
      "G=networkx.from_pandas_edgelist(character_df, 'character1', 'character2', 'edge_weight')\n",
      "#G.add_weighted_edges_from(character_df)\n",
      "import matplotlib.pyplot as plt\n",
      "networkx.write_graphml(G, 'lost-in-the-city-character-network.graphml', encoding='utf-8')\n",
      "networkx.draw(G, with_labels=False, font_weight='bold')\n",
      "import networkx as nx\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.models import (BoxZoomTool, Circle, HoverTool,\n",
      "                          MultiLine, Plot, Range1d, ResetTool,)\n",
      "from bokeh.palettes import Spectral4\n",
      "from bokeh.plotting import from_networkx\n",
      "\n",
      "# Prepare Data\n",
      "G = nx.karate_club_graph()\n",
      "\n",
      "SAME_CLUB_COLOR, DIFFERENT_CLUB_COLOR = \"black\", \"red\"\n",
      "edge_attrs = {}\n",
      "\n",
      "for start_node, end_node, _ in G.edges(data=True):\n",
      "    edge_color = SAME_CLUB_COLOR if G.nodes[start_node][\"club\"] == G.nodes[end_node][\"club\"] else DIFFERENT_CLUB_COLOR\n",
      "    edge_attrs[(start_node, end_node)] = edge_color\n",
      "\n",
      "nx.set_edge_attributes(G, edge_attrs, \"edge_color\")\n",
      "\n",
      "# Show with Bokeh\n",
      "plot = Plot(plot_width=400, plot_height=400,\n",
      "            x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n",
      "plot.title.text = \"Graph Interaction Demonstration\"\n",
      "\n",
      "node_hover_tool = HoverTool(tooltips=[(\"index\", \"@index\"), (\"club\", \"@club\")])\n",
      "plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
      "\n",
      "graph_renderer = from_networkx(G, nx.spring_layout, scale=1, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size=15, fill_color=Spectral4[0])\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"edge_color\", line_alpha=0.8, line_width=1)\n",
      "plot.renderers.append(graph_renderer)\n",
      "\n",
      "show(plot)\n",
      "G=networkx.from_pandas_edgelist(character_df, 'character1', 'character2', 'edge_weight')\n",
      "\n",
      "import bokeh.io\n",
      "# this is here only for completeness to clarify where\n",
      "# the methods are nested (you probably already imported this earlier)\n",
      "\n",
      "\n",
      "bokeh.io.reset_output()\n",
      "bokeh.io.output_notebook()\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='node_size', values=degrees)\n",
      "from bokeh.plotting import figure\n",
      "\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@node_size\")\n",
      "]\n",
      "\n",
      "plot = figure(plot_width=500, plot_height=500, tooltips = HOVER_TOOLTIPS, tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-5.1, 5.1), y_range=Range1d(-5.1, 5.1), title='Game of Thrones Network')\n",
      "\n",
      "#node_hover_tool = HoverTool(tooltips=[])\n",
      "\n",
      "#plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
      "\n",
      "graph_renderer = from_networkx(G, networkx.spring_layout, scale=5, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size='node_size', fill_color=Spectral4[0])\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_alpha=0.8, line_width=1)\n",
      "\n",
      "plot.renderers.append(graph_renderer)\n",
      "plot.xgrid.visible = False\n",
      "plot.ygrid.visible = False\n",
      "show(plot)\n",
      "## Circle Graph\n",
      "from bokeh.models import EdgesAndLinkedNodes, NodesAndLinkedEdges\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@node_size\")\n",
      "]\n",
      "\n",
      "plot = figure(plot_width=500, plot_height=500, tooltips = HOVER_TOOLTIPS, tools=\"pan,wheel_zoom,save,reset, tap\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-5.1, 5.1), y_range=Range1d(-5.1, 5.1), title='Game of Thrones Network')\n",
      "\n",
      "graph_renderer = from_networkx(G, networkx.circular_layout, scale=10, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size=10, fill_color=Spectral4[0])\n",
      "graph_renderer.node_renderer.selection_glyph = Circle(size=10, fill_color=Spectral4[2])\n",
      "graph_renderer.node_renderer.hover_glyph = Circle(size=10, fill_color=Spectral4[1])\n",
      "\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1)\n",
      "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1)\n",
      "\n",
      "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
      "graph_renderer.inspection_policy = EdgesAndLinkedNodes()\n",
      "\n",
      "plot.renderers.append(graph_renderer)\n",
      "plot.xgrid.visible = False\n",
      "plot.ygrid.visible = False\n",
      "show(plot)\n",
      "\n",
      "graph_renderer.node_renderer.data_source\n",
      "graph_renderer.layout_provider.graph_layout.items()\n",
      "names = []\n",
      "x_coordinates = []\n",
      "y_coordinates = []\n",
      "\n",
      "for name, x_y_coords in graph_renderer.layout_provider.graph_layout.items():\n",
      "    names.append(name)\n",
      "    x_coordinates.append(x_y_coords[0])\n",
      "    y_coordinates.append(x_y_coords[1])\n",
      "\n",
      "label_map = dict(zip(names, [x_coordinates, y_coordinates]))\n",
      "G.nodes()\n",
      "source = ColumnDataSource(data=graph_renderer.layout_provider.graph_layout)\n",
      "source[0]\n",
      "LabelSet(x='x', y='y', text='names', level='glyph',\n",
      "         x_offset=5, y_offset=5, source=source)\n",
      "values()\n",
      "type(graph_renderer.layout_provider.graph_layout)\n",
      "\n",
      "\n",
      "x, y = zip(*graph.layout_provider.graph_layout.values())\n",
      "\n",
      "from bokeh.models import EdgesAndLinkedNodes, NodesAndLinkedEdges\n",
      "from bokeh.models import CustomJSTransform, LabelSet\n",
      "\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@node_size\")\n",
      "]\n",
      "\n",
      "plot = figure(plot_width=500, plot_height=500, tooltips = HOVER_TOOLTIPS, tools=\"pan,wheel_zoom,save,reset, tap\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-5.1, 5.1), y_range=Range1d(-5.1, 5.1), title='Game of Thrones Network')\n",
      "\n",
      "graph_renderer = from_networkx(G, networkx.spring_layout, scale=5, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size='node_size', fill_color=Spectral4[0])\n",
      "graph_renderer.node_renderer.selection_glyph = Circle(size='node_size', fill_color='green')\n",
      "graph_renderer.node_renderer.hover_glyph = Circle(size='node_size', fill_color='red')\n",
      "\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color='green', line_width=1)\n",
      "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color='red', line_width=1)\n",
      "\n",
      "\n",
      "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
      "graph_renderer.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "\n",
      "\n",
      "plot.renderers.append(graph_renderer)\n",
      "plot.xgrid.visible = False\n",
      "plot.ygrid.visible = False\n",
      "\n",
      "\n",
      "\n",
      "from bokeh.transform import transform    \n",
      "\n",
      "# add the labels to the node renderer data source\n",
      "source = graph_renderer.node_renderer.data_source\n",
      "source.data['names'] = [str(x) for x in source.data['index']]\n",
      "\n",
      "# create a transform that can extract the actual x,y positions\n",
      "code = \"\"\"\n",
      "    var result = new Float64Array(xs.length)\n",
      "    for (var i = 0; i < xs.length; i++) {\n",
      "        result[i] = provider.graph_layout[xs[i]][%s]\n",
      "    }\n",
      "    return result\n",
      "\"\"\"\n",
      "xcoord = CustomJSTransform(v_func=code % \"0\", args=dict(provider=graph_renderer.layout_provider))\n",
      "ycoord = CustomJSTransform(v_func=code % \"1\", args=dict(provider=graph_renderer.layout_provider))\n",
      "\n",
      "# Use the transforms to supply coords to a LabelSet \n",
      "labels = LabelSet(x=transform('index', xcoord),\n",
      "                  y=transform('index', ycoord),\n",
      "                  text='names', text_font_size=\"12px\",\n",
      "                  x_offset=5, y_offset=5,\n",
      "                  source=source, render_mode='canvas')\n",
      "\n",
      "plot.add_layout(labels)\n",
      "show(plot)\n",
      "\n",
      "\n",
      "plot = Plot(plot_width=1000, plot_height=1000,\n",
      "            x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n",
      "\n",
      "node_hover_tool = HoverTool(tooltips=[(\"\", \"@index\")])\n",
      "plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
      "\n",
      "graph_renderer = from_networkx(G, networkx.spring_layout, scale=2, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size=15, fill_color=Spectral4[0])\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_alpha=0.8, line_width=1)\n",
      "plot.renderers.append(graph_renderer)\n",
      "\n",
      "show(plot)\n",
      "!pip install visJS2jupyter\n",
      "!pip install py2cytoscape\n",
      "import visJS2jupyter.visJS_module\n",
      "nodes = G.nodes()\n",
      "edges = list(G.edges())\n",
      "nodes_dict = [{\"id\":n, \"x\":pos[n][0]*300, \"y\":pos[n][1]*300} for n in nodes]\n",
      "node_map = dict(zip(nodes,range(len(nodes))))  # map to indices for source/target in edges\n",
      "for i in range(len(edges)):\n",
      "    print(node_map[edges][i][0])\n",
      "    #\"source\":node_map[edges[i][0]], \"target\":node_map[edges[i][1]]} \n",
      "nodes = list(G.nodes()) # must cast to list to maintain compatibility between nx 1.11 and 2.0\n",
      "edges = list(G.edges())\n",
      "!pip show bokeh\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.models import (BoxZoomTool, Circle, HoverTool,\n",
      "                          MultiLine, Plot, Range1d, ResetTool,)\n",
      "from bokeh.palettes import Spectral4\n",
      "from bokeh.plotting import from_networkx\n",
      "from bokeh.io import output_notebook, show\n",
      "output_notebook()\n",
      "!pip install pygraphviz\n",
      "degrees = [degree for node, degree in networkx.degree(G)]\n",
      "degrees = networkx.degree(G)\n",
      "G.get_node_attributes()\n",
      "networkx.info(G)\n",
      "from bokeh.models import ColumnDataSource\n",
      "node_size = {k:5*v for k,v in G.degree()} \n",
      "degree = networkx.degree(G)\n",
      "networkx.set_node_attributes(G,  degree, 'degree')\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='node_size', values=degrees)\n",
      "pd.DataFrame.from_dict({k:v for k,v in G.nodes(data=True)}, orient='index')\n",
      "node_source = ColumnDataSource(pd.DataFrame.from_dict({k:v for k,v in G.nodes(data=True)}, orient='index'))\n",
      "\n",
      "plot = Plot(plot_width=400, plot_height=400,\n",
      "            x_range=Range1d(-1.1,1.1), y_range=Range1d(-1.1,1.1))\n",
      "\n",
      "graph_renderer = from_networkx(G, networkx.spring_layout, scale=10, center=(0,0))\n",
      "\n",
      "#style\n",
      "graph_renderer.node_renderer.data_source = node_source\n",
      "graph_renderer.node_renderer.glyph = Circle(fill_color = 'blue',size = 'node_size' , line_color = None)\n",
      "\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width=2)\n",
      "\n",
      "\n",
      "plot.renderers.append(graph_renderer)\n",
      "\n",
      "show(plot)\n",
      "# Prepare Data\n",
      "#G = nx.karate_club_graph()\n",
      "\n",
      "#SAME_CLUB_COLOR, DIFFERENT_CLUB_COLOR = \"black\", \"red\"\n",
      "#edge_attrs = {}\n",
      "\n",
      "# for start_node, end_node, _ in G.edges(data=True):\n",
      "#     edge_color = SAME_CLUB_COLOR if G.nodes[start_node][\"club\"] == G.nodes[end_node][\"club\"] else DIFFERENT_CLUB_COLOR\n",
      "#     edge_attrs[(start_node, end_node)] = edge_color\n",
      "\n",
      "# nx.set_edge_attributes(G, edge_attrs, \"edge_color\")\n",
      "\n",
      "# Show with Bokeh\n",
      "plot = Plot(plot_width=1000, plot_height=1000,\n",
      "            x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n",
      "\n",
      "#plot.title.text = \"Lost in the City Characters\"\n",
      "\n",
      "node_hover_tool = HoverTool(tooltips=[(\"\", \"@index\")])\n",
      "plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
      "\n",
      "graph_renderer = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size=15, fill_color=Spectral4[0])\n",
      "#graph_renderer.edge_renderer.glyph = MultiLine(line_alpha=0.8, line_width=1)\n",
      "plot.renderers.append(graph_renderer)\n",
      "\n",
      "#output_file(\"interactive_graphs.html\")\n",
      "show(plot)\n",
      "fig = networkx.draw_networkx(G)\n",
      "fig(figsize=(20,10))\n",
      "fig, ax = plt.subplots(1, 1, figsize=(20, 10));\n",
      "networkx.draw_networkx(G, ax=ax, with_labels=True)\n",
      "import json\n",
      "nodes = [{'name': str(i)}\n",
      "         for i in G.nodes()]\n",
      "links = [{'source': u[0], 'target': u[1]}\n",
      "         for u in G.edges()]\n",
      "with open('graph.json', 'w') as f:\n",
      "    json.dump({'nodes': nodes, 'links': links},\n",
      "              f, indent=4,)\n",
      "<div id=\"d3-example\"></div>\n",
      "<style>\n",
      ".node {stroke: #fff; stroke-width: 1.5px;}\n",
      ".link {stroke: #999; stroke-opacity: .6;}\n",
      "</style>\n",
      "With the aggregated character names, we're going to use the Python library networkX to create a cluster of character names.\n",
      "G=networkx.Graph()\n",
      "G.add_edges_from(aggregated_people)\n",
      "people_clusters  = list(connected_components(G))\n",
      "people_clusters = [sorted(cluster, key=len, reverse=True) for cluster in people_clusters]\n",
      "people_clusters\n",
      "def add_clustered_characters(row):\n",
      "    character = row['character']\n",
      "    for cluster in people_clusters:\n",
      "        if character in cluster:\n",
      "            return cluster\n",
      "def clean_and_unpack_cluster(row):\n",
      "    if row['clustered_characters'] == None:\n",
      "        cluster = row['character']\n",
      "    else:\n",
      "        cluster = \" // \".join(row['clustered_characters'])\n",
      "    return cluster\n",
      "character_df['clustered_characters'] = character_df.apply(add_clustered_characters, axis=1)\n",
      "character_df['clustered_characters'] = character_df.apply(clean_and_unpack_cluster, axis=1)\n",
      "character_df\n",
      "Manually edit\n",
      "#character_df.sort_values(by=['clustered_characters', 'count']).to_csv('clustered_characters_draft.csv')\n",
      "#character_df = pd.read_csv('clustered_characters_edited.csv')\n",
      "character_df.groupby('clustered_characters')[['count']].sum().sort_values(by='count', ascending=False).reset_index()\n",
      "## Cluster By Name\n",
      "from fuzzywuzzy import process\n",
      "def cluster_characters(row):\n",
      "    possibilities = process.extract(row['character'], character_df['character'].unique(), scorer=fuzz.partial_ratio)\n",
      "    possibilities = [possible[0] for possible in possibilities if possible[1] == 100]\n",
      "    possibilities = [combo for combo in itertools.combinations(possibilities, 2)]\n",
      "    return possibilities\n",
      "clustered_by_named = character_df.apply(cluster_characters, axis=1)\n",
      "\n",
      "G=networkx.Graph()\n",
      "G.add_edges_from(clustered_by_named)\n",
      "people_clusters  = list(connected_components(G))\n",
      "people_clusters = [sorted(cluster, key=len, reverse=True) for cluster in people_clusters]\n",
      "people_clusters\n",
      "def add_clustered_characters(row):\n",
      "    character = row['character']\n",
      "    for cluster in people_clusters:\n",
      "        if character in cluster:\n",
      "            return cluster\n",
      "def clean_and_unpack_cluster(row):\n",
      "    if row['clustered_characters'] == None:\n",
      "        cluster = row['character']\n",
      "    else:\n",
      "        cluster = \" // \".join(row['clustered_characters'])\n",
      "    return cluster\n",
      "character_df['clustered_characters'] = character_df.apply(add_clustered_characters, axis=1)\n",
      "character_df['clustered_characters'] = character_df.apply(clean_and_unpack_cluster, axis=1)\n",
      "character_df\n",
      "import spacy\n",
      "import re\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import networkx as nx\n",
      "from networkx.algorithms import bipartite\n",
      "pd.set_option(\"display.max_rows\",1000)\n",
      "pd.set_option(\"display.max_columns\",1000)\n",
      "Eliminate blank lines\n",
      "nlp = spacy.load('en')\n",
      "nlp_mango_model = spacy.load('en-mango')\n",
      "mango2 = pd.read_csv('Mango-Characters-2.csv', delimiter='\\t')\n",
      "#count characters and organizations and include label\n",
      "def count_characters_by_match(filepath):\n",
      "    #open and read file with spacy\n",
      "    tokens = nlp(open(filepath).read())\n",
      "    #get the file name\n",
      "    filename = os.path.split(filepath)[-1].replace(\".txt\",\"\")\n",
      "    filename = filename.replace(\"-\",\" \")\n",
      "    story_number = re.match('(^[0-9]+)', filename).group()\n",
      "    filename = re.sub('(^[0-9]+)', '', filename)\n",
      "    filename = f'{filename} ({story_number})'\n",
      "    #get a list of tuples with people/organizations and entity label\n",
      "    character_list = [person for person in mango2['person']]\n",
      "    #character_list.append('I')\n",
      "    #character_list.append('me')\n",
      "    people = [item.text for item in tokens if item.text in character_list]\n",
      "    #clean up the people/organization names by getting rid of plurals, linebreaks, and some punctuation\n",
      "    if len(people) > 0:\n",
      "        people_counts = Counter(people)\n",
      "\n",
      "                # make datalist for pandas dataframe\n",
      "            #datalist = [(filename, people[0], people[1], people_counts) \n",
      "                     #           for ((people[0], people[1]), people_counts) in people_counts.items()]\n",
      "        datalist = [(filename, people_counts[0], people_counts[1], story_number) for people_counts in people_counts.items()]\n",
      "        tmp = pd.DataFrame(datalist)\n",
      "        tmp.columns = ['vignette','person','weight', 'story_number'] \n",
      "        return tmp\n",
      "    else:\n",
      "        return\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "characters_df_by_match = pd.DataFrame()\n",
      "for filepath in filepaths:\n",
      "    characters_df_by_match = characters_df_by_match.append(count_characters_by_match(filepath))\n",
      "characters_df_by_match = characters_df_by_match.replace('I', 'Esperanza')\n",
      "characters_df_by_match = characters_df_by_match.replace('me', 'Esperanza')\n",
      "characters_df_by_match\n",
      "G = nx.from_pandas_edgelist(characters_df_by_match, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df_by_match['person'], bimodal='character')\n",
      "G.add_nodes_from(characters_df_by_match['vignette'], bimodal='vignette')\n",
      "G.remove_node('People')\n",
      "nx.write_gexf(G, 'No-Esperanza-2020-by-match-mango-street-character-network.gexf')\n",
      "## Unimodal\n",
      "top_nodes = set(node for node, detail in G.nodes(data=True) if detail['bimodal']=='character')\n",
      "bottom_nodes = set(G) - top_nodes\n",
      "U = bipartite.weighted_projected_graph(G, bottom_nodes)\n",
      "bottom_nodes\n",
      "nx.write_gexf(U, 'no-Esperanza-2020-by-match-mango-street-character-network-unimodal-vignettes.gexf')\n",
      "top_nodes = set(node for node, detail in G.nodes(data=True) if detail['bimodal']=='character')\n",
      "bottom_nodes = set(G) - top_nodes\n",
      "U = bipartite.weighted_projected_graph(G, top_nodes)\n",
      "nx.write_gexf(U, 'no-Esperanza-2020-by-match-mango-street-character-network-unimodal-characters.gexf')\n",
      "SU = nx.Graph([(u,v,d) for u,v,d in U.edges(data=True) if d ['weight']>2] )\n",
      "#nx.set_node_attributes(G, pd.Series(nodes.story_number, index=nodes.node).to_dict(), 'story_number')\n",
      "\n",
      "\n",
      "## TF-IDF — Code\n",
      "This notebook is a streamlined version of the previous lesson on **term frequency–inverse document frequency** (tf–idf). It is primarily intended for those who want to reuse the code without the previous lesson's overview and explanations.\n",
      "## Import Libraries\n",
      "To calculate tf-idf scores, we're going to use a Python library called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. \n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 500)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html).\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're using `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "## Calculate tf–idf\n",
      "We need to initialize [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) with our desired parameters. Then we need to plug in the list of text file paths that we want to be calculated with `.fit_transform`.\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "Then we make a dataframe of every word in the collection and its corresponding tf-idf score.\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=10)\n",
      "top_tfidf\n",
      "If you want to change how many top tf-idf scores to show for every text, simply change the `top_n` value.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=20)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Text Analysis\n",
      "- TF-IDF\n",
      "- Topic Models\n",
      "- Named Entity Recognition\n",
      "## Part-of-Speech Tagging & Keyword Extraction\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "In this lesson, we're going to learn about the textual analysis methods **part-of-speech tagging** and **keyword extraction**. These methods will help us computationally parse sentences and better understand words in context.\n",
      "<img src=\"../images/Ada-dep-parse.png\" width=\"100%\", border=2>\n",
      "## Why is Part-of-Speech Tagging Useful?\n",
      "I don't mean to go all [Language Nerd](https://xkcd.com/1443/) on you, but parts of speech are important. Even if they seem kind of boring. As you may have learned in an English class at some point in your life, \"parts of speech\" are the grammatical units of language — such as (in English) nouns, verbs, adjectives, adverbs, pronouns, and prepositions. Each of these parts of speech plays a different role in a sentence.\n",
      "<img src=\"https://imgs.xkcd.com/comics/language_nerd.png\", border=2>\n",
      "\n",
      "By computationally identifying parts of speech, we can start computationally exploring *syntax*, the relationship between words — rather than only focusing on words in isolation, as we did with tf-idf. Though parts of speech may seem pedantic, they help computers (and us) crack at that ever-elusive abstract noun: *meaning*. \n",
      "## spaCy and Natural Language Processing (NLP)\n",
      "To computationally identify parts of speech, we're going to use the natural language processing library spaCy. For a more extensive introduction to NLP and spaCy, see the previous lesson.\n",
      "\n",
      "To parse sentences, spaCy relies on machine learning models that were trained on large amounts of labeled text data. The English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more.\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting nouns, verbs, adjectives, etc., and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
      "\n",
      "To test out spaCy's part-of-speech tagging, we'll begin by processing a sample sentence from Ada Lovelace's obituary:\n",
      "> \"[Charles] Babbage, who called [Ada Lovelace] the “enchantress of numbers,” once wrote that\n",
      "she “has thrown her magical **spell** around the most **abstract** of Sciences and has grasped\n",
      "it with a **force** which few masculine intellects (in our own country at least) could have exerted over it.\n",
      "This sentence makes for an interesting example because it is syntactically complex and because it includes contains difficultly ambiguous words such as \"spell,\" \"abstract,\" and \"force.\"\n",
      "sample = \"\"\"She “has thrown her magical spell around the most abstract of Sciences and has grasped\n",
      "it with a force which few masculine intellects (in our own country at least) could have exerted over it.\"\"\"\n",
      "document = nlp(sample)\n",
      "## spaCy Part-of-Speech Tagging\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
      "| ADP   | adposition                | in, to, during                                |\n",
      "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
      "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
      "| CONJ  | conjunction               | and, or, but                                  |\n",
      "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
      "| DET   | determiner                | a, an, the                                    |\n",
      "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
      "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
      "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
      "| PART  | particle                  | ’s, not,                                      |\n",
      "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
      "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
      "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
      "| SCONJ | subordinating conjunction | if, while, that                               |\n",
      "| SYM   | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :), 😝             |\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "| X     | other                     | sfpksdpsxmsa                                  |\n",
      "| SPACE | space                     |                                               |\n",
      "\n",
      "Above is a POS chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different parts of speech that spaCy can identify as well as their corresponding labels. To quickly see spaCy's POS tagging in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) on our sample `document` with the `style=` parameter set to \"dep\" (short for dependency parsing):\n",
      "#Set some display options for the visualizer\n",
      "options = {\"compact\": True, \"distance\": 90, \"color\": \"yellow\", \"bg\": \"black\", \"font\": \"Gill Sans\"}\n",
      "\n",
      "displacy.render(document, style=\"dep\", options=options)\n",
      "As you can see, spaCy has correctly identified that \"spell\" and \"force\" are nouns in our sample sentence:\n",
      "for token in document:\n",
      "    if token.pos_ == \"NOUN\":\n",
      "        print(token, token.pos_)\n",
      "But if we look at the same words in a different context — in a sentence that I made up — spaCy can identify when these words have changed  grammatical roles and meanings.\n",
      "> You shouldn't **force** someone to learn how to **spell** Babbage. They just need practice. You can't **abstract** it.\n",
      "document = nlp(\"You shouldn't force someone to learn how to spell Babbage. They just need practice. You can't abstract it.\")\n",
      "for token in document:\n",
      "    if token.pos_ == \"VERB\":\n",
      "        print(token, token.pos_)\n",
      "Where previously spaCy had identified \"force\" and \"spell\" as nouns, here spaCy correctly identifies the words \"force,\" \"spell,\" and \"abstract\" as verbs.\n",
      "## Get Part-Of-Speech Tags\n",
      "To get part of speech tags for every word in a document, we have to iterate through all the tokens in the document and pull out the `.pos_` attribute for each token. We can get even finer-grained dependency information with the attribute `.dep_`.\n",
      "\n",
      "for token in document:\n",
      "    print(token.text, token.pos_, token.dep_)\n",
      "## Practicing with *Lost in the City*\n",
      "For the rest of this lesson, we're going to explore Edward P. Jones's short story collection *Lost in the City*.\n",
      "<img src=\"https://mybinder.org/static/images/logo_social.png\" width=\"150\" align=\"left\", border=2> *If you're using this Jupyter notebook in Binder (in the cloud), please uncomment the cell below and work with only the first story from _Lost in the City_. The Binder notebook is currently having issues loading the entire collection.*\n",
      "#file = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "#document = nlp(open(file).read())\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "document = nlp(open(filepath, encoding=\"utf-8\").read())\n",
      "## Get Adjectives\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
      "To extract and count the adjectives in *Lost in the City*, we will follow the same model as above, except we'll add an `if` statement that will pull out words only if their POS label matches \"ADJ.\"\n",
      "```{admonition} Python Review!\n",
      ":class: pythonreview\n",
      "While we demonstrate how to extract parts of speech in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.\n",
      "```\n",
      "Here we make a list of the adjectives identified in *Lost in the City*:\n",
      "adjs = []\n",
      "for token in document:\n",
      "    if token.pos_ == 'ADJ':\n",
      "        adjs.append(token.text)\n",
      "adjs\n",
      "Then we count the unique adjectives in this list with the `Counter()` module:\n",
      "adjs_tally = Counter(adjs)\n",
      "adjs_tally.most_common()\n",
      "Then we make a dataframe from this list:\n",
      "df = pd.DataFrame(adjs_tally.most_common(), columns=['character', 'count'])\n",
      "df[:100]\n",
      "## Get Nouns\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
      "To extract and count nouns, we can follow the same model as above, except we will change our `if` statement to check for POS labels that match \"NOUN\".\n",
      "nouns = []\n",
      "for token in document:\n",
      "    if token.pos_ == 'NOUN':\n",
      "        nouns.append(token.text)\n",
      "\n",
      "nouns_tally = Counter(nouns)\n",
      "\n",
      "df = pd.DataFrame(nouns_tally.most_common(), columns=['noun', 'count'])\n",
      "df[:100]\n",
      "## Get Verbs\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the POS label \"VERB\").\n",
      "```{admonition} Python Review!\n",
      ":class: pythonreview\n",
      "We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of verbs in a single line of code! Closely examine the first line of code below:\n",
      "```\n",
      "verbs = [token.text for token in document if token.pos_ == 'VERB']\n",
      "\n",
      "verbs_tally = Counter(verbs)\n",
      "\n",
      "df = pd.DataFrame(verbs_tally.most_common(), columns=['verb', 'count'])\n",
      "df[:100]\n",
      "## Get Sentences with Keyword\n",
      "spaCy can also identify sentences in a document. To access sentences, we can iterate through `document.sents` and pull out the `.text` of each sentence.\n",
      "We can use spaCy's sentence-parsing capabilities to extract sentences that contain particular keywords, such as in the function below.\n",
      "\n",
      "With the function `find_sentences_with_keyword()`, we will iterate through `document.sents` and pull out any sentence that contains a particular \"keyword.\" Then we will display these sentence with the keywords bolded.\n",
      "import regex\n",
      "from IPython.display import Markdown, display\n",
      "def find_sentences_with_keyword(keyword, document):\n",
      "    \n",
      "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
      "    for sentence in document.sents:\n",
      "        sentence = sentence.text\n",
      "        \n",
      "        #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
      "        if keyword.lower() in sentence.lower():\n",
      "            \n",
      "            #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
      "            sentence = re.sub('\\n', ' ', sentence)\n",
      "            sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
      "            \n",
      "            display(Markdown(sentence))\n",
      "find_sentences_with_keyword(keyword=\"mother\", document=document)\n",
      "## Get Keyword in Context\n",
      "We can also find out about a keyword's more immediate context — its neighboring words to the left and right — and we can fine-tune our search with POS tagging.\n",
      "\n",
      "To do so, we will first create a list of what's called *ngrams*. \"Ngrams\" are any sequence of *n* tokens in a text. They're an important concept in computational linguistics and NLP. (Have you ever played with [Google's *Ngram* Viewer](https://books.google.com/ngrams)?)\n",
      "\n",
      "Below we're going to make a list of *bigrams*, that is, all the two-word combinations from *Lost in the City*. We're going to use these bigrams to find the neighboring words that appear alongside particular keywords.\n",
      "#Make a list of tokens and POS labels from document if the token is a word \n",
      "tokens_and_labels = [(token.text, token.pos_) for token in document if token.is_alpha]\n",
      "#Make a function to get all two-word combinations\n",
      "def get_bigrams(word_list, number_consecutive_words=2):\n",
      "    \n",
      "    ngrams = []\n",
      "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
      "    \n",
      "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
      "    for word_index in range(adj_length_of_word_list):\n",
      "        \n",
      "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
      "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
      "        \n",
      "        #Append this word combo to the master list \"ngrams\"\n",
      "        ngrams.append(ngram)\n",
      "        \n",
      "    return ngrams\n",
      "bigrams = get_bigrams(tokens_and_labels)\n",
      "Let's take a peek at the bigrams:\n",
      "bigrams[5:20]\n",
      "Now that we have our list of bigrams, we're going to make a function `get_neighbor_words()`. This function will return the most frequent words that appear next to a particular keyword. The function can also be fine-tuned to return neighbor words that match a certain part of speech by changing the `pos_label` parameter.\n",
      "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
      "    \n",
      "    neighbor_words = []\n",
      "    keyword = keyword.lower()\n",
      "    \n",
      "    for bigram in bigrams:\n",
      "        \n",
      "        #Extract just the lowercased words (not the labels) for each bigram\n",
      "        words = [word.lower() for word, label in bigram]        \n",
      "        \n",
      "        #Check to see if keyword is in the bigram\n",
      "        if keyword in words:\n",
      "            \n",
      "            for word, label in bigram:\n",
      "                \n",
      "                #Now focus on the neighbor word, not the keyword\n",
      "                if word.lower() != keyword:\n",
      "                    #If the neighbor word matches the right pos_label, append it to the master list\n",
      "                    if label == pos_label or pos_label == None:\n",
      "                        neighbor_words.append(word.lower())\n",
      "    \n",
      "    return Counter(neighbor_words).most_common()\n",
      "get_neighbor_words(\"mother\", bigrams)\n",
      "get_neighbor_words(\"mother\", bigrams, pos_label='ADJ')\n",
      "## Your Turn!\n",
      "Try out `find_sentences_with_keyword()` and `get_neighbor_words` with your own keywords of interest.\n",
      "find_sentences_with_keyword(keyword=\"YOUR KEY WORD\", document=document)\n",
      "get_neighbor_words(keyword=\"YOUR KEY WORD\", bigrams, pos_label=None)\n",
      "## Topic Modeling\n",
      "In the next lessons, we're going to learn about a text analysis method called **topic modeling**.\n",
      "\n",
      "This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "* [Topic Modeling Overview](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html)\n",
      "* [Topic Modeling Set-Up & Installation](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html)\n",
      "* [Topic Modeling with Text Files](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Text-Files.html)\n",
      "* [Topic Modeling with CSV Files](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-CSV.html)\n",
      "* [Topic Modeling & Making a Time Series](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Time-Series.html)\n",
      "## Network Analysis\n",
      "In this lesson, we're going to learn about **network analysis**. Network analysis will help us better understand the complex relationships between groups of people, fictional characters, or other kinds of things.\n",
      "## Why is Network Analysis Useful?\n",
      "import networkx \n",
      "import pandas as pd\n",
      "pd.set_option('max_rows', 400)\n",
      "import matplotlib.pyplot as plt\n",
      "## What Is a Network?\n",
      "\"It was the bastard **Jon Snow** who had taken that from him, him and his fat friend **Sam Tarly**.\"\n",
      "\"Lucky it might be, and red it certainly was, but **Ygritte**’s hair was such a tangle that **Jon** was tempted to ask her if she only brushed it at the changing of the seasons.\"\n",
      "\"**Arya** gave **Gendry** a sideways look. *He said it with me, like **Jon** used to do, back in Winterfell.* She missed **Jon Snow** the most of all her brothers.\"\"\n",
      "\"She will deliver Jaime to King's Landing, and bring Arya and Sansa back to us\"\n",
      "\"Bael the Bard,\" said Jon, remembering the tale that Ygritte had told him in the Frostganfs, the night he'd almost killed her.\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "## Create a Network From a Pandas DataFrame\n",
      "G=networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Draw Simple Network\n",
      "networkx.draw(G)\n",
      "plt.figure(figsize=(8,8))\n",
      "networkx.draw(G, with_labels=True, node_color='skyblue', width=.3, font_size=8)\n",
      "## Calculate Degree\n",
      "Who has the most number of connections in the network?\n",
      "networkx.degree(G)\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "degree_df = pd.DataFrame(G.nodes(data='degree'), columns=['node', 'degree'])\n",
      "degree_df = degree_df.sort_values(by='degree', ascending=False)\n",
      "degree_df\n",
      "num_nodes_to_inspect = 10\n",
      "degree_df[:num_nodes_to_inspect].plot(x='node', y='degree', kind='barh').invert_yaxis()\n",
      "## Calculate Weighted Degree\n",
      "Who has the most number of connections in the network (if you factor in edge weight)?\n",
      "weighted_degrees = dict(networkx.degree(G, weight='Weight'))\n",
      "networkx.set_node_attributes(G, name='weighted_degree', values=weighted_degrees)\n",
      "weighted_degree_df = pd.DataFrame(G.nodes(data='weighted_degree'), columns=['node', 'weighted_degree'])\n",
      "weighted_degree_df = weighted_degree_df.sort_values(by='weighted_degree', ascending=False)\n",
      "weighted_degree_df\n",
      "num_nodes_to_inspect = 10\n",
      "weighted_degree_df[:num_nodes_to_inspect].plot(x='node', y='weighted_degree', color='orange', kind='barh').invert_yaxis()\n",
      "## Calculate Betweenness Centrality Scores\n",
      "Who connects the most other nodes in the network?\n",
      "betweenness_centrality = networkx.betweenness_centrality(G)\n",
      "networkx.set_node_attributes(G, name='betweenness', values=betweenness_centrality)\n",
      "betweenness_df = pd.DataFrame(G.nodes(data='betweenness'), columns=['node', 'betweenness'])\n",
      "betweenness_df = betweenness_df.sort_values(by='betweenness', ascending=False)\n",
      "betweenness_df\n",
      "num_nodes_to_inspect = 10\n",
      "betweenness_df[:num_nodes_to_inspect].plot(x='node', y='betweenness', color='green', kind='barh').invert_yaxis()\n",
      "## Communities\n",
      "from networkx.algorithms import community\n",
      "communities = community.greedy_modularity_communities(G)\n",
      "# Create empty dictionaries\n",
      "modularity_class = {}\n",
      "#Loop through each community in the network\n",
      "for community_number, community in enumerate(communities):\n",
      "    #For each member of the community, add their community number\n",
      "    for name in community:\n",
      "        modularity_class[name] = community_number\n",
      "networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "communities_df = pd.DataFrame(G.nodes(data='modularity_class'), columns=['node', 'modularity_class'])\n",
      "#communities_df['modularity_class'] = communities_df['modularity_class'].astype(str)\n",
      "communities_df = communities_df.sort_values(by='modularity_class', ascending=False)\n",
      "communities_df\n",
      "communities_df[communities_df['modularity_class'] == 4]\n",
      "communities_df[communities_df['modularity_class'] == 3]\n",
      "communities_df[communities_df['modularity_class'] == 2]\n",
      "communities_df[communities_df['modularity_class'] == 1]\n",
      "communities_df[communities_df['modularity_class'] == 0]\n",
      "fig, ax = plt.subplots()\n",
      "communities_df.sample(50).plot(x='modularity_class', y='node', c='modularity_class', colormap='viridis',\n",
      "                               kind='scatter', s=100, marker='*', figsize=(5,10), legend=True, ax=ax)\n",
      "## Neighbors\n",
      "for neighbor in networkx.neighbors(G, 'Tyrion'):\n",
      "    print(neighbor)\n",
      "G.nodes()['Tyrion']\n",
      "G.nodes()['Sansa']\n",
      "## All Network Metrics\n",
      "nodes_df = pd.DataFrame(dict(G.nodes(data=True))).T\n",
      "nodes_df\n",
      "nodes_df.sort_values(by='betweenness', ascending=False)\n",
      "nodes_df.describe()\n",
      "## Draw with NetworkX\n",
      "plt.figure(figsize=(10,8))\n",
      "node_positions = networkx.spring_layout(G, k=.25, scale=10)\n",
      "\n",
      "networkx.draw(G, pos=node_positions , with_labels=True, font_size=10,\n",
      "              node_size=(node_sizes *30),\n",
      "              node_color='skyblue', width=.3)\n",
      "#plt.savefig('got-network.png')\n",
      "## Size By Degree and Only Label Certain Nodes\n",
      "high_degree_nodes = {}    \n",
      "for node in G.nodes():\n",
      "    if G.nodes()[node]['node_size'] > 10:\n",
      "        #set the node name as the key and the label as its value \n",
      "        high_degree_nodes[node] = node\n",
      "plt.figure(figsize=(10,8))\n",
      "node_positions = networkx.spring_layout(G, k=.2, scale=10)\n",
      "\n",
      "networkx.draw(G, pos=node_positions , with_labels=True, font_size=9,\n",
      "              node_size=(node_sizes *30), label='Game of Thrones Network',\n",
      "              node_color='skyblue', width=.3, labels=high_degree_nodes)\n",
      "#plt.savefig('got-network.png')\n",
      "from collections import defaultdict\n",
      "from datetime import datetime\n",
      "import math\n",
      "from operator import itemgetter\n",
      "import os\n",
      "import random\n",
      "import re\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import little_mallet_wrapper as lmw\n",
      "## MALLET Path\n",
      "\n",
      "path_to_mallet = '/Users/mah343/Documents/packages/mallet-2.0.8/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH\n",
      "## Load poetry dataset\n",
      "\n",
      "# 'https://www.kaggle.com/johnhallman/complete-poetryfoundationorg-dataset/download'\n",
      "poetry_path = '/Volumes/Maria\\'s Black Passport/data/kaggle-poem-dataset/kaggle_poem_dataset.csv'  # CHANGE THIS TO YOUR DATASET PATH\n",
      "poetry_df = pd.read_csv(poetry_path)\n",
      "poetry_df.sample(5)\n",
      "print(len(poetry_df.index))\n",
      "training_data = [lmw.process_string(t) for t in poetry_df['Content'].tolist()]\n",
      "training_data = [d for d in training_data if d.strip()]\n",
      "\n",
      "len(training_data)\n",
      "training_data[100]\n",
      "authors = poetry_df['Author'].tolist()\n",
      "\n",
      "len(authors)\n",
      "lmw.print_dataset_stats(training_data)\n",
      "## Train topic model\n",
      "\n",
      "num_topics = 20  # CHANGE THIS TO YOUR PREFERRED NUMBER OF TOPICS\n",
      "\n",
      "output_directory_path = '/Users/mah343/Desktop/lmw-output' # CHANGE THIS TO YOUR OUTPUT DIRECTORY\n",
      "\n",
      "path_to_training_data           = output_directory_path + '/training.txt'\n",
      "path_to_formatted_training_data = output_directory_path + '/mallet.training'\n",
      "path_to_model                   = output_directory_path + '/mallet.model.' + str(num_topics)\n",
      "path_to_topic_keys              = output_directory_path + '/mallet.topic_keys.' + str(num_topics)\n",
      "path_to_topic_distributions     = output_directory_path + '/mallet.topic_distributions.' + str(num_topics)\n",
      "lmw.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "lmw.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Load the topics\n",
      "topics = lmw.load_topic_keys(output_directory_path + '/mallet.topic_keys.20')\n",
      "\n",
      "for i, t in enumerate(topics):\n",
      "    print(i, '\\t', t[:10])\n",
      "topic_distributions = lmw.load_topic_distributions(output_directory_path + '/mallet.topic_distributions.20')\n",
      "\n",
      "len(topic_distributions), len(topic_distributions[0])\n",
      "assert(len(topic_distributions) == len(training_data))\n",
      "for p, d in lmw.get_top_docs(training_data, topic_distributions, 10):\n",
      "    print(round(p, 4), d)\n",
      "    print()\n",
      "## Plot topics by category\n",
      "target_labels = ['John Keats', 'Emily Dickinson', 'William Butler Yeats', 'Christina Rossetti']\n",
      "\n",
      "lmw.plot_categories_by_topics_heatmap(authors,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim=(10,4))\n",
      "target_labels = ['John Keats', 'Emily Dickinson', 'William Butler Yeats', 'Christina Rossetti']\n",
      "\n",
      "for _topic_index in range(0, len(topics)):\n",
      "    \n",
      "    lmw.plot_categories_by_topic_boxplots(authors,\n",
      "                                          topic_distributions,\n",
      "                                          topics, \n",
      "                                          _topic_index,\n",
      "                                          output_path=output_directory_path + '/boxplot.' + str(_topic_index) + '.pdf',\n",
      "                                          target_labels=target_labels,\n",
      "                                          dim=(4,4))\n",
      "## Plot topics over time\n",
      "divided_documents, document_ids, times = lmw.divide_training_data(training_data,\n",
      "                                                                  num_chunks=10)\n",
      "\n",
      "len(divided_documents), len(document_ids), len(times)\n",
      "path_to_divided_training_data           = output_directory_path + '/training.split.txt'\n",
      "path_to_divided_formatted_training_data = output_directory_path + '/mallet.split.training'\n",
      "path_to_divided_topic_distributions     = output_directory_path + '/mallet.split.topic_distributions.' + str(num_topics)\n",
      "lmw.import_data(path_to_mallet,\n",
      "                path_to_divided_training_data,\n",
      "                path_to_divided_formatted_training_data,\n",
      "                divided_documents,\n",
      "                path_to_formatted_training_data)\n",
      "lmw.infer_topics(path_to_mallet,\n",
      "                 path_to_model,\n",
      "                 path_to_divided_formatted_training_data,\n",
      "                 path_to_divided_topic_distributions)\n",
      "topic_distributions = lmw.load_topic_distributions(path_to_divided_topic_distributions)\n",
      "\n",
      "len(topic_distributions), len(topic_distributions[2])\n",
      "for _topic_index in range(0, len(topics)):\n",
      "    lmw.plot_topics_over_time(topic_distributions, topics, times, _topic_index)\n",
      "\n",
      "## Part-of-Speech Tagging & Keyword Extraction\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "In this lesson, we're going to learn about the textual analysis methods **part-of-speech tagging** and **keyword extraction**. These methods will help us computationally parse sentences and better understand words in context.\n",
      "<img src=\"../images/Ada-dep-parse.png\" width=\"100%\", border=2>\n",
      "## Why is Part-of-Speech Tagging Useful?\n",
      "I don't mean to go all [Language Nerd](https://xkcd.com/1443/) on you, but parts of speech are important. Even if they seem kind of boring. As you may have learned in an English class at some point in your life, \"parts of speech\" are the grammatical units of language — such as (in English) nouns, verbs, adjectives, adverbs, pronouns, and prepositions. Each of these parts of speech plays a different role in a sentence.\n",
      "<img src=\"https://imgs.xkcd.com/comics/language_nerd.png\", border=2>\n",
      "\n",
      "By computationally identifying parts of speech, we can start computationally exploring *syntax*, the relationship between words — rather than only focusing on words in isolation, as we did with tf-idf. Though parts of speech may seem pedantic, they help computers (and us) crack at that ever-elusive abstract noun: *meaning*. \n",
      "## spaCy and Natural Language Processing (NLP)\n",
      "To computationally identify parts of speech, we're going to use the natural language processing library spaCy. For a more extensive introduction to NLP and spaCy, see the previous lesson.\n",
      "\n",
      "To parse sentences, spaCy relies on machine learning models that were trained on large amounts of labeled text data. The English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more.\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting nouns, verbs, adjectives, etc., and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
      "\n",
      "To test out spaCy's part-of-speech tagging, we'll begin by processing a sample sentence from Ada Lovelace's obituary:\n",
      "> \"[Charles] Babbage, who called [Ada Lovelace] the “enchantress of numbers,” once wrote that\n",
      "she “has thrown her magical **spell** around the most **abstract** of Sciences and has grasped\n",
      "it with a **force** which few masculine intellects (in our own country at least) could have exerted over it.\n",
      "This sentence makes for an interesting example because it is syntactically complex and because it includes contains difficultly ambiguous words such as \"spell,\" \"abstract,\" and \"force.\"\n",
      "sample = \"\"\"She “has thrown her magical spell around the most abstract of Sciences and has grasped\n",
      "it with a force which few masculine intellects (in our own country at least) could have exerted over it.\"\"\"\n",
      "document = nlp(sample)\n",
      "## spaCy Part-of-Speech Tagging\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
      "| ADP   | adposition                | in, to, during                                |\n",
      "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
      "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
      "| CONJ  | conjunction               | and, or, but                                  |\n",
      "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
      "| DET   | determiner                | a, an, the                                    |\n",
      "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
      "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
      "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
      "| PART  | particle                  | ’s, not,                                      |\n",
      "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
      "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
      "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
      "| SCONJ | subordinating conjunction | if, while, that                               |\n",
      "| SYM   | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :), 😝             |\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "| X     | other                     | sfpksdpsxmsa                                  |\n",
      "| SPACE | space                     |                                               |\n",
      "\n",
      "Above is a POS chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different parts of speech that spaCy can identify as well as their corresponding labels. To quickly see spaCy's POS tagging in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) on our sample `document` with the `style=` parameter set to \"dep\" (short for dependency parsing):\n",
      "#Set some display options for the visualizer\n",
      "options = {\"compact\": True, \"distance\": 90, \"color\": \"yellow\", \"bg\": \"black\", \"font\": \"Gill Sans\"}\n",
      "\n",
      "displacy.render(document, style=\"dep\", options=options)\n",
      "As you can see, spaCy has correctly identified that \"spell\" and \"force\" are nouns in our sample sentence:\n",
      "for token in document:\n",
      "    if token.pos_ == \"NOUN\":\n",
      "        print(token, token.pos_)\n",
      "But if we look at the same words in a different context — in a sentence that I made up — spaCy can identify when these words have changed  grammatical roles and meanings.\n",
      "> You shouldn't **force** someone to learn how to **spell** Babbage. They just need practice. You can't **abstract** it.\n",
      "document = nlp(\"You shouldn't force someone to learn how to spell Babbage. They just need practice. You can't abstract it.\")\n",
      "for token in document:\n",
      "    if token.pos_ == \"VERB\":\n",
      "        print(token, token.pos_)\n",
      "Where previously spaCy had identified \"force\" and \"spell\" as nouns, here spaCy correctly identifies the words \"force,\" \"spell,\" and \"abstract\" as verbs.\n",
      "## Get Part-Of-Speech Tags\n",
      "To get part of speech tags for every word in a document, we have to iterate through all the tokens in the document and pull out the `.pos_` attribute for each token. We can get even finer-grained dependency information with the attribute `.dep_`.\n",
      "\n",
      "for token in document:\n",
      "    print(token.text, token.pos_, token.dep_)\n",
      "## Practicing with *Lost in the City*\n",
      "For the rest of this lesson, we're going to explore Edward P. Jones's short story collection *Lost in the City*.\n",
      "<img src=\"https://mybinder.org/static/images/logo_social.png\" width=\"150\" align=\"left\", border=2> *If you're using this Jupyter notebook in Binder (in the cloud), please uncomment the cell below and work with only the first story from _Lost in the City_. The Binder notebook is currently having issues loading the entire collection.*\n",
      "#file = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "#document = nlp(open(file).read())\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "document = nlp(open(filepath, encoding=\"utf-8\").read())\n",
      "## Get Adjectives\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
      "To extract and count the adjectives in *Lost in the City*, we will follow the same model as above, except we'll add an `if` statement that will pull out words only if their POS label matches \"ADJ.\"\n",
      "```{admonition} Python Review!\n",
      ":class: pythonreview\n",
      "While we demonstrate how to extract parts of speech in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.\n",
      "```\n",
      "Here we make a list of the adjectives identified in *Lost in the City*:\n",
      "adjs = []\n",
      "for token in document:\n",
      "    if token.pos_ == 'ADJ':\n",
      "        adjs.append(token.text)\n",
      "adjs\n",
      "Then we count the unique adjectives in this list with the `Counter()` module:\n",
      "adjs_tally = Counter(adjs)\n",
      "adjs_tally.most_common()\n",
      "Then we make a dataframe from this list:\n",
      "df = pd.DataFrame(adjs_tally.most_common(), columns=['character', 'count'])\n",
      "df[:100]\n",
      "## Get Nouns\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
      "To extract and count nouns, we can follow the same model as above, except we will change our `if` statement to check for POS labels that match \"NOUN\".\n",
      "nouns = []\n",
      "for token in document:\n",
      "    if token.pos_ == 'NOUN':\n",
      "        nouns.append(token.text)\n",
      "\n",
      "nouns_tally = Counter(nouns)\n",
      "\n",
      "df = pd.DataFrame(nouns_tally.most_common(), columns=['noun', 'count'])\n",
      "df[:100]\n",
      "## Get Verbs\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the POS label \"VERB\").\n",
      "```{admonition} Python Review!\n",
      ":class: pythonreview\n",
      "We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of verbs in a single line of code! Closely examine the first line of code below:\n",
      "```\n",
      "verbs = [token.text for token in document if token.pos_ == 'VERB']\n",
      "\n",
      "verbs_tally = Counter(verbs)\n",
      "\n",
      "df = pd.DataFrame(verbs_tally.most_common(), columns=['verb', 'count'])\n",
      "df[:100]\n",
      "## Get Sentences with Keyword\n",
      "spaCy can also identify sentences in a document. To access sentences, we can iterate through `document.sents` and pull out the `.text` of each sentence.\n",
      "We can use spaCy's sentence-parsing capabilities to extract sentences that contain particular keywords, such as in the function below.\n",
      "\n",
      "With the function `find_sentences_with_keyword()`, we will iterate through `document.sents` and pull out any sentence that contains a particular \"keyword.\" Then we will display these sentence with the keywords bolded.\n",
      "import regex\n",
      "def find_sentences_with_keyword(keyword, document):\n",
      "    \n",
      "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
      "    for sentence in document.sents:\n",
      "        sentence = sentence.text\n",
      "        \n",
      "        #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
      "        if keyword.lower() in sentence.lower():\n",
      "            \n",
      "            #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
      "            sentence = re.sub('\\n', ' ', sentence)\n",
      "            sentence = re.sub(f\"{keyword}\", f\"**{keyword}**\", sentence, flags=re.IGNORECASE)\n",
      "            \n",
      "            display(Markdown(sentence))\n",
      "find_sentences_with_keyword(keyword=\"mother\", document=document)\n",
      "## Get Keyword in Context\n",
      "We can also find out about a keyword's more immediate context — its neighboring words to the left and right — and we can fine-tune our search with POS tagging.\n",
      "\n",
      "To do so, we will first create a list of what's called *ngrams*. \"Ngrams\" are any sequence of *n* tokens in a text. They're an important concept in computational linguistics and NLP. (Have you ever played with [Google's *Ngram* Viewer](https://books.google.com/ngrams)?)\n",
      "\n",
      "Below we're going to make a list of *bigrams*, that is, all the two-word combinations from *Lost in the City*. We're going to use these bigrams to find the neighboring words that appear alongside particular keywords.\n",
      "#Make a list of tokens and POS labels from document if the token is a word \n",
      "tokens_and_labels = [(token.text, token.pos_) for token in document if token.is_alpha]\n",
      "#Make a function to get all two-word combinations\n",
      "def get_bigrams(word_list, number_consecutive_words=2):\n",
      "    \n",
      "    ngrams = []\n",
      "    adj_length_of_word_list = len(word_list) - (number_consecutive_words - 1)\n",
      "    \n",
      "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
      "    for word_index in range(adj_length_of_word_list):\n",
      "        \n",
      "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
      "        ngram = word_list[word_index : word_index + number_consecutive_words]\n",
      "        \n",
      "        #Append this word combo to the master list \"ngrams\"\n",
      "        ngrams.append(ngram)\n",
      "        \n",
      "    return ngrams\n",
      "bigrams = get_bigrams(tokens_and_labels)\n",
      "Let's take a peek at the bigrams:\n",
      "bigrams[5:20]\n",
      "Now that we have our list of bigrams, we're going to make a function `get_neighbor_words()`. This function will return the most frequent words that appear next to a particular keyword. The function can also be fine-tuned to return neighbor words that match a certain part of speech by changing the `pos_label` parameter.\n",
      "def get_neighbor_words(keyword, bigrams, pos_label = None):\n",
      "    \n",
      "    neighbor_words = []\n",
      "    keyword = keyword.lower()\n",
      "    \n",
      "    for bigram in bigrams:\n",
      "        \n",
      "        #Extract just the lowercased words (not the labels) for each bigram\n",
      "        words = [word.lower() for word, label in bigram]        \n",
      "        \n",
      "        #Check to see if keyword is in the bigram\n",
      "        if keyword in words:\n",
      "            \n",
      "            for word, label in bigram:\n",
      "                \n",
      "                #Now focus on the neighbor word, not the keyword\n",
      "                if word.lower() != keyword:\n",
      "                    #If the neighbor word matches the right pos_label, append it to the master list\n",
      "                    if label == pos_label or pos_label == None:\n",
      "                        neighbor_words.append(word.lower())\n",
      "    \n",
      "    return Counter(neighbor_words).most_common()\n",
      "get_neighbor_words(\"mother\", bigrams)\n",
      "get_neighbor_words(\"mother\", bigrams, pos_label='ADJ')\n",
      "## Your Turn!\n",
      "Try out `find_sentences_with_keyword()` and `get_neighbor_words` with your own keywords of interest.\n",
      "find_sentences_with_keyword(keyword=\"YOUR KEY WORD\", document=document)\n",
      "get_neighbor_words(keyword=\"YOUR KEY WORD\", bigrams, pos_label=None)\n",
      "## Named Entity Recognition — Cluster Characters\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "!pip install fuzzywuzzy\n",
      "!pip install python-Levenshtein\n",
      "import spacy\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import itertools\n",
      "import networkx \n",
      "from networkx.algorithms.components.connected import connected_components, node_connected_component\n",
      "import itertools\n",
      "from fuzzywuzzy import fuzz\n",
      "import glob\n",
      "from pathlib import Path\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "document = nlp(text)\n",
      "displacy.render(document, style=\"ent\")\n",
      "## Get People (More Accurately)\n",
      "Sometimes spaCy will correctly tag a person as a \"PERSON\" but then later tag the same person as a different entity as an organization (\"ORG\") or a place (\"GPE\").\n",
      "\n",
      "So, to get a more accurate character count, we're going to extract all the named entities that spaCy identified as a \"PERSON\" and then count *any* instance of that entitiy, regardless of its NER label.\n",
      "Extract list of all named entities labeled \"PERSON\":\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified to a CSV file and manually edit by hand\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Re-upload CSV file for accurate list of people\n",
      "spacy_identified_people = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in spacy_identified_people:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe s\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "people_tally = Counter(all_people_matches)\n",
      "character_df = pd.DataFrame(people_tally.most_common())\n",
      "character_df.columns = ['character', 'count']\n",
      "\n",
      "character_df\n",
      "## Cluster By Name and Distance\n",
      "\n",
      "spaCy doesn't know that \"Betsy Ann Morgan\" and \"Betsy Ann\" should be the same person. So we're also going to pair two character names if they're an extremely close match and they occur within 800 characters of one another.\n",
      "from datetime import datetime\n",
      "startTime = datetime.now()\n",
      "\n",
      "aggregated_people = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, person_index in all_people_matches_plus_ids:\n",
      "    for another_person, another_person_index in all_people_matches_plus_ids:\n",
      "        distance = abs(person_index - another_person_index)\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person != another_person:\n",
      "                \n",
      "                if fuzz.partial_ratio(person, another_person) == 100:\n",
      "                    aggregated_people.append((person, another_person))\n",
      "                    \n",
      "print(datetime.now() - startTime)\n",
      "With itertools (slightly faster)\n",
      "from datetime import datetime\n",
      "startTime = datetime.now()\n",
      "\n",
      "aggregated_people = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                if fuzz.partial_ratio(person[0], another_person[0]) == 100:\n",
      "                    aggregated_people.append((person[0], another_person[0]))\n",
      "                    \n",
      "print(datetime.now() - startTime)\n",
      "Make a network!\n",
      "from datetime import datetime\n",
      "startTime = datetime.now()\n",
      "\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "                    \n",
      "print(datetime.now() - startTime)\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'edge_weight'])\n",
      "character_df['character1']=character_df['character_pair'].str[0]\n",
      "character_df['character2']=character_df['character_pair'].str[1]\n",
      "character_df\n",
      "G=networkx.from_pandas_edgelist(character_df, 'character1', 'character2', 'edge_weight')\n",
      "#G.add_weighted_edges_from(character_df)\n",
      "import matplotlib.pyplot as plt\n",
      "networkx.draw(G, with_labels=False, font_weight='bold')\n",
      "!pip install visJS2jupyter\n",
      "!pip install py2cytoscape\n",
      "import visJS2jupyter.visJS_module\n",
      "nodes = list(G.nodes()) # must cast to list to maintain compatibility between nx 1.11 and 2.0\n",
      "edges = list(G.edges())\n",
      "!pip show bokeh\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.models import (BoxZoomTool, Circle, HoverTool,\n",
      "                          MultiLine, Plot, Range1d, ResetTool,)\n",
      "from bokeh.palettes import Spectral4\n",
      "from bokeh.plotting import from_networkx\n",
      "\n",
      "# Prepare Data\n",
      "#G = nx.karate_club_graph()\n",
      "\n",
      "#SAME_CLUB_COLOR, DIFFERENT_CLUB_COLOR = \"black\", \"red\"\n",
      "#edge_attrs = {}\n",
      "\n",
      "# for start_node, end_node, _ in G.edges(data=True):\n",
      "#     edge_color = SAME_CLUB_COLOR if G.nodes[start_node][\"club\"] == G.nodes[end_node][\"club\"] else DIFFERENT_CLUB_COLOR\n",
      "#     edge_attrs[(start_node, end_node)] = edge_color\n",
      "\n",
      "# nx.set_edge_attributes(G, edge_attrs, \"edge_color\")\n",
      "\n",
      "# Show with Bokeh\n",
      "plot = Plot(plot_width=400, plot_height=400,\n",
      "            x_range=Range1d(-1.1, 1.1), y_range=Range1d(-1.1, 1.1))\n",
      "\n",
      "plot.title.text = \"Graph Interaction Demonstration\"\n",
      "\n",
      "node_hover_tool = HoverTool(tooltips=[(\"index\", \"@index\"), (\"club\", \"@club\")])\n",
      "plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n",
      "\n",
      "graph_renderer = from_networkx(G, nx.spring_layout, scale=1, center=(0, 0))\n",
      "\n",
      "graph_renderer.node_renderer.glyph = Circle(size=15, fill_color=Spectral4[0])\n",
      "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"edge_color\", line_alpha=0.8, line_width=1)\n",
      "plot.renderers.append(graph_renderer)\n",
      "\n",
      "output_file(\"interactive_graphs.html\")\n",
      "show(plot)\n",
      "# define the initial positions of the nodes using networkx's spring_layout function, and add to the nodes_dict.\n",
      "pos = networkx.spring_layout(G)\n",
      "\n",
      "nodes_dict = [{\"id\":n,\n",
      "              \"x\":pos[n][0]*300,\n",
      "              \"y\":pos[n][1]*300} for n in nodes]\n",
      "\n",
      "node_map = dict(zip(nodes,range(len(nodes))))  # map to indices for source/target in edges\n",
      "\n",
      "edges_dict = [{\"source\":node_map[edges[i][0]], \"target\":node_map[edges[i][1]], \n",
      "              \"title\":'test'} for i in range(len(edges))]\n",
      "\n",
      "visJS2jupyter.visJS_module.visjs_network(nodes_dict, edges_dict)\n",
      "nodes = G.nodes()\n",
      "edges = G.edges()\n",
      "nodes_dict = [{\"id\":n} for n in nodes]\n",
      "node_map = dict(zip(nodes,range(len(nodes)))) # map to indices for source/target in edges\n",
      "edges_dict = [{\"source\":node_map[edges[i][0]], \"target\":node_map[edges[i][1]], \"title\":'test'} for i in range(len(edges))]\n",
      "visJS_module.visjs_network(nodes_dict, edges_dict, time_stamp=0)\n",
      "With the aggregated character names, we're going to use the Python library networkX to create a cluster of character names.\n",
      "G=networkx.Graph()\n",
      "G.add_edges_from(aggregated_people)\n",
      "people_clusters  = list(connected_components(G))\n",
      "people_clusters = [sorted(cluster, key=len, reverse=True) for cluster in people_clusters]\n",
      "people_clusters\n",
      "def add_clustered_characters(row):\n",
      "    character = row['character']\n",
      "    for cluster in people_clusters:\n",
      "        if character in cluster:\n",
      "            return cluster\n",
      "def clean_and_unpack_cluster(row):\n",
      "    if row['clustered_characters'] == None:\n",
      "        cluster = row['character']\n",
      "    else:\n",
      "        cluster = \" // \".join(row['clustered_characters'])\n",
      "    return cluster\n",
      "character_df['clustered_characters'] = character_df.apply(add_clustered_characters, axis=1)\n",
      "character_df['clustered_characters'] = character_df.apply(clean_and_unpack_cluster, axis=1)\n",
      "character_df\n",
      "Manually edit\n",
      "#character_df.sort_values(by=['clustered_characters', 'count']).to_csv('clustered_characters_draft.csv')\n",
      "#character_df = pd.read_csv('clustered_characters_edited.csv')\n",
      "character_df.groupby('clustered_characters')[['count']].sum().sort_values(by='count', ascending=False).reset_index()\n",
      "## Cluster By Name\n",
      "from fuzzywuzzy import process\n",
      "def cluster_characters(row):\n",
      "    possibilities = process.extract(row['character'], character_df['character'].unique(), scorer=fuzz.partial_ratio)\n",
      "    possibilities = [possible[0] for possible in possibilities if possible[1] == 100]\n",
      "    possibilities = [combo for combo in itertools.combinations(possibilities, 2)]\n",
      "    return possibilities\n",
      "clustered_by_named = character_df.apply(cluster_characters, axis=1)\n",
      "\n",
      "G=networkx.Graph()\n",
      "G.add_edges_from(clustered_by_named)\n",
      "people_clusters  = list(connected_components(G))\n",
      "people_clusters = [sorted(cluster, key=len, reverse=True) for cluster in people_clusters]\n",
      "people_clusters\n",
      "def add_clustered_characters(row):\n",
      "    character = row['character']\n",
      "    for cluster in people_clusters:\n",
      "        if character in cluster:\n",
      "            return cluster\n",
      "def clean_and_unpack_cluster(row):\n",
      "    if row['clustered_characters'] == None:\n",
      "        cluster = row['character']\n",
      "    else:\n",
      "        cluster = \" // \".join(row['clustered_characters'])\n",
      "    return cluster\n",
      "character_df['clustered_characters'] = character_df.apply(add_clustered_characters, axis=1)\n",
      "character_df['clustered_characters'] = character_df.apply(clean_and_unpack_cluster, axis=1)\n",
      "character_df\n",
      "## Topic Modeling — Text Files\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model 378 obituaries published by *The New York Times*. \n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Note: \"uncomment\" means delete the initial `#`*\n",
      "*Note: the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "Before we topic model the *NYT* obituaries, we need to process the text files and prepare them for analysis. The steps below demonstrate how to process texts if your corpus is a collection of separate text files. In the next lesson, we'll demonstrate how to process texts that come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "files\n",
      "## Process Texts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual text file, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "```{admonition} Python Review!\n",
      ":class: python_review\n",
      "Take a moment to study this code and reflect about what's happening here. This is a very common Python pattern! We make an empty list `training_data = []`, then we use a `for` loop to iterate through every file path in the list of file paths, then we `open()` and `.read()` each text file associated with that file path, then we processes `little_mallet_wrapper.process_string()` the text, and finally we `.append()` the processed text to our master list.\n",
      "```\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "Here we extract the relevant part of each file name by using [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem), which conveniently extracts just the last part of the file path without the \".txt\" file extension. Because each file name includes the obituary subject's name as well as the year that the subject died, we're going to use this information as a title or label for each obituary.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very simple list comprehension!*\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "obit_titles\n",
      "## Get Training Data Stats\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "According to this little report, we have 378 documents (or obituaries) that average 1345 words in length.\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed obituary texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Topic Model Output Files\n",
      "Finally, we need to tell Little MALLET Wrapper where to find and output all of our topic modeling results. The code below will set Little MALLET Wrapper up to output your results inside a directory called \"topic-model-output\" and a subdirectory called \"NYT-Obits\", all of which will be inside your current directory.\n",
      "\n",
      "If you'd like to change this output location, simply change `output_directory_path` below.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/NYT-Obits'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "Now we import our training data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Finally, we train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 45 seconds to 1 minute to fully train and complete. If you want, you can look at your Terminal or PowerShell while it's running and see what the model looks like as it trains.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "When the topic model finishes, it will output your results to your `output_directory_path`.\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the *NYT* obituaries, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "\n",
      ">*Take a minute to read through every topic. Reflect on what each topic seems to capture as well as how well you think the topics capture the broad themes of the entire collection. Note any oddities, outliers, or inconsistencies.*\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single obituary in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific obituaries.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "If we look at the 32nd topic distribution in this list of `topic_distributions`, which corresponds to Marilyn Monroe's obituary, we will see a list of 15 probabilities. This  list corresponds to the likelihood that each of the 15 topics exists in Marilyn Monroe's obituary.\n",
      "topic_distributions[32]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 0 \"miss film theater movie broadway films\" has a relatively high probability of existing in Marilyn Monroe's obituary `.202` while Topic 5 \"soviet hitler german germany stalin union\" has a relatively low probability `.002`. This seems to comport with what we know about Marilyn Monroe.\n",
      "obituary_to_check = \"1962-Marilyn-Monroe\"\n",
      "\n",
      "obit_number = obit_titles.index(obituary_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {obit_titles[obit_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[obit_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "The darker squares in this heatmap represent a high probability for the corresponding topic (compared to everyone else in the heatmap) and the lighter squares in the heatmap represent a low probability for the corresponding topic. For example, if you scan across the row of Marilyn Monroe, you can see a dark square for the topic \"miss film theater movie theater broadway\". If you scan across the row of Ada Lovelace, an English mathematician who is now recognized as the first computer programmer, according to her [NYT obituary](https://www.nytimes.com/interactive/2018/obituaries/overlooked-ada-lovelace.html), you can see a dark square for \"university professor research science also\".\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the obituaries that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "\n",
      "Because most of the obituaries in our corpus are pretty long, however, it will be more useful for us to simply display the title of each obituary, rather than the entire document—at least as a first step. To do so, we'll first need to make two dictionaries, which will allow us to find the corresponding obituary title and the original text from a given training document.\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "Then we'll make our own function `display_top_titles_per_topic()` that will display the top text titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "**Topic 0 Label**: Hollywood\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Hollywood.\"\n",
      "**Topic 9**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "**Topic 9 Label**: Global Affairs\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Global Affairs.\"\n",
      "**Topic 8**\n",
      "To display the top 7 obituaries with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=8, number_of_documents=7)\n",
      "**Topic 8 Label**: Authors\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Authors.\"\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original obituary texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "**Topic 8**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=8, number_of_documents=3)\n",
      "## Your Turn!\n",
      "Choose a topic from the results above and write down its corresponding topic number below.\n",
      "**Topic: *Your Number Choice Here***\n",
      "**1.** Display the top 6 obituary titles for this topic.\n",
      "#Your Code Here\n",
      "**2.** Display the topic words in the context of the original obituary for these 6 top titles.\n",
      "#Your Code Here\n",
      "**3.** Come up with a label for your topic and write it below:\n",
      "**Topic Label: *Your Label Here***\n",
      "**Reflection**\n",
      "**4.** Why did you label your topic the way you did? What do you think this topic means in the context of all the *NYT* obituaries?\n",
      "**#**Your answer here\n",
      "**5.** What's another collection of texts that you think might be interesting to topic model? Why?\n",
      "**#**Your answer here\n",
      "## Cluster Characters\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "!pip install fuzzywuzzy\n",
      "!pip install python-Levenshtein\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "import en_core_web_sm\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import networkx \n",
      "from networkx.algorithms.components.connected import connected_components, node_connected_component\n",
      "import itertools\n",
      "from fuzzywuzzy import fuzz\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "nlp = en_core_web_sm.load()\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "document = nlp(text)\n",
      "## Get People (More Accurately)\n",
      "Here's a common spaCy NER scenario:\n",
      "\n",
      "You process your text with spaCy, and you find that the model has correctly tagged a person as a \"PERSON.\" Nice! 🎉\n",
      "\n",
      "But then, paragraphs later, you notice that spaCy has tagged the exact same person as a different entity — perhaps an organization (\"ORG\") or a place (\"GPE\"). Ugh 😫 \n",
      "\n",
      "To get a more accurate character/people count, we're going to extract all the named entities that spaCy identified as a \"PERSON\" and then count *any* instance of that entitiy, regardless of its NER label.\n",
      "\n",
      "Additionally, we're going to output this character list to a CSV file, so we can clean and edit the list by hand (if we wish).\n",
      "Extract list of all named entities labeled \"PERSON\":\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "After manual editing, re-upload CSV file for accurate list of people:\n",
      "spacy_identified_people = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "Count any entity that matches a person in our cleaned list of people. Also extract the [index number](https://spacy.io/usage/linguistic-features#named-entities-101) where the person appears in the document.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in spacy_identified_people:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe 's from character name\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        #Get the character index number from the text\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "people_tally = Counter(all_people_matches)\n",
      "character_df = pd.DataFrame(people_tally.most_common())\n",
      "character_df.columns = ['character', 'count']\n",
      "\n",
      "character_df\n",
      "## Cluster Characters By Name Similarity and Distance\n",
      "\n",
      "spaCy doesn't know that \"Betsy Ann Morgan\" and \"Betsy Ann\" should be the same person. So we're also going to pair two character names if they're an extremely close match and they occur within 750 characters of one another.\n",
      "aggregated_people = []\n",
      "\n",
      "threshold_distance = 750\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                if fuzz.partial_ratio(person[0], another_person[0]) == 100:\n",
      "                    aggregated_people.append((person[0], another_person[0]))\n",
      "G=networkx.Graph()\n",
      "G.add_edges_from(aggregated_people)\n",
      "people_clusters  = list(connected_components(G))\n",
      "people_clusters = [sorted(cluster, key=len, reverse=True) for cluster in people_clusters]\n",
      "people_clusters\n",
      "def add_clustered_characters(row):\n",
      "    character = row\n",
      "    if any(character in cluster for cluster in people_clusters):\n",
      "        for cluster in people_clusters:\n",
      "            if character in cluster:\n",
      "                return \" // \".join(cluster)\n",
      "    else:\n",
      "        return character\n",
      "character_df['clustered_characters'] = character_df['character'].apply(add_clustered_characters)\n",
      "character_df\n",
      "Manually edit\n",
      "#character_df.to_csv('clustered_characters_draft_Lost.csv')\n",
      "#character_df = pd.read_csv('clustered_characters_edited.csv')\n",
      "character_df.groupby('clustered_characters')[['count']].sum().sort_values(by='count', ascending=False).reset_index()\n",
      "## Make a Network of Characters\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 50\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'edge_weight'])\n",
      "character_df['character1']=character_df['character_pair'].str[0]\n",
      "character_df['character2']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['character1', 'character2', 'edge_weight']]\n",
      "character_network[:100]\n",
      "def add_clustered_characters(row):\n",
      "    character = row\n",
      "    if any(character in cluster for cluster in people_clusters):\n",
      "        for cluster in people_clusters:\n",
      "            if character in cluster:\n",
      "                return \" // \".join(cluster)\n",
      "    else:\n",
      "        return character\n",
      "character_network['character1'] = character_network['character1'].apply(add_clustered_characters)\n",
      "character_network['character2'] = character_network['character2'].apply(add_clustered_characters)\n",
      "character_network.to_csv('lost-in-the-city-network.csv', index=False, encoding='utf-8')\n",
      "character_network.groupby(['character1', 'character2'])[['edge_weight']].sum().sort_values(by='edge_weight', ascending=False).reset_index()\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath).read()\n",
      "chunked_text= text.split('\\n')\n",
      "chunked_text[2]\n",
      "number_of_chunks = 5000\n",
      "chunked_text = [text[i:i+number_of_chunks] for i in range(0, len(text), number_of_chunks)]\n",
      "len(chunked_text)\n",
      "import math\n",
      "number_of_chunks = 80\n",
      "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
      "chunked_text = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "## Get People (More Accurately)\n",
      "Extract list of all named entities labeled \"PERSON\":\n",
      "spacy_identified_people = []\n",
      "\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "#pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "After manual editing, re-upload CSV file for accurate list of people:\n",
      "#spacy_identified_people = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "document_length = 0\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for document in chunked_documents:\n",
      "    document_length += len(document.text)\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.text in spacy_identified_people:\n",
      "            person = named_entity.text\n",
      "\n",
      "            #Remove apostrophe 's from character name\n",
      "            person = person.replace(\"’s\", \"\").strip()\n",
      "            #Get the character index number from the text\n",
      "            \n",
      "            person_index =  (document_length - named_entity.start_char)\n",
      "\n",
      "            all_people_matches.append(person)\n",
      "            all_people_matches_plus_ids.append([person, person_index])\n",
      "people_tally = Counter(all_people_matches)\n",
      "character_df = pd.DataFrame(people_tally.most_common())\n",
      "character_df.columns = ['character', 'count']\n",
      "\n",
      "character_df\n",
      "## Make a network!\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 100\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'edge_weight'])\n",
      "character_df['character1']=character_df['character_pair'].str[0]\n",
      "character_df['character2']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['character1', 'character2', 'edge_weight']]\n",
      "character_network[character_network['edge_weight'] > 2]\n",
      "character_network[character_network['edge_weight'] > 2].to_csv('Little-Women-character-network.csv')\n",
      "\n",
      "## Named Entity Recognition — Code\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "This notebook is a streamlined version of a previous lesson on **Named Entity Recognition**. It is primarily intended for those who want to reuse the code without the previous lessons' overview and explanations.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things later on; the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "## Get Named Entities\n",
      "|Type Label|Description|\n",
      "|--- |--- |\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "All the named entities in our `document` can be found in the `document.ents` property. We can access the entity labels by iterating through the `document.ents` with a simple `for` loop and pulling out the `.label_` attribute.\n",
      "for named_entity in document.ents:\n",
      "    print(named_entity, named_entity.label_)\n",
      "## Get People\n",
      "|Type Label|Description|\n",
      "|--- |--- |\n",
      "|PERSON|People, including fictional.|\n",
      "people = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"PERSON\"]\n",
      "people_tally = Counter(people)\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "## Process Long Documents (or Many Documents)\n",
      "Rather than creating a single processed `document` with `nlp()`, we're going to create a bunch of smaller spaCy `documents` with `nlp.pipe()`. The [`nlp.pipe()`](https://spacy.io/usage/processing-pipelines#processing) method is faster and more efficient when we're processing many documents.\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath, encoding=\"utf-8\").read()\n",
      "\n",
      "#Split text on line breaks \n",
      "chunked_text = text.split('\\n')\n",
      "#Process each chunk of text and return a list of processed documents\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "We `open()` and `.read()` our text file, then `.split()` the text on every line break `\\n` and process each chunk of the text as its own document, returning a list of `chunked_documents`.\n",
      "To extract people from all the `chunked_documents`, all we need to do is add one more `for` loop to our code and iterate through every document in `chunked_documents`.\n",
      "people = []\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            people.append(named_entity.text)\n",
      "            \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "places = [named_entity.text  for document in chunked_documents for named_entity in document.ents if named_entity.label_ == \"GPE\"]\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "To write these dataframe to a CSV file, we can use `df.to_csv()`:\n",
      "#df.to_csv(\"people.csv\", encoding='utf-8', index=False)\n",
      "## TF-IDF — Code\n",
      "This notebook is a streamlined version of the previous lesson on **term frequency–inverse document frequency** (tf–idf). It is primarily intended for those who want to reuse the code without the previous lesson's overview and explanations.\n",
      "## Import Libraries\n",
      "To calculate tf-idf scores, we're going to use a Python library called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. \n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 500)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html).\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're using `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "## Calculate tf–idf\n",
      "We need to initialize [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) with our desired parameters. Then we need to plug in the list of text file paths that we want to be calculated with `.fit_transform`.\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "Then we make a dataframe of every word in the collection and its corresponding tf-idf score.\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=10)\n",
      "top_tfidf\n",
      "If you want to change how many top tf-idf scores to show for every text, simply change the `top_n` value.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=20)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Text Analysis\n",
      "- TF-IDF\n",
      "- Topic Models\n",
      "- Named Entity Recognition\n",
      "\n",
      "## Keyword in Context\n",
      "def get_keyword_in_context(keyword, word_list, number_surrounding_words, pos_label=None):\n",
      "    \n",
      "    ngrams = []\n",
      "    words_around_keyword = []\n",
      "    adj_length_of_word_list = len(word_list) - (number_surrounding_words)\n",
      "    \n",
      "    keyword = keyword.lower()\n",
      "    \n",
      "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
      "    for word_index in range(adj_length_of_word_list):\n",
      "        \n",
      "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
      "        ngram = word_list[word_index : word_index + (number_surrounding_words + 1)]\n",
      "        \n",
      "        #Append this word combo to the master list \"ngrams\"\n",
      "        ngrams.append(ngram)\n",
      "    \n",
      "    \n",
      "    for word_label_pair in ngrams:\n",
      "    \n",
      "        words = [word.lower() for word, label in word_label_pair]\n",
      "        labels = [label for word, label in word_label_pair if word != keyword]\n",
      "\n",
      "        if keyword in words:\n",
      "\n",
      "            if pos_label in labels or pos_label == None:\n",
      "                if keyword in words[0]:\n",
      "                    words_around_keyword.append(\" \".join(words[1:]))\n",
      "\n",
      "                elif keyword in words[number_surrounding_words]:\n",
      "                    words_around_keyword.append(\" \".join(words[:number_surrounding_words]))\n",
      "    \n",
      "    words_around_keyword = [word.lower() for word in words_around_keyword]\n",
      "    \n",
      "    return Counter(words_around_keyword).most_common()\n",
      "get_keyword_in_context(\"mother\", tokens_and_labels, number_surrounding_words=2, pos_label='VERB')\n",
      "## Topic Modeling\n",
      "In the next lessons, we're going to learn about a text analysis method called **topic modeling**.\n",
      "\n",
      "This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "* [Topic Modeling Overview](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html)\n",
      "* [Topic Modeling Set-Up & Installation](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html)\n",
      "* [Topic Modeling with Text Files](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Text-Files.html)\n",
      "* [Topic Modeling with CSV Files](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-CSV.html)\n",
      "* [Topic Modeling & Making a Time Series](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Time-Series.html)\n",
      "# Topic Modeling — Time Series\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\" border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model a CSV file of Donald Trump's tweets and plot the fluctuation of topics over time.\n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Remember that \"uncomment\" means delete the initial `#`*\n",
      "*Remember that the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import the `random` module for generating random numbers; the `pandas` library for reading CSV data (we're also changing its default column width display setting); and  [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import random\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_columns\", 50)\n",
      "pd.set_option(\"max_colwidth\", 200)\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From CSV File\n",
      "Before we topic model Donald Trump's tweets, we need to process the tweets and prepare them for analysis. The steps below demonstrate how to process texts if they come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## Trump Tweets\n",
      "This dataset of Donald Trump's tweets is taken from [Trump Twitter Archive](http://www.trumptwitterarchive.com/). To read in the CSV file, we're going to use Pandas.\n",
      "trump_df = pd.read_csv(\"../texts/politics/Trump-Tweets.csv\", encoding='utf-8')\n",
      "trump_df.head()\n",
      "trump_df['text'] = trump_df['text'].astype(str)\n",
      "### Process Trump Tweets\n",
      "training_data = [little_mallet_wrapper.process_string(text, numbers='remove') for text in trump_df['text']]\n",
      "### Get Original Trump Tweets\n",
      "original_trump_tweets = [title for title in trump_df['text']]\n",
      "### Get Dataset Statistics\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 35\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed Reddit post texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Other MALLET File Paths\n",
      "Then we're going to set a file path where we want all our MALLET topic modeling data to be dumped. I'm going to output everything onto my Desktop inside a folder called \"topic-model-output\" and a subfolder specific to the Reddit posts called \"Reddit.\"\n",
      "\n",
      "All the other necessary variables below `output_directory_path` will be automatically created inside this directory.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/Trump-Tweets'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "We're going to import the data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Then we're going to train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 1-1.5 minutes to train and complete. If you want it, you can look at your Terminal or PowerShell and see what the model looks like as it's training.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Display Topics and Top Words\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Display Top Tweets For Topic\n",
      "## Load Topic Distributions\n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "tweet_dict = dict(zip(training_data, original_trump_tweets))\n",
      "def display_top_tweets_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), tweet_dict[document] + \"\\n\")\n",
      "    return\n",
      "display_top_tweets_per_topic(topic_number=0, number_of_documents=5)\n",
      "## Show Topic Words in Context of Full Tweet\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "\n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        original_text = tweet_dict[document]\n",
      "        original_text_lowered = original_text.lower()\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text_lowered:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word.upper()}**\", original_text, flags=re.I)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(original_text))\n",
      "    return\n",
      "display_bolded_topic_words_in_context(topic_number=0, number_of_documents=4)\n",
      "## Plot Topics Over Time\n",
      "## Load Topic Distributions\n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "## Add Topic Distribution Columns\n",
      "Add column with all topic distributions for every tweet\n",
      "trump_df['topic_distributions'] = pd.Series(topic_distributions)\n",
      "Make a separate dataframe with each topic distribution as a separate column\n",
      "topic_distributions_df = trump_df['topic_distributions'].apply(pd.Series)\n",
      "Rename each of those columns with the first four words from the topic\n",
      "topic_distributions_df.columns = [\" \".join(topic[:4]) for topic in topics]\n",
      "Merge that column into the dataframe\n",
      "trump_df = pd.concat([trump_df, topic_distributions_df], axis=1)\n",
      "## Date Formatting For Time Series Plot\n",
      "Convert to datetime\n",
      "trump_df['date'] = pd.to_datetime(trump_df['created_at'])\n",
      "Extract year\n",
      "trump_df['year'] = pd.to_datetime(trump_df['date'].dt.year, format='%Y')\n",
      "Extract year and month\n",
      "trump_df['year-month'] = trump_df['date'].dt.to_period('M')\n",
      "trump_df['Date (by month)'] = [month.to_timestamp() for month in trump_df['year-month']]\n",
      "Set year and month as index\n",
      "trump_df = trump_df.set_index('Date (by month)')\n",
      "## Plot All Topics as Times Series\n",
      "#Ignore warning about opening too many plots\n",
      "import matplotlib as plt\n",
      "plt.rcParams.update({'figure.max_open_warning': 0})\n",
      "\n",
      "for number in range(0,33):\n",
      "    topic_number = number\n",
      "    topic_label = \" \".join(topics[topic_number][:4])\n",
      "    trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title=f'Trump Tweets \\n Topic {topic_number}: {topic_label}\\n ', linewidth=2)\n",
      "## Plot Individual Topics as Time Series\n",
      "**Topic 10**: Hillary Clinton\n",
      "For every Trump tweet, plot the probability of Topic 10 \"hillary crooked clinton court\"\n",
      "topic_number = 10\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df[topic_label].plot(style='.', title='Trump Tweets By Topic')\n",
      "For every month of Trump tweets, plot the average probability of Topic 10 \"hillary crooked clinton court\"\n",
      "topic_number = 10\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 18** Fake News\n",
      "For every month of Trump tweets, plot the average probability of Topic 18 \"news fake media story\"\n",
      "topic_number = 18\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 9** Border Wall\n",
      "For every month of Trump tweets, plot the average probability of Topic 9 \"border wall security country\"\n",
      "topic_number = 9\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 19** Make America Great Again\n",
      "For every month of Trump tweets, plot the average probability of Topic 19 \"great america make https\"\n",
      "topic_number = 19\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "**Topic 13** The Apprentice\n",
      "For every month of Trump tweets, plot the average probability of Topic 13 \"last night celebapprentice http\"\n",
      "topic_number = 13\n",
      "\n",
      "topic_label = \" \".join(topics[topic_number][:4])\n",
      "\n",
      "trump_df.groupby(trump_df.index)[[topic_label]].mean().plot(title='Trump Tweets By Topic', linewidth=2)\n",
      "## Topic Modeling — Overview\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In the next lessons, we're going to learn about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "When we calculated term frequency-inverse document frequency (tf-idf) scores, we identified individual words that were statistically meaningful for certain documents (i.e., they were more likely to show up in certain documents rather than in other ones). When we topic model, we're going to identify *clusters of words* that show up together in statistically meaningful ways throughout the corpus.\n",
      "## Why Are Topic Models Useful?\n",
      "Topic models are useful for understanding collections of texts in their broadest outlines and themes. If you wanted to get a sense of the primary subjects discussed in thousands of journal articles published over multiple decades, then topic modeling might be a good choice. Topic models can also be helpful for looking at the fluctuation of topics and themes over time (this is a time series approach that we will discuss in the next lesson) or finding clusters of texts that contain the same or similar topics. \n",
      "In these lessons, we will train a topic model on a collection of 378 obituaries published by *The New York Times*. As we'll find out, the topics—or clusters of words—that emerge from this analysis are broadly related to art, literature, sports, the military, and politics, among other subjects. These results are pretty satisfying! They seem to capture what the obituaries are \"about.\" Frida Kahlo's obituary contains a significant discussion of her art, while Nella Larsen's obituary discusses her novels, Jackie Robinson's obituary discusses his role in sports, and Ulysses S. Grant's obituary discusses his military career. How does the topic model \"know\" or \"discover\" what these *NYT* obituaries are about?\n",
      "## How LDA Topic Models Work\n",
      "There are numerous kinds of topic models, but the most popular and widely-used kind is latent Dirichlet allocation (LDA). It's so popular, in fact, that \"LDA\" and \"topic model\" are sometimes used interchangeably, even though LDA is only one type.\n",
      "\n",
      "LDA math is pretty complicated. We're not going to get very deep into the math here. But we are going to introduce two important concepts that will help us conceptually understand how LDA topic models work.\n",
      "## 1) LDA is an Unsupervised Algorithm\n",
      "\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "\n",
      "## Topics and Labels\n",
      "What we call a \"topic\" is really just a list of the most probable words for that topic, which are sorted in descending order of probability. The most probable word for the topic is the first word. Here are some sample topics from a previous run on the *NYT* obituaries:\n",
      "\n",
      "✨Topic 3✨\n",
      "\n",
      "['book', 'new', 'wrote', 'work', 'published', 'art', 'years', 'writing', 'world', 'writer', 'books', 'novel', 'paris', 'life', 'story']\n",
      "\n",
      "✨Topic 6✨\n",
      "\n",
      "['president', 'state', 'court', 'roosevelt', 'justice', 'house', 'years', 'law', 'party', 'political', 'republican', 'senator', 'governor', 'democratic', 'campaign']\n",
      "\n",
      "✨Topic 10✨\n",
      "\n",
      "['miss', 'film', 'years', 'theater', 'broadway', 'movie', 'films', 'hollywood', 'stage', 'movies', 'actor', 'new', 'director', 'york', 'show']\n",
      "\n",
      "Topic models start to get more powerful when we, as human researchers, analyze the most probable words for every topic and summarize what these words have in common. This summary can then be used as a descriptive label for the topic. Remember, since an LDA topic model is an unsupervised algorithm, it doesn't know what these words mean in relationship to one another. It's up to us, as the human researchers, to make meaning out of the topics.\n",
      "\n",
      "For example, we might label the topics as follows:\n",
      "\n",
      "✨Topic 3: **Literature**✨\n",
      "\n",
      "['book', 'new', 'wrote', 'work', 'published', 'art', 'years', 'writing', 'world', 'writer', 'books', 'novel', 'paris', 'life', 'story']\n",
      "\n",
      "✨Topic 6: **Politics**✨\n",
      "\n",
      "['president', 'state', 'court', 'roosevelt', 'justice', 'house', 'years', 'law', 'party', 'political', 'republican', 'senator', 'governor', 'democratic', 'campaign']\n",
      "\n",
      "✨Topic 10: **Hollywood**✨\n",
      "\n",
      "['miss', 'film', 'years', 'theater', 'broadway', 'movie', 'films', 'hollywood', 'stage', 'movies', 'actor', 'new', 'director', 'york', 'show']\n",
      "\n",
      "However, even when the topics are relatively straightforward, as these topics seem to be, they're still open to interpretation. For instance, we could just as easily label Topic 3 \"Writing & Art,\" Topic 6 \"Government,\" and Topic 10 \"Film & Theater.\" These subtle changes might shape the direction of our analysis and eventual argument in substantial ways. Topics can be far more ambiguous than the above examples, as well, which makes the business of interpretation even more significant. We'll discuss the ambiguity of topics and topic labels in more depth in the next lessons.\n",
      "\n",
      "## Part-of-Speech Tagging & Keyword Extraction — Code\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "This notebook is a streamlined version of a previous lesson on **part-of-speech tagging**. It is primarily intended for those who want to reuse the code without the previous lessons' overview and explanations.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things later on; the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "`document = nlp(open(filepath, , encoding='utf-8').read())`\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "\n",
      "document = nlp(open(filepath, encoding='utf-8').read())\n",
      "## Get Part-Of-Speech Tags\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| ADJ   | adjective                 | big, old, green, incomprehensible, first      |\n",
      "| ADP   | adposition                | in, to, during                                |\n",
      "| ADV   | adverb                    | very, tomorrow, down, where, there            |\n",
      "| AUX   | auxiliary                 | is, has (done), will (do), should (do)        |\n",
      "| CONJ  | conjunction               | and, or, but                                  |\n",
      "| CCONJ | coordinating conjunction  | and, or, but                                  |\n",
      "| DET   | determiner                | a, an, the                                    |\n",
      "| INTJ  | interjection              | psst, ouch, bravo, hello                      |\n",
      "| NOUN  | noun                      | girl, cat, tree, air, beauty                  |\n",
      "| NUM   | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
      "| PART  | particle                  | ’s, not,                                      |\n",
      "| PRON  | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
      "| PROPN | proper noun               | Mary, John, London, NATO, HBO                 |\n",
      "| PUNCT | punctuation               | ., (, ), ?                                    |\n",
      "| SCONJ | subordinating conjunction | if, while, that                               |\n",
      "| SYM   | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :), 😝             |\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "| X     | other                     | sfpksdpsxmsa                                  |\n",
      "| SPACE | space                     |                                               |\n",
      "\n",
      "To get part of speech tags for every word in a document, we have to iterate through all the tokens in the document and pull out the `.pos_` attribute for each token. We can get even finer-grained dependency information with the attribute `.dep_`.\n",
      "\n",
      "for token in document:\n",
      "    print(token.text, token.pos_, token.dep_)\n",
      "## Get Verbs\n",
      "| POS   | Description               | Examples                                      |\n",
      "|:-----:|:-------------------------:|:---------------------------------------------:|\n",
      "| VERB  | verb                      | run, runs, running, eat, ate, eating          |\n",
      "verbs = [token.text for token in document if token.pos_ == 'VERB']\n",
      "verbs_tally = Counter(verbs)\n",
      "df = pd.DataFrame(verbs_tally.most_common(), columns=['verb', 'count'])\n",
      "df[:100]\n",
      "To write this dataframe to a CSV file, we can use `df.to_csv()`:\n",
      "#df.to_csv(\"Lovelace-verbs.csv\", encoding='utf-8', index=False)\n",
      "## Get Keyword in Context\n",
      "#Make a list of tokens and POS labels from document if the token is a word \n",
      "tokens_and_labels = [(token.text, token.pos_) for token in document if token.is_alpha]\n",
      "def get_keyword_in_context(keyword, word_list, number_surrounding_words, pos_label=None):\n",
      "    \n",
      "    ngrams = []\n",
      "    words_around_keyword = []\n",
      "    adj_length_of_word_list = len(word_list) - (number_surrounding_words)\n",
      "    \n",
      "    keyword = keyword.lower()\n",
      "    \n",
      "    #Loop through numbers from 0 to the (slightly adjusted) length of your word list\n",
      "    for word_index in range(adj_length_of_word_list):\n",
      "        \n",
      "        #Index the list at each number, grabbing the word at that number index as well as N number of words after it\n",
      "        ngram = word_list[word_index : word_index + (number_surrounding_words + 1)]\n",
      "        \n",
      "        #Append this word combo to the master list \"ngrams\"\n",
      "        ngrams.append(ngram)\n",
      "    \n",
      "    \n",
      "    for word_label_pair in ngrams:\n",
      "    \n",
      "        words = [word.lower() for word, label in word_label_pair]\n",
      "        labels = [label for word, label in word_label_pair if word != keyword]\n",
      "\n",
      "        if keyword in words:\n",
      "\n",
      "            if pos_label in labels or pos_label == None:\n",
      "                if keyword in words[0]:\n",
      "                    words_around_keyword.append(\" \".join(words[1:]))\n",
      "\n",
      "                elif keyword in words[number_surrounding_words]:\n",
      "                    words_around_keyword.append(\" \".join(words[:number_surrounding_words]))\n",
      "    \n",
      "    words_around_keyword = [word.lower() for word in words_around_keyword]\n",
      "    \n",
      "    return Counter(words_around_keyword).most_common()\n",
      "get_keyword_in_context(\"computer\", tokens_and_labels, number_surrounding_words=2)\n",
      "get_keyword_in_context(\"computer\", tokens_and_labels, number_surrounding_words=2, pos_label=\"ADJ\")\n",
      "get_keyword_in_context(\"women\", tokens_and_labels, number_surrounding_words=2)\n",
      "## Make a Character Network\n",
      "This notebook demonstrates how to create a social network of people mentioned in a text based on how often people appear within a certain distance of one another.\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import networkx \n",
      "import itertools\n",
      "## Load spaCy Language Model\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "nlp = en_core_web_sm.load()\n",
      "## Character Networks For Shorter Texts\n",
      "This section demonstrates how to create a character network from a text if you can process the entire text with spaCy at one time (mostly shorter texts).\n",
      "## Read in Text File\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "## Process Text\n",
      "document = nlp(text)\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Then output this list of spaCy's identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in my_list_of_characters:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe 's from character name\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        #Get the character index number from the text\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 50 characters of one another, add them to the `edge_list`. To change the number of characters, simply change the `threshold_distance` variable below:\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 50\n",
      "\n",
      "#If two people fall within 50 characters of one another, add them to the edge list\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        \n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Lost-in-the-City-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Lost-in-the-City-network.csv\")\n",
      "## Character Networks For Longer Texts\n",
      "This section demonstrates how to create a character network from a text if you cannot process the entire text with spaCy at one time and need to chunk it into smaller documents (mostly longer texts).\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath).read()\n",
      "## Chunk Text By Number of Chunks\n",
      "To chunk text by a specific number of chunks, choose a `number_of_chunks` value and run the cell below. The current default value is 80 chunks. \n",
      "import math\n",
      "number_of_chunks = 80\n",
      "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
      "chunked_text = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
      "The code above is dividing the total number of characters in the text `len(text)` by the number of chunks you want, then rounding up `math.ceil()` to a whole number. This is calculating the necessary chunk size for the number of chunks you want. The final line iterates through the text and creates slices at the necessary chunk size.\n",
      "## Or Chunk Text By Line Breaks\n",
      "#chunked_text= text.split('\\n')\n",
      "## Process Chunked Text\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document(s)\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "document_length = 0\n",
      "\n",
      "for document in chunked_documents:\n",
      "    document_length += len(document.text)\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.text in my_list_of_characters:\n",
      "            person = named_entity.text\n",
      "\n",
      "            #Remove apostrophe 's from character name\n",
      "            person = person.replace(\"’s\", \"\").strip()\n",
      "            \n",
      "            #Get the character index number from the text\n",
      "            person_index =  (document_length - named_entity.start_char)\n",
      "\n",
      "            all_people_matches.append(person)\n",
      "            all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 100 characters of one another, add them to the `edge_list`.\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 100\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Filter Network By Edge Weights\n",
      "character_network[character_network['Weight'] > 2]\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Little-Women-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Little-Women-network.csv\")\n",
      "## Term Frequency–Inverse Document Frequency\n",
      "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "In this lesson, we're going to learn about a text analysis method called **term frequency–inverse document frequency** (tf–idf). This method will help us identify the most unique words in a document from a given corpus. \n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "### Why is tf–idf Useful?\n",
      "\n",
      "### The Basic Math\n",
      "> `term_frequency * inverse_document_frequency`\n",
      "#### Breaking Down the Formula\n",
      "\n",
      "> `term_frequency = number of times a given word appears in story or text`\n",
      "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n",
      "\n",
      "> `total_number_of_documents / number_of_documents_with_term`\n",
      "\n",
      "...the result of which we're going to take the logarithm of and then add 1\n",
      "\n",
      "> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n",
      "\n",
      "Do you see how if we flipped the fraction — making it `number_of_documents_with_term /  total_number_of_documents`— that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\"\n",
      "### The Formula in Action\n",
      "**\"said\" vs \"pigeons\"**\n",
      "Using this formula, we're going to calculate and compare the tf–idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*.\n",
      "We need the log() function for our calculation, so we're going to import it from the `math` package.\n",
      "from math import log\n",
      "**\"said\"**\n",
      "total_number_of_documents = 14 ##total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 13 ##number of short stories the contain the word \"said\"\n",
      "term_frequency = 47 ##number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**\"pigeons\"**\n",
      "total_number_of_documents = 14 ##total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 2 ##number of short stories the contain the word \"pigeons\"\n",
      "term_frequency = 30 ##number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**tf–idf Scores**\n",
      "\n",
      "\"said\" = 50.48<br>\n",
      "\"pigeons\" = 88.38\n",
      "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf–idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them.\n",
      "## tf–idf with scikit-learn\n",
      "#### Import Libraries\n",
      "We could continue calculating tf–idf scores in this manner — by doing all the math with Python — but conveniently there's a Python library that can calculate tf–idf scores in just a few lines of code.\n",
      "\n",
      "This library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`.\n",
      "!pip install sklearn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 200)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html##basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*.\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "#### Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_files\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "Path(\"../texts/literature/Lost-in-the-City_Stories/04-Young-Lions.txt\").stem\n",
      "text_titles\n",
      "Let's display them to make sure they're correct:\n",
      "text_files, text_titles\n",
      "#### Calculate Word Frequency (Optional Step)\n",
      "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf–idf scores, we can compare these two methods and see how tf–idf helps us find more unique words.\n",
      "\n",
      "(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)\n",
      "##Initialize CountVectorizer with desired parameters\n",
      "count_vectorizer= CountVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "##Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\n",
      "word_count_vector = count_vectorizer.fit_transform(text_files)\n",
      "Check the sciki-learn stop words\n",
      "count_vectorizer.get_stop_words()\n",
      "#Make a DataFrame out of the word count vector and sort by title\n",
      "word_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\n",
      "word_count_df = word_count_df.sort_index()\n",
      "\n",
      "#Add column for number of times each word appears in all the documents\n",
      "word_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()\n",
      "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\").\n",
      "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words.\n",
      "word_count_df.sample(10, axis='columns')\n",
      "Let's zoom in on some specific words.\n",
      "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`\n",
      "def get_top_n_counts(dataframe, top_n=10):\n",
      "    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words.\n",
      "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')\n",
      "top_word_freq = get_top_n_counts(word_count_df)\n",
      "top_word_freq\n",
      "## Calculate tf–idf\n",
      "To calculate tf–idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
      "\n",
      "When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
      "#### Without Smoothing or Normalization (Not Recommended)\n",
      "Remember how we calculated the tf–idf score for the word \"pigeons\" above?\n",
      "total_number_of_documents = 14 \n",
      "number_of_documents_with_term = 2\n",
      "term_frequency = 30\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "\n",
      "term_frequency * inverse_document_frequency\n",
      "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf–idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn.\n",
      "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "#### With Smoothing and Normalization (Defaults/Recommended)\n",
      "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. \n",
      "\n",
      "Smoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all.\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df)\n",
      "top_tfidf\n",
      "#### Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Compare Word Frequency and tf–idf Scores\n",
      "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*.\n",
      "First, we're going to merge the top raw word frequency ranks into our top tf–idf dataframe.\n",
      "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')\n",
      "Then we're going to add a column that calculates the change in rank—that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\n",
      "tfidf_compare = tfidf_compare.fillna(\"*new top word*\")\n",
      "Finally, we're going to make some functions that will alter the style of our Pandas dataframe—such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign.\n",
      "def make_positive(value):\n",
      "    if value != '*new top word*':\n",
      "        if float(value) > 0:\n",
      "            value = f'+{round(value)}'\n",
      "    return value\n",
      "\n",
      "def make_bold(value):\n",
      "    return 'font-weight: bold'\n",
      "\n",
      "def color_df(value):\n",
      "    if value == '*new top word*':\n",
      "        color = 'green'    \n",
      "    else:\n",
      "        value = str(value).replace('+', '')\n",
      "        value = float(value)\n",
      "        \n",
      "        if value < 0:\n",
      "            color = 'red'\n",
      "        elif value > 0:\n",
      "            color = 'green'\n",
      "        else:\n",
      "             color = 'black'        \n",
      "    df_style = f'color: {color}; font-weight: bold'\n",
      "    return df_style\n",
      "Now let's display the dataframe and explore which words have become more significant and which words have become less so.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\n",
      "tfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\n",
      "tfidf_compare_styled\n",
      "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n",
      "\n",
      "```{note}\n",
      "Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below\n",
      "```\n",
      "tfidf_compare[tfidf_compare['word'] == 'said']\n",
      "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer.\n",
      "tfidf_compare[tfidf_compare['word'] == 'pigeons']\n",
      "Words that were not frequent enough to make the top 10 for raw word frequency — such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks — now suddenly show up in the top 10 for tf-idf scores.\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreaming']\n",
      "tfidf_compare[tfidf_compare['word'] == 'gospelteers']\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']\n",
      "## Your Turn!\n",
      "Take a few minutes to explore the dataframe below and then answer the following questions.\n",
      "tfidf_compare\n",
      "**1.** What is the difference between a tf-idf score and raw word frequency?\n",
      "**Your answer here**\n",
      "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?\n",
      "**Your answer here**\n",
      "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**Your answer here**\n",
      "## Topic Models\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Models.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In this lesson, we're going to learn about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "When we calculated term frequency-inverse document frequency (tf-idf) scores, we identified individual words that were statistically meaningful for certain documents (i.e., they were more likely to show up in certain documents rather than in other ones). When we topic model, we're going to identify *clusters of words* that show up together in statistically meaningful ways throughout the corpus.\n",
      "## Why Are Topic Models Useful?\n",
      "Topic models are useful for understanding collections of texts in their broadest outlines and themes. If you wanted to get a sense of the primary subjects discussed in thousands of journal articles published over multiple decades, then topic modeling might be a good choice. Topic models can also be helpful for looking at the fluctuation of topics and themes over time (this is a time series approach that we will discuss in the next lesson) or finding clusters of texts that contain the same or similar topics. \n",
      "In this lesson, we will train a topic model on a collection of 378 obituaries published by *The New York Times*. As we'll find out, the topics—or clusters of words—that emerge from this analysis are broadly related to art, literature, sports, the military, and politics, among other subjects. These results are pretty satisfying! They seem to capture what the obituaries are \"about.\" Frida Kahlo's obituary contains a significant discussion of her art, while Nella Larsen's obituary discusses her novels, Jackie Robinson's obituary discusses his role in sports, and Ulysses S. Grant's obituary discusses his military career. How does the topic model \"know\" or \"discover\" what these *NYT* obituaries are about?\n",
      "## How LDA Topic Models Work\n",
      "There are numerous kinds of topic models, but the most popular and widely-used kind is latent Dirichlet allocation (LDA). It's so popular, in fact, that \"LDA\" and \"topic model\" are sometimes used interchangeably, even though LDA is only one type.\n",
      "\n",
      "LDA math is pretty complicated. We're not going to get very deep into the math here. But we are going to introduce two important concepts that will help us conceptually understand how LDA topic models work.\n",
      "## 1) LDA is an Unsupervised Algorithm\n",
      "\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "\n",
      "## Topics and Labels\n",
      "What we call a \"topic\" is really just a list of the most probable words for that topic, which are sorted in descending order of probability. The most probable word for the topic is the first word. Here are some sample topics from a previous run on the *NYT* obituaries:\n",
      "\n",
      "✨Topic 3✨\n",
      "\n",
      "['book', 'new', 'wrote', 'work', 'published', 'art', 'years', 'writing', 'world', 'writer', 'books', 'novel', 'paris', 'life', 'story']\n",
      "\n",
      "✨Topic 6✨\n",
      "\n",
      "['president', 'state', 'court', 'roosevelt', 'justice', 'house', 'years', 'law', 'party', 'political', 'republican', 'senator', 'governor', 'democratic', 'campaign']\n",
      "\n",
      "✨Topic 10✨\n",
      "\n",
      "['miss', 'film', 'years', 'theater', 'broadway', 'movie', 'films', 'hollywood', 'stage', 'movies', 'actor', 'new', 'director', 'york', 'show']\n",
      "\n",
      "Topic models start to get more powerful when we, as human researchers, analyze the most probable words for every topic and summarize what these words have in common. This summary can then be used as a descriptive label for the topic. Remember, since an LDA topic model is an unsupervised algorithm, it doesn't know what these words mean in relationship to one another. It's up to us, as the human researchers, to make meaning out of the topics.\n",
      "\n",
      "For example, we might label the topics as follows:\n",
      "\n",
      "✨Topic 3: **Literature**✨\n",
      "\n",
      "['book', 'new', 'wrote', 'work', 'published', 'art', 'years', 'writing', 'world', 'writer', 'books', 'novel', 'paris', 'life', 'story']\n",
      "\n",
      "✨Topic 6: **Politics**✨\n",
      "\n",
      "['president', 'state', 'court', 'roosevelt', 'justice', 'house', 'years', 'law', 'party', 'political', 'republican', 'senator', 'governor', 'democratic', 'campaign']\n",
      "\n",
      "✨Topic 10: **Hollywood**✨\n",
      "\n",
      "['miss', 'film', 'years', 'theater', 'broadway', 'movie', 'films', 'hollywood', 'stage', 'movies', 'actor', 'new', 'director', 'york', 'show']\n",
      "\n",
      "However, even when the topics are relatively straightforward, as these topics seem to be, they're still open to interpretation. For instance, we could just as easily label Topic 3 \"Writing & Art,\" Topic 6 \"Government,\" and Topic 10 \"Film & Theater.\" These subtle changes might shape the direction of our analysis and eventual argument in substantial ways. Topics can be far more ambiguous than the above examples, as well, which makes the business of interpretation even more significant. We'll discuss the ambiguity of topics and topic labels in more depth below.\n",
      "## MALLET (For Python)\n",
      "[MALLET](http://mallet.cs.umass.edu/topics.php), short for **MA**chine **L**earning for **L**anguag**E** **T**oolkit, is a software package for  topic modeling and other natural language processing techniques. It's maintained by David Mimno, a Cornell professor in Information Science. Go Big Red!\n",
      "\n",
      "MALLET is great, but it's written in Java, another programming language, which makes it slightly difficult to use. Luckily, another Cornellian, Maria Antoniak, a PhD student in Information Science, has written a Python package that will let us easily use MALLET in this Jupyter notebook. It's called [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper). (A \"wrapper\" is a Python package that makes complicated code easier to use and/or makes code from a different programming language accessible in Python.) \n",
      "## Download and Unzip MALLET\n",
      "The first thing we need to do is download the Java-based MALLET package. To download MALLET, click the following link http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip or find the link on the [MALLET home page](http://mallet.cs.umass.edu/download.php).\n",
      "Once the zip file downloads, unzip it. Make note of where the \"mallet-2.0.8\" directory is located on your computer. I decided to keep my \"mallet-2.0.8\" directory in my Downloads folder, but moving it into your home folder would also be a good idea.\n",
      "## Set MALLET Path\n",
      "<img src=\"../images/mallet-bin.png\" width=100%, border=2>\n",
      "Next we make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "For example: `your-file-path/mallet-2.0.8/bin/mallet`\n",
      "**Change this file path to your own file path!**\n",
      "path_to_mallet = '/Users/melaniewalsh/Downloads/mallet-2.0.8/bin/mallet' \n",
      "## Install and Import Libraries\n",
      "Then we install the Python package [little_mallet_wrapper](https://github.com/maria-antoniak/little-mallet-wrapper). To install it, run `pip install little_mallet_wrapper`, as below.\n",
      "!pip install little_mallet_wrapper\n",
      "Once the package is installed, we will `import` the `little_mallet_wrapper` package into this Jupyter notebook.\n",
      "from little_mallet_test import little_mallet_wrapper\n",
      "import little_mallet_wrapper\n",
      "We're also going to import [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "Before we topic model the *NYT* obituaries, we need to process the text files and prepare them for analysis. The steps below demonstrate how to process texts if your corpus is a collection of separate text files. In the next lesson, we'll demonstrate how to process texts that come from a CSV file.\n",
      "\n",
      "Note: we're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "files\n",
      "## Process Texts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual text file, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very common Python pattern! We make an empty list `training_data = []`, then we use a `for` loop to iterate through every file path in the list of file paths, then we `open()` and `.read()` each text file associated with that file path, then we processes `little_mallet_wrapper.process_string()` the text, and finally we `.append()` the processed text to our master list.*\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "Here we extract the relevant part of each file name by using [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem), which conveniently extracts just the last part of the file path without the \".txt\" file extension. Because each file name includes the obituary subject's name as well as the year that the subject died, we're going to use this information as a title or label for each obituary.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very simple list comprehension!*\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "obit_titles\n",
      "## Get Training Data Stats\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "According to this little report, we have 378 documents (or obituaries) that average 1345 words in length.\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed obituary texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Other MALLET File Paths\n",
      "To set up the rest of the necessary file paths, all we need to do is change the location of the `output_directory_path` in the cell below. This file path is where all of your MALLET topic modeling results are going to be dumped. The other necessary variables below `output_directory_path` will be automatically created inside this directory, so you don't need to change anything else.\n",
      "\n",
      "Below I decided to output everything onto my Desktop in a folder called \"topic-model-output\" and a subfolder specific to the NYT obituaries called \"NYT-Obits.\"\n",
      "#Change to your desired output directory\n",
      "output_directory_path = '/Users/melaniewalsh/Desktop/topic-model-output/NYT-Obits-Test'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "Now we import our training data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "And, finally, we train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 45 seconds to 1 minute to fully train and complete. If you want, you can look at your Terminal or PowerShell while it's running and see what the model looks like as it trains.\n",
      "path_to_mallet = ''\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "When the topic model finishes, it will output your results to your `output_directory_path`, as below.\n",
      "<img src=\"../images/mallet-topic-keys.png\" width=100%, border=2>\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the *NYT* obituaries, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "\n",
      ">*Take a minute to read through every topic. Reflect on what each topic seems to capture as well as how well you think the topics capture the broad themes of the entire collection. Note any oddities, outliers, or inconsistencies.*\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single obituary in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific obituaries.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "If we look at the 32nd topic distribution in this list of `topic_distributions`, which corresponds to Marilyn Monroe's obituary, we will see a list of 15 probabilities. This  list corresponds to the likelihood that each of the 15 topics exists in Marilyn Monroe's obituary.\n",
      "topic_distributions[32]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 0 \"miss film theater movie broadway films\" has a relatively high probability of existing in Marilyn Monroe's obituary `.202` while Topic 5 \"soviet hitler german germany stalin union\" has a relatively low probability `.002`. This seems to comport with what we know about Marilyn Monroe.\n",
      "obituary_to_check = \"1962-Marilyn-Monroe\"\n",
      "\n",
      "obit_number = obit_titles.index(obituary_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {obit_titles[obit_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[obit_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "The darker squares in this heatmap represent a high probability for the corresponding topic (compared to everyone else in the heatmap) and the lighter squares in the heatmap represent a low probability for the corresponding topic. For example, if you scan across the row of Marilyn Monroe, you can see a dark square for the topic \"miss film theater movie theater broadway\". If you scan across the row of Ada Lovelace, an English mathematician who is now recognized as the first computer programmer, according to her [NYT obituary](https://www.nytimes.com/interactive/2018/obituaries/overlooked-ada-lovelace.html), you can see a dark square for \"university professor research science also\".\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the obituaries that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "\n",
      "Because most of the obituaries in our corpus are pretty long, however, it will be more useful for us to simply display the title of each obituary, rather than the entire document—at least as a first step. To do so, we'll first need to make two dictionaries, which will allow us to find the corresponding obituary title and the original text from a given training document.\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "Then we'll make our own function `display_top_titles_per_topic()` that will display the top text titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "### Topic 0\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "### Topic 0 Label: Hollywood\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Hollywood.\"\n",
      "### Topic 9\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "### Topic 9 Label: Global Affairs\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Global Affairs.\"\n",
      "### Topic 8\n",
      "To display the top 7 obituaries with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=10, number_of_documents=8)\n",
      "### Topic 8 Label: Authors\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Authors.\"\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original obituary texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "### Topic 3\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "### Topic 8\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=8, number_of_documents=3)\n",
      "## Your Turn!\n",
      "Pick a topic number. Display the top 6 obituary titles for that topic. Then display the top 6 original obituary texts for that topic with the relevant topic words bolded. Finally, come up with a label for the topic and explain why you chose the label you did.\n",
      "### Topic: *Your Number Choice Here*\n",
      "**1.** Display the top 6 obituary titles for this topic\n",
      "#Your Code Here\n",
      "**2.** Display the topic words in the context of the original obituary for the 6 titles\n",
      "#Your Code Here\n",
      "**3.** Come up with a label for your topic:\n",
      "### Topic Label: *Your Label Here*\n",
      "### Reflection\n",
      "**4.** Why did you label your topic the way you did? What is one alternative label?\n",
      "**#**Your answer here\n",
      "**5.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**#**Your answer here\n",
      "## Topic Modeling — Code\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "This notebook is a streamlined version of a previous lesson on **topic modeling**. It is primarily intended for those who want to reuse the code without the previous lesson's overview and explanations.\n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [\"Topic Modeling-Set Up\"](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed.\n",
      "## Set MALLET Path\n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory. If MALLET is located somewhere else, change the directory path:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "## Import Libraries\n",
      "import little_mallet_wrapper\n",
      "import seabornimport glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "## Process Texts\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "## Get Training Data Stats\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "## Training the Topic Model\n",
      "## Set Number of Topics\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "training_data = training_data\n",
      "## Set Topic Model Output Files\n",
      "If you'd like to change this output location, simply change `output_directory_path` below.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/NYT-Obits'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Display Topics and Top Words\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "## Explore Heatmap of Topics and Texts\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "## Display Top Titles Per Topic\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "## Display Topic Words in Context of Original Text\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "import spacy\n",
      "import re\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import networkx as nx\n",
      "from networkx.algorithms import bipartite\n",
      "pd.set_option(\"display.max_rows\",1000)\n",
      "pd.set_option(\"display.max_columns\",1000)\n",
      "Eliminate blank lines\n",
      "nlp = spacy.load('en')\n",
      "nlp_mango_model = spacy.load('en-mango')\n",
      "mango2 = pd.read_csv('Mango-Characters-2.csv', delimiter='\\t')\n",
      "#count characters and organizations and include label\n",
      "def count_characters_by_match(filepath):\n",
      "    #open and read file with spacy\n",
      "    tokens = nlp(open(filepath).read())\n",
      "    #get the file name\n",
      "    filename = os.path.split(filepath)[-1].replace(\".txt\",\"\")\n",
      "    filename = filename.replace(\"-\",\" \")\n",
      "    story_number = re.match('(^[0-9]+)', filename).group()\n",
      "    filename = re.sub('(^[0-9]+)', '', filename)\n",
      "    filename = f'{filename} ({story_number})'\n",
      "    #get a list of tuples with people/organizations and entity label\n",
      "    character_list = [person for person in mango2['person']]\n",
      "    #character_list.append('I')\n",
      "    #character_list.append('me')\n",
      "    people = [item.text for item in tokens if item.text in character_list]\n",
      "    #clean up the people/organization names by getting rid of plurals, linebreaks, and some punctuation\n",
      "    if len(people) > 0:\n",
      "        people_counts = Counter(people)\n",
      "\n",
      "                # make datalist for pandas dataframe\n",
      "            #datalist = [(filename, people[0], people[1], people_counts) \n",
      "                     #           for ((people[0], people[1]), people_counts) in people_counts.items()]\n",
      "        datalist = [(filename, people_counts[0], people_counts[1], story_number) for people_counts in people_counts.items()]\n",
      "        tmp = pd.DataFrame(datalist)\n",
      "        tmp.columns = ['vignette','person','weight', 'story_number'] \n",
      "        return tmp\n",
      "    else:\n",
      "        return\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "characters_df_by_match = pd.DataFrame()\n",
      "for filepath in filepaths:\n",
      "    characters_df_by_match = characters_df_by_match.append(count_characters_by_match(filepath))\n",
      "characters_df_by_match = characters_df_by_match.replace('I', 'Esperanza')\n",
      "characters_df_by_match = characters_df_by_match.replace('me', 'Esperanza')\n",
      "characters_df_by_match\n",
      "G = nx.from_pandas_edgelist(characters_df_by_match, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df_by_match['person'], bimodal='character')\n",
      "G.add_nodes_from(characters_df_by_match['vignette'], bimodal='vignette')\n",
      "G.remove_node('People')\n",
      "nx.write_gexf(G, 'No-Esperanza-2020-by-match-mango-street-character-network.gexf')\n",
      "## Unimodal\n",
      "top_nodes = set(node for node, detail in G.nodes(data=True) if detail['bimodal']=='character')\n",
      "bottom_nodes = set(G) - top_nodes\n",
      "U = bipartite.weighted_projected_graph(G, bottom_nodes)\n",
      "bottom_nodes\n",
      "nx.write_gexf(U, 'no-Esperanza-2020-by-match-mango-street-character-network-unimodal-vignettes.gexf')\n",
      "top_nodes = set(node for node, detail in G.nodes(data=True) if detail['bimodal']=='character')\n",
      "bottom_nodes = set(G) - top_nodes\n",
      "U = bipartite.weighted_projected_graph(G, top_nodes)\n",
      "nx.write_gexf(U, 'no-Esperanza-2020-by-match-mango-street-character-network-unimodal-characters.gexf')\n",
      "SU = nx.Graph([(u,v,d) for u,v,d in U.edges(data=True) if d ['weight']>2] )\n",
      "#nx.set_node_attributes(G, pd.Series(nodes.story_number, index=nodes.node).to_dict(), 'story_number')\n",
      "\n",
      "\n",
      "## Text Analysis\n",
      "from pathlib import Path\n",
      "def split_text_by_delimiters(input_text_file, output_text_file, first_split_phrase='About the Publisher', last_split_phrase=\"ACKNOWLEDGMENTS\"):\n",
      "    text = open(input_text_file).read()\n",
      "    text_sans_intro = text.split(first_split_phrase)[1].strip()\n",
      "    desired_text = text_sans_intro.split(last_split_phrase)[0].strip()\n",
      "    \n",
      "    output_file = open(f'{output_text_file}', \"w\")\n",
      "    output_file.write(desired_text)\n",
      "split_text_by_delimiters(\"../texts/literature/Lost.txt\", \"../texts/literature/Lost_experiment.txt\")\n",
      "def split_text_into_stories(text_file, directory_name, story_split_delimiter=\"\\n\\n\\n\\n\\n\\n\"):\n",
      "    title = directory_name\n",
      "    text = open(text_file).read()\n",
      "    #text_sans_intro = text.split(first_split_phrase)[1].strip()\n",
      "    #body_text = text_sans_intro.split(last_split_phrase)[0].strip()\n",
      "    stories = text.split(story_split_delimiter)\n",
      "    \n",
      "    story_number = 0\n",
      "    for story in stories:\n",
      "        story_number += 1\n",
      "        story_title = story.split('\\n\\n\\n')\n",
      "        story_title = story_title[0].split()\n",
      "        story_title = \"-\".join(story_title)\n",
      "        story_title = story_title.title()\n",
      "\n",
      "        Path(f\"../texts/literature/{title}_Stories\").mkdir(parents=True, exist_ok=True)\n",
      "        smallfile = open(f'../texts/literature/{title}_Stories/{story_number}-{story_title}.txt', \"w\")\n",
      "        smallfile.write(story)\n",
      "def split_text_into_stories(text_file, directory_name, first_split_phrase='About the Publisher', last_split_phrase=\"ACKNOWLEDGMENTS\", story_split_delimiter=\"\\n\\n\\n\\n\\n\\n\"):\n",
      "    title = directory_name\n",
      "    text = open(text_file).read()\n",
      "    text_sans_intro = text.split(first_split_phrase)[1].strip()\n",
      "    body_text = text_sans_intro.split(last_split_phrase)[0].strip()\n",
      "    stories = body_text.split(story_split_delimiter)\n",
      "    \n",
      "    story_number = 0\n",
      "    for story in stories:\n",
      "        story_number += 1\n",
      "        story_title = story.split('\\n\\n\\n')\n",
      "        story_title = story_title[0].split()\n",
      "        story_title = \"-\".join(story_title)\n",
      "        story_title = story_title.title()\n",
      "\n",
      "        Path(f\"../texts/literature/{title}_Stories\").mkdir(parents=True, exist_ok=True)\n",
      "        smallfile = open(f'../texts/literature/{title}_Stories/{story_number}-{story_title}.txt', \"w\")\n",
      "        smallfile.write(story)\n",
      "split_text_into_stories(text_file=\"../texts/literature/Jones-All-Aunt-Hagar's-Children.txt\", directory_name=\"All-Aunt-Hagar's-Children\")\n",
      "from pathlib import Path\n",
      "\n",
      "story_number = 0\n",
      "stories = lost.split('\\n\\n\\n\\n\\n\\n')\n",
      "for story in stories:\n",
      "    story_number += 1\n",
      "    title = story.split('\\n\\n\\n')\n",
      "    title = title[0].split()\n",
      "    title = \"-\".join(title)\n",
      "    title = title.title()\n",
      "    Path(\"../texts/literature/Lost-in-the-City_Stories\").mkdir(parents=True, exist_ok=True)\n",
      "    smallfile = open(f'../texts/literature/Lost-in-The-City/{story_number}-{title}.txt', \"w\")\n",
      "    smallfile.write(story)\n",
      "def shuffle_text_per_page():\n",
      "    \n",
      "lost = open(\"../texts/literature/Jones-Lost-in-The-City.txt\").read()\n",
      "sample = ' jkj;'\n",
      "re.match('\\s', sample) != None\n",
      "students = ['LeBron James',\n",
      "'Giannis Antetokounmpo',                \n",
      "'Kevin Durant',\n",
      "'Steph Curry',\n",
      "'Kyrie Irving',\n",
      "'Joel Embiid', \n",
      "'Kawhi Leonard', \n",
      "'Paul George', \n",
      "'James Harden', \n",
      "'Kemba Walker', \n",
      "'Khris Middleton', \n",
      "'Anthony Davis', \n",
      "'Nikola Jokić', \n",
      "'Klay Thompson', \n",
      "'Ben Simmons', \n",
      "'Damian Lillard', \n",
      "'Blake Griffin', \n",
      "'Russell Westbrook', \n",
      "'D\\'Angelo Russell', \n",
      "'LaMarcus Aldridge', \n",
      "'Nikola Vučević', \n",
      "'Karl-Anthony Towns', \n",
      "'Kyle Lowry', \n",
      "'Bradley Beal', \n",
      "'Dwyane Wade', \n",
      "'Dirk Nowitzki']\n",
      "shuffle_lines_per_page(students)\n",
      "all_pages = []\n",
      "for index in range(number_of_groups):\n",
      "        group = students[::5]\n",
      "        all_groups.append(page)\n",
      "import random\n",
      "import re\n",
      "novel_lines = []\n",
      "for line in lost.split('\\n'):\n",
      "    if line != '':\n",
      "        novel_lines.append(line)\n",
      "    #line = line.strip('\\n')\n",
      "    #line = re.sub('\\s+', ' ', line)\n",
      "    #if re.match('\\s', line) == None:\n",
      "        #novel_lines.append(line)\n",
      "\n",
      "#random.shuffle(lines)\n",
      "def shuffle_lines_per_page(page, n=5):\n",
      "    \n",
      "    all_pages = []\n",
      "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
      "    for i in range(0, len(page), n):\n",
      "        \n",
      "        page_lines = page[i:i + n]\n",
      "        random.shuffle(page_lines)\n",
      "        all_pages.append(page_lines)\n",
      "    return all_pages\n",
      "shuffle_lines_per_page(novel_lines)\n",
      "print(lines[4])\n",
      "lost\n",
      "\n",
      "\"~Downloads\"\n",
      "~DeprecationWarning\n",
      "## Topic Modeling — CSV Files\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model a CSV file with 5,000 Reddit posts from the subreddit [r/AmITheAsshole](https://www.reddit.com/r/AmItheAsshole/). This is an online forum where people share their personal conflicts and ask the community to judge who's the a**hole in the story.\n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Remember that \"uncomment\" means delete the initial `#`*\n",
      "*Remember that the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import the `random` module for generating random numbers; the `pandas` library for reading CSV data (we're also changing its default column width display setting); and  [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import random\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_colwidth\", 500)\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From CSV File\n",
      "Before we topic model the Reddit posts, we need to process the posts and prepare them for analysis. The steps below demonstrate how to process texts if they come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## Reddit — Am I The Asshole?\n",
      "This dataset of Reddit posts is a sample of a larger dataset published by [Elle O'Brien](https://dvc.org/blog/a-public-reddit-dataset) and [Iterative](https://github.com/iterative/aita_dataset). To read in the CSV file, we're going to use Pandas.\n",
      "reddit_df = pd.read_csv(\"../texts/social-media/reddit-aita-sample.csv\")\n",
      "reddit_df.head()\n",
      "reddit_df['body'] = reddit_df['body'].astype(str)\n",
      "### Process Reddit Posts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we're going to process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual post, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "training_data = [little_mallet_wrapper.process_string(text, numbers='remove') for text in reddit_df['body']]\n",
      "original_texts = [text for text in reddit_df['body']]\n",
      "### Process Reddit Post Titles\n",
      "We're also going to extract the file name for each Reddit post.\n",
      "reddit_titles = [title for title in reddit_df['title']]\n",
      "### Get Dataset Statistics\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed Reddit post texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Other MALLET File Paths\n",
      "Then we're going to set a file path where we want all our MALLET topic modeling data to be dumped. I'm going to output everything onto my Desktop inside a folder called \"topic-model-output\" and a subfolder specific to the Reddit posts called \"Reddit.\"\n",
      "\n",
      "All the other necessary variables below `output_directory_path` will be automatically created inside this directory.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/reddit'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "We're going to import the data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Then we're going to train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 1-1.5 minutes to train and complete. If you want it, you can look at your Terminal or PowerShell and see what the model looks like as it's training.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the Reddit posts, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single Reddit post in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific posts.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "topic_distributions[0]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 6 \"family wedding party want birthday would\" has a relatively high probability of existing in the Reddit post \"AITA for not attending holiday gatherings?\" `.124` while Topic 14 \"dog cats dog house would take\" has a relatively low probability `.006`.\n",
      "reddit_post_to_check = \"AITA for not attending holiday gatherings?\"\n",
      "\n",
      "reddit_post_number = reddit_titles.index(reddit_post_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {reddit_titles[reddit_post_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[reddit_post_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = [\"AITA For putting a dog poop bag sidewalk and picking it up on my way home?\",\n",
      " \"AITA for telling my friend that she shouldn't get a tattoo?\",\n",
      " \"AITA for cutting off all contact with my dad?\",\n",
      " \"AITA for being upset/disappointed my bf went and worked out when we had arranged to go to the cinema for a date without letting me know he was going to be late?\",\n",
      " \"WIBTA if I refused to pay for half of the groceries when my boyfriend eats all of the things I get for myself?\",\n",
      " \"WIBTA if I got students from my school to sign a petition to have a teacher 'talked' too?\",\n",
      " \"AITA if I don't invite my aunt to my birthday\"]\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#target_labels = random.sample(reddit_titles, 7)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(reddit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      #output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (25, 8)\n",
      "                                     )\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the Reddit posts and titles that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "training_data_reddit_titles = dict(zip(training_data, reddit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "We'll make our own function `display_top_titles_per_topic()` that will display the top Reddit post titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_reddit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 Reddit post titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "**Topic 9**\n",
      "To display the top 5 Reddit post titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "**Topic 8**\n",
      "To display the top 7 Reddit posts with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=8, number_of_documents=7)\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original Reddit post texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        reddit_title = f\"**{training_data_reddit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(reddit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original Reddit posts with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "**Topic 8**\n",
      "To display the top 3 original Reddit posts with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=14, number_of_documents=3)\n",
      "## Named Entity Recognition\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "In this lesson, we're going to learn about a text analysis method called **Named Entity Recognition** (NER). This method will help us computationally identify people, places, and things (of various kinds) in a text or collection of texts.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\" width=\"100%\", border=2>\n",
      "## Why is NER Useful?\n",
      "Named Entity Recognition is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters (something we'll do in a later lesson!). Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping the locations (something we'll also do in a later lesson!).\n",
      "## Natural Language Processing (NLP)\n",
      "Named Entity Recognition is a fundamental task in the field of *natural language processing* (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called *spellcheck*? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!\n",
      "\n",
      "Thanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts are now getting eerily good. (If you don't believe me, check out [this app that will autocomplete your sentences](https://transformer.huggingface.co/doc/gpt2-large/qCNMTfzephfZMBkryTNvSRKQ/edit) with GPT-2, a state-of-the-art text generation model. When I ran it, the model generated a mini-lecture from a \"university professor\" that sounds spookily close to home...)\n",
      "<img src=\"../images/GPT-2.png\", border=2>\n",
      "Open-source NLP tools are getting very good, too. We're going to use one of these open-source tools, the Python library `spaCy`, for our Named Entity Recognition tasks in this lesson.\n",
      "## How spaCy Works\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "The screenshot above shows spaCy correctly identifying named entities in Ada Lovelace's *New York Times* obituary (something that we'll test out for ourselves below). How does spaCy know that \"Ada Lovelace\" is a person and that \"1843\" is a date?\n",
      "Well, spaCy doesn't *know*, not for sure anyway. Instead, spaCy is making a very educated guess. This \"guess\" is based on what spaCy has learned about the English language after seeing lots of other examples.\n",
      "That's a colloquial way of saying: spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand). This is similar to our <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html#1)-LDA-is-an-Unsupervised-Algorithm\">topic modeling work</a> from the previous lesson, except our topic model wasn't using labeled data.\n",
      "\n",
      "The English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n",
      "\n",
      "When spaCy identifies people and places in Ada Lovelace's obituary, in other words, the NLP model is actually making *predictions* about the text based on what it has learned about how people and places function in English-language sentences.\n",
      "## NER with spaCy\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things, and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it. There are two ways to load a spaCy language model.\n",
      "**1.** We can import it as a module and then load it from the module, as below:\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "**2.** Or we can load it by name:\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "If you just downloaded the model for the first time, it's advisable to use Option 1. Then you can use the model immediately. Otherwise, you'll likely need to restart your Jupyter kernel (which you can do by clicking Kernel -> Restart Kernel.. in the Jupyter Lab menu).\n",
      "## Create a Processed spaCy Document\n",
      "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
      "\n",
      "In the cell below, we `open()` and `.read()` Ada Lovelace's obituary. Then we run`nlp()` on the `text` and create our `document`.\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "document = nlp(text)\n",
      "## spaCy Named Entities\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "Above is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels. To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):\n",
      "displacy.render(document, style=\"ent\")\n",
      "From a quick glance at the text above, we can see that spaCy is doing quite well with NER. But it's definitely not perfect.\n",
      "\n",
      "Though spaCy correctly identifies \"Ada Lovelace\" as a `PERSON` in the first sentence, just a few sentences later it labels her as a `WORK_OF_ART`. Though spaCy correctly identifies \"London\" as a place `GPE` a few paragraphs down, it incorrectly identifies \"Jacquard\" as a place `GPE`, too (when really \"Jacquard\" is a type of loom, named after [Marie Jacquard](https://en.wikipedia.org/wiki/Jacquard_machine)). \n",
      "\n",
      "This inconsistency is very important to note and keep in mind. If we wanted to use spaCy's NER for a project, it would almost certainly require manual correction and cleaning. And even then it wouldn't be perfect. That's why understanding the limitations of this tool is so crucial. While spaCy's NER can be very good for identifying entities in broad strokes, it can't be relied upon for anything exact and fine-grained — not out of the box anyway.\n",
      "## Get Named Entities\n",
      "All the named entities in our `document` can be found in the `document.ents` property. If we check out `document.ents`, we can see all the entities from Ada Lovelace's obituary.\n",
      "document.ents\n",
      "Each of the named entities in `document.ents` contains [more information about itself](https://spacy.io/usage/linguistic-features#accessing), which we can access by iterating through the `document.ents` with a simple `for` loop. `For` each `named_entity` in `document.ents`, we will extract the `named_entity` and its corresponding `named_entity.label_`.\n",
      "for named_entity in document.ents:\n",
      "    print(named_entity, named_entity.label_)\n",
      "To extract just the named entities that have been identified as `PERSON`, we can add a simple `if` statement into the mix:\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        print(named_entity)\n",
      "## Practicing with *Lost in the City*\n",
      "For the rest of this lesson, we're going to work with Edward P. Jones's short story collection *Lost in the City*, specifically the first story, \"The Girl Who Raised Pigeons.\"\n",
      "filepath = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "text = open(filepath, encoding=\"utf-8\").read()\n",
      "document = nlp(text)\n",
      "## Get People\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "To extract and count the people identified in \"The Girl Who Raised Pigeons,\" we will follow the same model as above, using an `if` statement that will pull out words only if their \"ent\" label matches \"PERSON.\"\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">*While we demonstrate how to extract named entities in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.* \n",
      "Here's the code all together:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "        \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "Here's the code broken up. We make a list of all the people identified in *Lost in the City*:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "people\n",
      "Then we count the unique people in this list with the `Counter()` module:\n",
      "people_tally = Counter(people)\n",
      "people_tally.most_common()\n",
      "Then we make a dataframe from this list with `pd.DataFrame()`:\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "To write this dataframe (or any dataframe!) to a CSV file, we can use `df.to_csv()`. To create a CSV file of character counts, uncomment the cell below:\n",
      "#df.to_csv(\"Lost-in-the-City-characters.csv\", encoding='utf-8', index=False)\n",
      "## Get Places\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "To extract and count places, we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"GPE\" or \"LOC.\" These are the type labels for \"counties cities, states\" and \"locations, mountain ranges, bodies of water.\"\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"GPE\" or named_entity.label_ == \"LOC\":\n",
      "        places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "## Get Streets & Parks\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "To extract and count streets and parks (which show up a lot in *Lost in the City*!), we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"FAC.\" This is the type label for \"buildings, airports, highways, bridges, etc.\"\n",
      "streets = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"FAC\":\n",
      "        streets.append(named_entity.text)\n",
      "\n",
      "streets_tally = Counter(streets)\n",
      "\n",
      "df = pd.DataFrame(streets_tally.most_common(), columns = ['street', 'count'])\n",
      "df\n",
      "## Get Works of Art\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the \"ent\" label \"WORK_OF_ART\").\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of named entities in a single line of code! Closely examine the first line of code below:\n",
      "works_of_art = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"WORK_OF_ART\"]\n",
      "\n",
      "art_tally = Counter(works_of_art)\n",
      "\n",
      "df = pd.DataFrame(art_tally.most_common(), columns = ['work_of_art', 'count'])\n",
      "df\n",
      "## Working with Longer Texts (or Many Texts) 📚\n",
      "On most computers, spaCy should be able to read and process Jones's entire short story collection at once, as we did above:\n",
      "\n",
      "`filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"  \n",
      "text = open(filepath, encoding='utf-8').read()  \n",
      "document = nlp(text)`\n",
      "However, processing an entire book at once is computationally intensive. Because it takes up so much memory, this method won't work in Binder, for example, since Binder only lets us use a certain amount.\n",
      "\n",
      "What's more, spaCy won't be able to process longer texts, such as long novels, with this method. Even on a super fast computer, you'd get an error. (Though you can manually [increase the amount of memory spaCy uses](https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit), it's not advisable).\n",
      "If we want to process a long document, or many documents at once, with spaCy, then here's what we'll do. Rather than creating a single processed `document` with `nlp()`, we're going to create a bunch of smaller spaCy `documents` with `nlp.pipe()`. The [`nlp.pipe()`](https://spacy.io/usage/processing-pipelines#processing) method is faster and more efficient when we're processing many documents.\n",
      "We `open()` and `.read()` our *Lost in the City* text file as we did with the first short story.\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath, encoding=\"utf-8\").read()\n",
      "\n",
      "But then we `.split()` the short story collection on every line break `\\n` and process each chunk of the text as its own document, returning a list of `chunked_documents`.\n",
      "#Split text on line breaks \n",
      "chunked_text = text.split('\\n')\n",
      "#Process each chunk of text and return a list of processed documents\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "To extract people from all the `chunked_documents`, all we need to do is add one more `for` loop to our code and iterate through every document in `chunked_documents`.\n",
      "people = []\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            people.append(named_entity.text)\n",
      "            \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "places = [named_entity.text  for document in chunked_documents for named_entity in document.ents if named_entity.label_ == \"GPE\"]\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "## Get NER in Context\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def get_ner_in_context(keyword, document, desired_ner_labels= False):\n",
      "    \n",
      "    if desired_ner_labels != False:\n",
      "        desired_ner_labels = desired_ner_labels\n",
      "    else:\n",
      "        desired_ner_labels = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']  \n",
      "        \n",
      "    #Iterate through all the sentences in the document and pull out the text of each sentence\n",
      "    for sentence in document.sents:\n",
      "        #process each sentence\n",
      "        sentence_doc = nlp(sentence.text)\n",
      "        for named_entity in sentence_doc.ents:\n",
      "            #Check to see if the keyword is in the sentence (and ignore capitalization by making both lowercase)\n",
      "            if keyword.lower() in named_entity.text.lower()  and named_entity.label_ in desired_ner_labels:\n",
      "                #Use the regex library to replace linebreaks and to make the keyword bolded, again ignoring capitalization\n",
      "                #sentence_text = sentence.text\n",
      "            \n",
      "                sentence_text = re.sub('\\n', ' ', sentence.text)\n",
      "                sentence_text = re.sub(f\"{named_entity.text}\", f\"**{named_entity.text}**\", sentence_text, flags=re.IGNORECASE)\n",
      "\n",
      "                display(Markdown('---'))\n",
      "                display(Markdown(f\"**{named_entity.label_}**\"))\n",
      "                display(Markdown(sentence_text))\n",
      "for document in chunked_documents:\n",
      "    get_ner_in_context('Cassandra', document)\n",
      "## Your Turn!\n",
      "Now it's your turn to take a crack at NER with a whole new text!\n",
      "\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "In this section, you're going to extract and count named entities from Barack Obama's memoir *The Audacity of Hope*. We're exploring Obama's memoir because it's chock full of named entities.\n",
      "Open and read the text file\n",
      "filepath = \"../texts/literature/Obama-The-Audacity-of-Hope.txt\"\n",
      "text = open(filepath, encoding='utf-8').read()\n",
      "To process *The Audacity of Hope* in smaller chunks (if working in Binder or on a computer with memory constraints):\n",
      "chunked_text = text.split('\\n')\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "To process *The Audacity of Hope* all at once (if working on a computer with a larger amount of memory):\n",
      "document = nlp(text)\n",
      "**1.** Choose a named entity from the possible spaCy named entities listed above. Extract, count, and make a dataframe from the most frequent named entities (of the type that you've chosen) in *The Audacity of Hope*. If you need help, study the examples above.\n",
      "#Your Code Here 👇 \n",
      "\n",
      "**2.** What is a result from this NER extraction that conformed to your expectations, that you find obvious or predictable? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**3.** What is a result from this NER extraction that defied your expectations, that you find curious or counterintuitive? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**4.** What's an insight that you might be able to glean about *The Audacity of Hope* based on your NER extraction?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "## Term Frequency–Inverse Document Frequency\n",
      "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "In this lesson, we're going to learn about a text analysis method called **term frequency–inverse document frequency** (tf–idf). This method will help us identify the most unique words in a document from a given corpus. \n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "## Why is tf–idf Useful?\n",
      "\n",
      "## The Basic Math\n",
      "> `term_frequency * inverse_document_frequency`\n",
      "## Breaking Down the Formula\n",
      "\n",
      "> `term_frequency = number of times a given word appears in story or text`\n",
      "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n",
      "\n",
      "> `total_number_of_documents / number_of_documents_with_term`\n",
      "\n",
      "...the result of which we're going to take the logarithm of and then add 1\n",
      "\n",
      "> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n",
      "\n",
      "Do you see how if we flipped the fraction — making it `number_of_documents_with_term /  total_number_of_documents`— that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\"\n",
      "## The Formula in Action\n",
      "**\"said\" vs \"pigeons\"**\n",
      "Using this formula, we're going to calculate and compare the tf–idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*.\n",
      "We need the log() function for our calculation, so we're going to import it from the `math` package.\n",
      "from math import log\n",
      "**\"said\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 13 #number of short stories the contain the word \"said\"\n",
      "term_frequency = 47 #number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**\"pigeons\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 2 #number of short stories the contain the word \"pigeons\"\n",
      "term_frequency = 30 #number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**tf–idf Scores**\n",
      "\n",
      "\"said\" = 50.48<br>\n",
      "\"pigeons\" = 88.38\n",
      "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf–idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them.\n",
      "## tf–idf with scikit-learn\n",
      "## Import Libraries\n",
      "We could continue calculating tf–idf scores in this manner — by doing all the math with Python — but conveniently there's a Python library that can calculate tf–idf scores in just a few lines of code.\n",
      "\n",
      "This library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`.\n",
      "!pip install sklearn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 200)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*.\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_files\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "Path(\"../texts/literature/Lost-in-the-City_Stories/04-Young-Lions.txt\").stem\n",
      "text_titles\n",
      "Let's display them to make sure they're correct:\n",
      "text_files, text_titles\n",
      "## Calculate Word Frequency (Optional Step)\n",
      "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf–idf scores, we can compare these two methods and see how tf–idf helps us find more unique words.\n",
      "\n",
      "(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)\n",
      "#Initialize CountVectorizer with desired parameters\n",
      "count_vectorizer= CountVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\n",
      "word_count_vector = count_vectorizer.fit_transform(text_files)\n",
      "Check the sciki-learn stop words\n",
      "count_vectorizer.get_stop_words()\n",
      "#Make a DataFrame out of the word count vector and sort by title\n",
      "word_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\n",
      "word_count_df = word_count_df.sort_index()\n",
      "\n",
      "#Add column for number of times each word appears in all the documents\n",
      "word_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()\n",
      "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\").\n",
      "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words.\n",
      "word_count_df.sample(10, axis='columns')\n",
      "Let's zoom in on some specific words.\n",
      "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`\n",
      "def get_top_n_counts(dataframe, top_n=10):\n",
      "    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words.\n",
      "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')\n",
      "top_word_freq = get_top_n_counts(word_count_df)\n",
      "top_word_freq\n",
      "## Calculate tf–idf\n",
      "To calculate tf–idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
      "\n",
      "When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
      "### Without Smoothing or Normalization (Not Recommended)\n",
      "Remember how we calculated the tf–idf score for the word \"pigeons\" above?\n",
      "total_number_of_documents = 14 \n",
      "number_of_documents_with_term = 2\n",
      "term_frequency = 30\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "\n",
      "term_frequency * inverse_document_frequency\n",
      "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf–idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn.\n",
      "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. \n",
      "\n",
      "Smoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all.\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Compare Word Frequency and tf–idf Scores\n",
      "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*.\n",
      "First, we're going to merge the top raw word frequency ranks into our top tf–idf dataframe.\n",
      "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')\n",
      "Then we're going to add a column that calculates the change in rank—that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\n",
      "tfidf_compare = tfidf_compare.fillna(\"*new top word*\")\n",
      "Finally, we're going to make some functions that will alter the style of our Pandas dataframe—such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign.\n",
      "def make_positive(value):\n",
      "    if value != '*new top word*':\n",
      "        if float(value) > 0:\n",
      "            value = f'+{round(value)}'\n",
      "    return value\n",
      "\n",
      "def make_bold(value):\n",
      "    return 'font-weight: bold'\n",
      "\n",
      "def color_df(value):\n",
      "    if value == '*new top word*':\n",
      "        color = 'green'    \n",
      "    else:\n",
      "        value = str(value).replace('+', '')\n",
      "        value = float(value)\n",
      "        \n",
      "        if value < 0:\n",
      "            color = 'red'\n",
      "        elif value > 0:\n",
      "            color = 'green'\n",
      "        else:\n",
      "             color = 'black'        \n",
      "    df_style = f'color: {color}; font-weight: bold'\n",
      "    return df_style\n",
      "Now let's display the dataframe and explore which words have become more significant and which words have become less so.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\n",
      "tfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\n",
      "tfidf_compare_styled\n",
      "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n",
      "\n",
      "*Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below*\n",
      "tfidf_compare[tfidf_compare['word'] == 'said']\n",
      "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer.\n",
      "tfidf_compare[tfidf_compare['word'] == 'pigeons']\n",
      "Words that were not frequent enough to make the top 10 for raw word frequency — such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks — now suddenly show up in the top 10 for tf-idf scores.\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreaming']\n",
      "tfidf_compare[tfidf_compare['word'] == 'gospelteers']\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']\n",
      "## Your Turn!\n",
      "Take a few minutes to explore the dataframe below and then answer the following questions.\n",
      "tfidf_compare\n",
      "**1.** What is the difference between a tf-idf score and raw word frequency?\n",
      "**#** Your answer here\n",
      "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?\n",
      "**#** Your answer here\n",
      "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**#** Your answer here\n",
      "## TF-IDF — Code\n",
      "This notebook is a streamlined version of the previous lesson on **term frequency–inverse document frequency** (tf–idf). It is primarily intended for those who want to reuse the code without the previous lesson's overview and explanations.\n",
      "## Import Libraries\n",
      "To calculate tf-idf scores, we're going to use a Python library called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. \n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 500)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html).\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're using `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "## Calculate tf–idf\n",
      "We need to initialize [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) with our desired parameters. Then we need to plug in the list of text file paths that we want to be calculated with `.fit_transform`.\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "Then we make a dataframe of every word in the collection and its corresponding tf-idf score.\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=10)\n",
      "top_tfidf\n",
      "If you want to change how many top tf-idf scores to show for every text, simply change the `top_n` value.\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df, top_n=20)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Network Analysis\n",
      "In this lesson, we're going to learn about **network analysis**. Network analysis will help us better understand the complex relationships between groups of people, fictional characters, or other kinds of things.\n",
      "## Why is Network Analysis Useful?\n",
      "import networkx \n",
      "import pandas as pd\n",
      "pd.set_option('max_rows', 400)\n",
      "import matplotlib.pyplot as plt\n",
      "## What Is a Network?\n",
      "\"It was the bastard **Jon Snow** who had taken that from him, him and his fat friend **Sam Tarly**.\"\n",
      "\"Lucky it might be, and red it certainly was, but **Ygritte**’s hair was such a tangle that **Jon** was tempted to ask her if she only brushed it at the changing of the seasons.\"\n",
      "\"**Arya** gave **Gendry** a sideways look. *He said it with me, like **Jon** used to do, back in Winterfell.* She missed **Jon Snow** the most of all her brothers.\"\"\n",
      "\"She will deliver Jaime to King's Landing, and bring Arya and Sansa back to us\"\n",
      "\"Bael the Bard,\" said Jon, remembering the tale that Ygritte had told him in the Frostganfs, the night he'd almost killed her.\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "## Create a Network From a Pandas DataFrame\n",
      "G=networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Draw Simple Network\n",
      "networkx.draw(G)\n",
      "plt.figure(figsize=(8,8))\n",
      "networkx.draw(G, with_labels=True, node_color='skyblue', width=.3, font_size=8)\n",
      "## Calculate Degree\n",
      "Who has the most number of connections in the network?\n",
      "networkx.degree(G)\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "degree_df = pd.DataFrame(G.nodes(data='degree'), columns=['node', 'degree'])\n",
      "degree_df = degree_df.sort_values(by='degree', ascending=False)\n",
      "degree_df\n",
      "num_nodes_to_inspect = 10\n",
      "degree_df[:num_nodes_to_inspect].plot(x='node', y='degree', kind='barh').invert_yaxis()\n",
      "## Calculate Weighted Degree\n",
      "Who has the most number of connections in the network (if you factor in edge weight)?\n",
      "weighted_degrees = dict(networkx.degree(G, weight='Weight'))\n",
      "networkx.set_node_attributes(G, name='weighted_degree', values=weighted_degrees)\n",
      "weighted_degree_df = pd.DataFrame(G.nodes(data='weighted_degree'), columns=['node', 'weighted_degree'])\n",
      "weighted_degree_df = weighted_degree_df.sort_values(by='weighted_degree', ascending=False)\n",
      "weighted_degree_df\n",
      "num_nodes_to_inspect = 10\n",
      "weighted_degree_df[:num_nodes_to_inspect].plot(x='node', y='weighted_degree', color='orange', kind='barh').invert_yaxis()\n",
      "## Calculate Betweenness Centrality Scores\n",
      "Who connects the most other nodes in the network?\n",
      "betweenness_centrality = networkx.betweenness_centrality(G)\n",
      "networkx.set_node_attributes(G, name='betweenness', values=betweenness_centrality)\n",
      "betweenness_df = pd.DataFrame(G.nodes(data='betweenness'), columns=['node', 'betweenness'])\n",
      "betweenness_df = betweenness_df.sort_values(by='betweenness', ascending=False)\n",
      "betweenness_df\n",
      "num_nodes_to_inspect = 10\n",
      "betweenness_df[:num_nodes_to_inspect].plot(x='node', y='betweenness', color='green', kind='barh').invert_yaxis()\n",
      "## Communities\n",
      "from networkx.algorithms import community\n",
      "communities = community.greedy_modularity_communities(G)\n",
      "# Create empty dictionaries\n",
      "modularity_class = {}\n",
      "#Loop through each community in the network\n",
      "for community_number, community in enumerate(communities):\n",
      "    #For each member of the community, add their community number\n",
      "    for name in community:\n",
      "        modularity_class[name] = community_number\n",
      "networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "communities_df = pd.DataFrame(G.nodes(data='modularity_class'), columns=['node', 'modularity_class'])\n",
      "#communities_df['modularity_class'] = communities_df['modularity_class'].astype(str)\n",
      "communities_df = communities_df.sort_values(by='modularity_class', ascending=False)\n",
      "communities_df\n",
      "communities_df[communities_df['modularity_class'] == 4]\n",
      "communities_df[communities_df['modularity_class'] == 3]\n",
      "communities_df[communities_df['modularity_class'] == 2]\n",
      "communities_df[communities_df['modularity_class'] == 1]\n",
      "communities_df[communities_df['modularity_class'] == 0]\n",
      "fig, ax = plt.subplots()\n",
      "communities_df.sample(50).plot(x='modularity_class', y='node', c='modularity_class', colormap='viridis',\n",
      "                               kind='scatter', s=100, marker='*', figsize=(5,10), legend=True, ax=ax)\n",
      "## Neighbors\n",
      "for neighbor in networkx.neighbors(G, 'Tyrion'):\n",
      "    print(neighbor)\n",
      "G.nodes()['Tyrion']\n",
      "G.nodes()['Sansa']\n",
      "## All Network Metrics\n",
      "nodes_df = pd.DataFrame(dict(G.nodes(data=True))).T\n",
      "nodes_df\n",
      "nodes_df.sort_values(by='betweenness', ascending=False)\n",
      "nodes_df.describe()\n",
      "## Draw with NetworkX\n",
      "plt.figure(figsize=(10,8))\n",
      "node_positions = networkx.spring_layout(G, k=.25, scale=10)\n",
      "\n",
      "networkx.draw(G, pos=node_positions , with_labels=True, font_size=10,\n",
      "              node_size=(node_sizes *30),\n",
      "              node_color='skyblue', width=.3)\n",
      "#plt.savefig('got-network.png')\n",
      "## Size By Degree and Only Label Certain Nodes\n",
      "high_degree_nodes = {}    \n",
      "for node in G.nodes():\n",
      "    if G.nodes()[node]['node_size'] > 10:\n",
      "        #set the node name as the key and the label as its value \n",
      "        high_degree_nodes[node] = node\n",
      "plt.figure(figsize=(10,8))\n",
      "node_positions = networkx.spring_layout(G, k=.2, scale=10)\n",
      "\n",
      "networkx.draw(G, pos=node_positions , with_labels=True, font_size=9,\n",
      "              node_size=(node_sizes *30), label='Game of Thrones Network',\n",
      "              node_color='skyblue', width=.3, labels=high_degree_nodes)\n",
      "#plt.savefig('got-network.png')\n",
      "import spacy\n",
      "import re\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import networkx as nx\n",
      "pd.set_option(\"display.max_rows\",1000)\n",
      "pd.set_option(\"display.max_columns\",1000)\n",
      "!python -m spacy download es_core_news_md\n",
      "#get multilingual support for Spanish and English\n",
      "!python -m spacy download xx_ent_wiki_sm\n",
      "!python -m spacy download en_core_web_sm\n",
      "Eliminate blank lines\n",
      "with open('The-House-on-Mango-Street-No-Blank-Lines.txt', 'w') as file_write:\n",
      "    with open('The-House-on-Mango-Street.txt','r') as file_read:\n",
      "        for line in file_read:\n",
      "            if not line.isspace():\n",
      "                file_write.write(line)\n",
      "file = open('The-House-on-Mango-Street.txt', 'r')\n",
      "file.readlines()\n",
      "\n",
      "with open('The-House-on-Mango-Street.txt') as file_object:\n",
      "    file = file_object.read()\n",
      "    file = file.split('\\n')\n",
      "file = [line for line in file if line.strip() != '']\n",
      "with open('The-House-on-Mango-Street-No-Blank-Lines.txt', 'w') as file_object:\n",
      "    for line in file:\n",
      "        file_object.writelines(line)\n",
      "for line in file:\n",
      "    print(line)\n",
      "nlp = spacy.load('en')\n",
      "nlp_mango_model = spacy.load('en-mango')\n",
      "nlp_full_mango_model = spacy.load('en-full-mango')\n",
      "doc = nlp(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "doc_mango = nlp_mango_model(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "doc_full_mango = nlp_full_mango_model(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "from spacy import displacy\n",
      "from spacy.tokenizer import Tokenizer\n",
      "from spacy.lang.en import English\n",
      "nlp = English()\n",
      "tokenizer = Tokenizer(nlp.vocab)\n",
      "tokens = tokenizer(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "tokens\n",
      "displacy.render(doc, style=\"ent\")\n",
      "displacy.render(doc, style=\"ent\")\n",
      "displacy.render(doc_full_mango, style=\"ent\")\n",
      "displacy.render(doc_mango_model, style=\"ent\")\n",
      "displacy.render(doc, style=\"ent\")\n",
      "doc\n",
      "people = [(named_entity.text, named_entity.label_) for named_entity in doc.ents if named_entity.label_ == 'PERSON']\n",
      "for named_entity in doc.ents:\n",
      "    print(named_entity.text, named_entity.label_)\n",
      "people\n",
      "for tokens in doc.ents:\n",
      "    print(tokens.text[:10])\n",
      "\"\"\"Notes to myself\n",
      "\n",
      ".ents retruns entities, which comes with the attributes .text and .label_\"\n",
      "\n",
      "\"\"\"\n",
      "def remove_whitespace_entities(doc):\n",
      "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
      "    return doc\n",
      "nlp.add_pipe(remove_whitespace_entities, after='ner')\n",
      "#count characters and organizations and include label\n",
      "def count_characters(filepath):\n",
      "            #open and read file with spacy\n",
      "            tokens = nlp(open(filepath).read())\n",
      "            #get the file name\n",
      "            filename = os.path.split(filepath)[-1].replace(\".txt\",\"\")\n",
      "            filename = filename.replace(\"-\",\" \")\n",
      "            #get a list of tuples with people/organizations and entity label\n",
      "            people = [(item.text, item.label_) for item in tokens.ents if item.label_ == 'ORG' or item.label_ == 'PERSON']\n",
      "            #clean up the people/organization names by getting rid of plurals, linebreaks, and some punctuation\n",
      "            if len(people) > 0:\n",
      "                people = [((re.sub('([0-9])|’s|’|—|\\.|\\n*', '', person[0])), person[1]) for person in people]\n",
      "                #get rid of people/organizations that begin with lowecase letters unless they start with 'the' or 'and'\n",
      "                people = [(person[0], person[1]) for person in people if re.match('(^[a-z])', person[0]) == None or re.match('^(the)', person[0]) != None or re.match('^(and)', person[0]) != None]\n",
      "                #change people/organizations names to title case\n",
      "                people = [(person[0].title(), person[1]) for person in people]\n",
      "                #count tuples\n",
      "                people_counts = Counter(people)\n",
      "                    # make datalist for pandas dataframe\n",
      "                #datalist = [(filename, people[0], people[1], people_counts) \n",
      "                 #           for ((people[0], people[1]), people_counts) in people_counts.items()]\n",
      "                datalist = [(filename, people_counts[0][0], people_counts[1]) for people_counts in people_counts.items()]\n",
      "                tmp = pd.DataFrame(datalist)\n",
      "                tmp.columns = ['vignette','person','weight'] \n",
      "                return tmp\n",
      "            else:\n",
      "                return \n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "characters_df = pd.DataFrame()\n",
      "for filepath in filepaths:\n",
      "    characters_df = characters_df.append(count_characters(filepath))\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "for filepath in filepaths:\n",
      "    filename = os.path.split(filepath)\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "for filepath in filepaths:\n",
      "    filename = os.path.split(filepath)[-1].replace(\".txt\",\"\").strip()\n",
      "    print(filename)\n",
      "characters_df\n",
      "characters_df = characters_df.replace('I', 'Esperanza')\n",
      "characters_df = characters_df.replace('Meme', 'Meme Ortiz')\n",
      "characters_df_by_match.groupby(['person']).sum().sort_values(by='weight', ascending=False).to_csv('Mango-Characters-2.csv')\n",
      "characters_df.groupby(['person']).sum().sort_values(by='weight', ascending=False)\n",
      "characters_df_by_match.groupby(['person', 'vignette']).sum().sort_values(by=\"weight\", ascending=False).reset_index()\n",
      "characters_df.groupby(['person', 'vignette']).sum().sort_values(by=\"weight\", ascending=False).reset_index()\n",
      "G = nx.from_pandas_edgelist(characters_df_by_match, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df_by_match['person'], bimodal='character', category='category')\n",
      "G.add_nodes_from(characters_df_by_match['vignette'], bimodal='vignette')\n",
      "#import matplotlib as plt\n",
      "# fig, ax = plt.subplots(1, 1, figsize=(8, 6));\n",
      "# nx.draw_networkx(G, ax=ax)\n",
      "nx.write_gexf(G, '2020-by-match-mango-street-character-network.gexf')\n",
      "G = nx.from_pandas_edgelist(characters_df, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df['person'], bimodal='character', category='category')\n",
      "G.add_nodes_from(characters_df['vignette'], bimodal='vignette')\n",
      "#import matplotlib as plt\n",
      "# fig, ax = plt.subplots(1, 1, figsize=(8, 6));\n",
      "# nx.draw_networkx(G, ax=ax)\n",
      "nx.write_gexf(G, '2020-mango-street-character-network.gexf')\n",
      "mango = pd.read_csv('Mango-Characters.csv', delimiter='\\t')\n",
      "mango2 = pd.read_csv('Mango-Characters-2.csv', delimiter='\\t')\n",
      "tokens = nlp(open('The-House-on-Mango-Street-No-Blank-Lines.txt').read())\n",
      "    #get the file name\n",
      "sample = '12 fjakdlaf;j'\n",
      "re.match('(^[0-9]+)', sample).group()\n",
      "#count characters and organizations and include label\n",
      "def count_characters_by_match(filepath):\n",
      "    #open and read file with spacy\n",
      "    tokens = nlp(open(filepath).read())\n",
      "    #get the file name\n",
      "    filename = os.path.split(filepath)[-1].replace(\".txt\",\"\")\n",
      "    filename = filename.replace(\"-\",\" \")\n",
      "    story_number = re.match('(^[0-9]+)', filename).group()\n",
      "    filename = re.sub('(^[0-9]+)', '', filename)\n",
      "    filename = f'{filename} ({story_number})'\n",
      "    #get a list of tuples with people/organizations and entity label\n",
      "    character_list = [person for person in mango2['person']]\n",
      "    character_list.append('I')\n",
      "    character_list.append('me')\n",
      "    people = [item.text for item in tokens if item.text in character_list]\n",
      "    #clean up the people/organization names by getting rid of plurals, linebreaks, and some punctuation\n",
      "    if len(people) > 0:\n",
      "        people_counts = Counter(people)\n",
      "\n",
      "                # make datalist for pandas dataframe\n",
      "            #datalist = [(filename, people[0], people[1], people_counts) \n",
      "                     #           for ((people[0], people[1]), people_counts) in people_counts.items()]\n",
      "        datalist = [(filename, people_counts[0], people_counts[1], story_number) for people_counts in people_counts.items()]\n",
      "        tmp = pd.DataFrame(datalist)\n",
      "        tmp.columns = ['vignette','person','weight', 'story_number'] \n",
      "        return tmp\n",
      "    else:\n",
      "        return\n",
      "filepaths = sorted(glob.glob('/Users/melaniewalsh/dissertation/draft/3.Cisneros/other-materials/digital/vignettes/*.txt'))\n",
      "characters_df_by_match = pd.DataFrame()\n",
      "for filepath in filepaths:\n",
      "    characters_df_by_match = characters_df_by_match.append(count_characters_by_match(filepath))\n",
      "characters_df_by_match = characters_df_by_match.replace('I', 'Esperanza')\n",
      "characters_df_by_match = characters_df_by_match.replace('me', 'Esperanza')\n",
      "characters_df_by_match.sample(40)\n",
      "G = nx.from_pandas_edgelist(characters_df_by_match, source='person', target='vignette', edge_attr='weight')\n",
      "G.add_nodes_from(characters_df_by_match['person'], bimodal='character')\n",
      "G.add_nodes_from(characters_df_by_match['vignette'], bimodal='vignette')\n",
      "#nx.set_node_attributes(G, pd.Series(nodes.story_number, index=nodes.node).to_dict(), 'story_number')\n",
      "G.remove_node('People')\n",
      "#import matplotlib as plt\n",
      "# fig, ax = plt.subplots(1, 1, figsize=(8, 6));\n",
      "# nx.draw_networkx(G, ax=ax)\n",
      "nx.write_gexf(G, '2020-by-match-mango-street-character-network.gexf')\n",
      "\n",
      "\n",
      "for item in count.items():\n",
      "    print(item[1])\n",
      "characters_df_by_match\n",
      "\n",
      "character_list = [person for person in mango['person']]\n",
      "\n",
      "character_list.append('I')\n",
      "\n",
      "character_list.append('me')\n",
      "characters_df_by_match.to_csv('Mango-Character-By-Match.csv')\n",
      "\n",
      "## Topic Modeling — Set Up\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "\n",
      "This page describes how to set up the packages and programs that you'll need if you want to start topic modeling on your own computer. If you want to topic model without installing anything, however, you can skip ahead and explore these Jupyter notebook topic modeling lessons in the cloud. They notebooks already have the necessary requirements installed:\n",
      "- [Topic Modeling - Text Files](https://mybinder.org/v2/gh/melaniewalsh/Intro-Cultural-Analytics/master?filepath=Website-Content%2FText-Analysis%2FTopic-Modeling-Text-Files.ipynb)\n",
      "- [Topic Modeling - CSV Files](https://mybinder.org/v2/gh/melaniewalsh/Intro-Cultural-Analytics/master?filepath=Website-Content%2FText-Analysis%2FTopic-Modeling-CSV.ipynb)\n",
      "- [Topic Modeling - Time-Series](https://mybinder.org/v2/gh/melaniewalsh/Intro-Cultural-Analytics/master?filepath=Website-Content%2FText-Analysis%2FTopic-Modeling-Time-Series.ipynb)\n",
      "## MALLET & Little MALLET Wrapper\n",
      "For our topic modeling analysis, we're going to use a tool called [MALLET](http://mallet.cs.umass.edu/topics.php). MALLET, short for **MA**chine **L**earning for **L**anguag**E** **T**oolkit, is a software package for  topic modeling and other natural language processing techniques. It's maintained by David Mimno, a Cornell professor in Information Science. Go Big Red!\n",
      "\n",
      "MALLET is great, but it's written in Java, another programming language, which means that we have to install Java before we can use it. It also means that MALLET isn't typically ideal for Python and Jupyter notebooks.\n",
      "\n",
      "Luckily, another Cornellian, Maria Antoniak, a PhD student in Information Science, has written a conveinet Python package that will allow us to use MALLET in this Jupyter notebook after we download and install Java. This package is called [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper).\n",
      "\n",
      "Note: A \"wrapper\" is a Python package that makes complicated code easier to use and/or makes code from a different programming language accessible in Python.\n",
      "## Download and Install Java Development Kit\n",
      "But first, we have to install Java, specifically the Java Development Kit.\n",
      "\n",
      "Go to the Java Development Kit download page, find your operating system, and click on the corresponding download link: https://www.oracle.com/java/technologies/javase-jdk14-downloads.html\n",
      "\n",
      "- Linux -> Linux Compressed Archive\n",
      "- Mac -> macOS Installer\n",
      "- Windows -> Windowsx64 Installer\n",
      "\n",
      "Then open or unzip the file and follow all the prompts. You can use all the suggested defaults.\n",
      "## Tell Your Computer Where to Find Java\n",
      "Now that we have the JDK downloaded, we have to tell our computers where to find it. For Mac/Chrome/Linux users, we have to set up a special [\"environment\" variable](https://launchschool.com/books/command_line/read/environment#environmentvariables) called `JAVA_HOME` and give it the file path where we just downloaded our Java Development Kit. For Windows users, we have to edit the special environmental variable called `PATH` and add the file path of the JDK.\n",
      "\n",
      "Note: \"Environment\" variables are kind of like Python variables, except they exist in your whole computer environment. The Launch School has a helpful chapter on [environment variables](https://launchschool.com/books/command_line/read/environment#environmentvariables) and the [PATH](https://launchschool.com/books/command_line/read/environment#path) variable.\n",
      "###  Mac\n",
      "To set up the `JAVA_HOME` environment variable on a Mac, you can run the following on the command line. The line of code adds your `JAVA_HOME` variable to a file called \"bash_profile\", which is where environment variables are stored.\n",
      "!echo \"export JAVA_HOME=$(/usr/libexec/java_home)\" >> ~/.bash_profile\n",
      "To immediately update your \"bash_profile,\" run:\n",
      "!source ~/.bash_profile\n",
      "Then, to test whether Java installed correctly, run `javac` on the command line. If you get a list of options, as below, then you've installed the JDK properly. If it says the command is not recognized, then you don't have JDK set up yet.\n",
      "!javac\n",
      "### <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "To edit the `PATH` variable on a Windows computer, follow the instructions below:\n",
      "- Open Search and type \"advanced system settings\"\n",
      "- In the shown options, select the \"View advanced system settings\" link\n",
      "- Under the Advanced tab, click \"Environment Variables\"\n",
      "- Under \"System variables,\" click the variable \"PATH\" and then click \"Edit\"\n",
      "- Click \"New\" and add the file path to the JDK (e.g. `C:\\Program Files\\Java\\jdk13.0.2\\bin`)\n",
      "For more Windows installation help, see Prof. Paul Vierthaler's video tutorial [\"Practical Python for DH: Topic Modeling Software Install\"](https://youtu.be/2C3cDEd7h4o?t=224).\n",
      "Now restart your PowerShell. To test whether java is installed, run `javac` in the PowerShell. If you get a list of options, then you've installed the JDK properly. If it says the command is not recognized, then you don't have it yet.\n",
      "!javac\n",
      "### Chrome / Linux\n",
      "To set up the `JAVA_HOME` environment variable on a Linux machine or a Chrome computer running Linux, you can run the following on the command line. The line of code adds your `JAVA_HOME` variable to a file called \"bashrc\", which is where environment variables are stored.\n",
      "Make sure to change `/fill-in-the-path/to/your-java_installation` to the file path where your JDK actually exists below:\n",
      "!echo \"export JAVA_HOME=/fill-in-the-path/to/your-java_installation/bin\" >> ~/.bashrc\n",
      "To immediately update your \"bash_profile,\" run:\n",
      "!source ~/.bashrc\n",
      "To test whether java is installed, run `javac` on the command line. If you get a list of options, as below, then you've installed the JDK properly. If it says the command is not recognized, then you don't have it yet.\n",
      "!javac\n",
      "## Download and Unzip MALLET\n",
      "Now we need to download the MALLET package. To download MALLET, click the following link http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip or find the link on the [MALLET home page](http://mallet.cs.umass.edu/download.php). Once the zip file downloads, unzip it.\n",
      "If you're using a Mac, move the \"mallet-2.0.8\" directory into your home folder.\n",
      "\n",
      "*Note: To open your \"home\" folder, open \"Finder\" and type `Cmd` + `Shift` + `H`. To move one directory up, type `Cmd` + `↑`. Now, if you want to bookmark your home folder so you can find it more easily in the future, simply drag and drop your home folder to the sidebar.*\n",
      "\n",
      "If you're using a Windows computer, move the \"mallet-2.0.8\" directory int your `C:\\` drive. \n",
      "### <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Heads Up Windows Users! \n",
      "You need to complete one more step. You need to once again tell your computer where MALLET is located:\n",
      "- Open Search and type \"advanced system settings\"\n",
      "- In the shown options, select the View advanced system settings link\n",
      "- Under the Advanced tab, click \"Environment Variables\"\n",
      "- In the User variables section, click \"New\"\n",
      "- For the Variable name, type `MALLET_HOME`. For the Value, type the path to your MALLET: `C:\\mallet-2.0.8`. Then click OK\n",
      "- Click OK and click Apply to apply the changes\n",
      "For more Windows help, see Prof. Paul Vierthaler's [topic modeling tutorial](https://youtu.be/2C3cDEd7h4o?t=107).\n",
      "To test whether MALLET works on your computer, type in the file path for MALLET on the command line and `import-file`.\n",
      "\n",
      "If it's working, then you'll get a message that says \"A tool for creating instance lists of feature vectors from comma-separated-values\" and a list of options.\n",
      "!~/mallet-2.0.8/bin/mallet import-file\n",
      "## Install Little MALLET Wrapper\n",
      "Finally, we're going to install the Python package [little_mallet_wrapper](https://github.com/maria-antoniak/little-mallet-wrapper). To install it, run `pip install little_mallet_wrapper`, as below.\n",
      "!pip install little_mallet_wrapper\n",
      "Since Little MALLET Wrapper also uses the data visualization library `seaborn`, we're also going to `pip install seaborn`:\n",
      "!pip install seaborn\n",
      "## You're Ready! 🥳\n",
      "\n",
      "# Pandas — More Targeted Analysis\n",
      "[Download this \"Pandas — More Targeted Analysis\" notebook and other relevant files here](https://melaniewalsh.org/Functions-More-Pandas.zip)\n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100% border=2>\n",
      "\n",
      "import pandas as pd\n",
      "pudding_film_df = pd.read_csv('../data/Pudding/Pudding-Film-Dialogue-Clean.csv', encoding='utf-8', sep=',')\n",
      "pudding_film_df.head()\n",
      "## Filter Data\n",
      "Let's create two filtered dataframes for all the men characters and all the women characters.\n",
      "men_film_df = pudding_film_df[pudding_film_df['gender'] == 'Man']\n",
      "women_film_df = pudding_film_df[pudding_film_df['gender'] == 'Woman']\n",
      "## Men in Film\n",
      "How many average words do the men in this dataset speak? What's the average age for the men characters?\n",
      "men_film_df.describe()\n",
      "Based on this description, the average number of words spoken is 932. The average age is 44 years old.\n",
      "Now let's find the movies with the highest proportion of men speaking.\n",
      "men_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "men_titles_aggregated = men_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "men_titles_aggregated.sort_values(by='proportion_of_dialogue', ascending=False)[:20]\n",
      "## Women in Film\n",
      "How many average words do the women in this dataset speak? What's the average age for the women characters?\n",
      "women_film_df.describe()\n",
      "Based on this description, the average number of words spoken is 850. The average age is 37 years old.\n",
      "women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "women_titles_aggregated = women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "women_titles_aggregated.sort_values(by='proportion_of_dialogue', ascending=False)[:20]\n",
      "## Create an Interactive Data Visualization\n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100% border=2>\n",
      "To create the fancy data visualizations in their piece, The Pudding used an entirely different programming language called JavaScript, the most common programming language for the web. They specifically used a data viz library called [d3](https://observablehq.com/@d3/gallery) and a \"scrollytelling\" library called [Scrollama](https://pudding.cool/process/introducing-scrollama/).\n",
      "Just for fun, I thought we could try approximate one of their data visualizations with a Python interactive data visualization library called Bokeh. If this code doesn't work for you, don't worry about it. It's just for fun. \n",
      "To install Bokeh, run the cell below:\n",
      "!pip install bokeh\n",
      "We'll need to import a bunch of specific modules from Bokeh.\n",
      "from bokeh.plotting import figure, show\n",
      "from bokeh.models import ColumnDataSource\n",
      "from bokeh.models.tools import HoverTool\n",
      "from bokeh.io import output_notebook, show\n",
      "from bokeh.plotting import output_file, save\n",
      "from bokeh.palettes import RdBu\n",
      "from bokeh.transform import linear_cmap\n",
      "We also need to set it up to work in a Jupyter notebook.\n",
      "output_notebook()\n",
      "women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "women_film_groupby = women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "# Set up the source data that will suppply the x,y columns and the film title hover text\n",
      "\n",
      "source = ColumnDataSource(\n",
      "    data=dict(\n",
      "    x= women_film_groupby['proportion_of_dialogue'],\n",
      "    y= [0 for title in women_film_groupby.index],\n",
      "    title= women_film_groupby.index\n",
      "                             ))\n",
      "# Set the hover tool tip to the film title\n",
      "\n",
      "TOOLTIPS = [(\"Title\", \"@title\")]\n",
      "\n",
      "# Set up the data viz figure including width, height and labels\n",
      "\n",
      "p = figure(tooltips=TOOLTIPS, plot_width=2000,plot_height=500,\n",
      "           title=\"Film Dialogue Gender Breakdown\", x_axis_label = 'Proportion of Men —> Women Speaking')\n",
      "\n",
      "p.title.text_font_size = '18pt'\n",
      "p.yaxis.bounds = [0, 0]\n",
      "\n",
      "# Create a red to blue color palette\n",
      "\n",
      "mapper = linear_cmap(field_name='x', palette=RdBu[4], low=0, high=1)\n",
      "\n",
      "# Supply inidivudal points values\n",
      "p.circle(x='x',\n",
      "         y='y',\n",
      "         size = 20,\n",
      "         source=source, \n",
      "         alpha=0.7,\n",
      "        color= mapper)\n",
      "\n",
      "# Output file as an html file as well as show it in this Jupyter notebook\n",
      "output_file('film-dialogue.html')\n",
      "\n",
      "show(p)\n",
      "[Explore our interactive data viz here](film-dialogue.html)\n",
      "# Data Analysis\n",
      "\n",
      "# Pandas — Merge Datasets\n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100% border=2 border=2>\n",
      "\n",
      "<img src='../images/Pudding-Github-data.png' width=100%, border=2 border=2>\n",
      "\n",
      "import pandas as pd\n",
      "The Pudding published one CSV file called \"meta_data7.csv\" that contains, among things, the title of each movie, the year of its release, and its box office gross. Let's read in this CSV file and make it into a dataframe.\n",
      "metadata = pd.read_csv('../data/Pudding/meta_data7.csv', delimiter=',', encoding='utf-8')\n",
      "metadata\n",
      "We're going to drop the column \"lines_data\", which contains information about when during the film each character speaks.\n",
      "metadata = metadata.drop(columns='lines_data')\n",
      "The Pudding published another CSV file called \"character_list5.csv\" that contains, among other things, the name, gender, and age of each character as well as the number of words the character speaks.\n",
      "characters = pd.read_csv('../data/Pudding/character_list5.csv', delimiter=',',encoding='utf-8')\n",
      "characters\n",
      "As you can see, the characters dataframe doesn't include the actual title of the movie in which the character appears or the movie's release year or box office gross. The metadata datafram doesn't contain any information about the characters. We want that info all in one place. So how can we combine all of this data together?\n",
      "## `pd.merge()`\n",
      "If you look closely, there's one column that both datasets share in common: \"script_id\". If two datasets share at least one column in common, then you can merge them together based on this column. You can use the `pd.merge()` function and then type in the name of the first dataframe, the name of the second dataframe, and the shared column to be merged on.\n",
      "pd.merge(characters, metadata, on='script_id')\n",
      "merged_movie_character = pd.merge(characters, metadata, on='script_id')\n",
      "## Calculate Dialogue Proportions\n",
      "We're going to add one more column to this dataset before the next lesson. We're going to calculate the proportion of words spoken in each film by each character. To do so, we're going to `.groupby()` the movie's title and calculate the sum total number of words spoken in each movie. \n",
      "merged_movie_character.groupby(['title'])[['words']].sum()\n",
      "If we use the `.transform()`, we can turn this groupby into a single column of data.\n",
      "merged_movie_character.groupby(['title'])[['words']].transform(sum)\n",
      "total_movie_words = merged_movie_character.groupby(['title'])[['words']].transform(sum)\n",
      "Then we're going to divide the total number of words spoken by each character by the total number of words spoken in each film.\n",
      "total_character_words = merged_movie_character[['words']]\n",
      "total_character_words / total_movie_words\n",
      "dialogue_proportion = total_character_words / total_movie_words\n",
      "Then we're going to add it as a new column.\n",
      "merged_movie_character['proportion_of_dialogue'] = dialogue_proportion\n",
      "merged_movie_character\n",
      "## Write to CSV File\n",
      "Finally, we're going to output this merged and more comprehensive dataset to a CSV file by using the `.to_csv()` method. We set the `index` parameter to `False` to remove the index column (the numbers in the left-most column).\n",
      "merged_movie_character.to_csv('../data/Pudding/Merged-Pudding-Film-Dialogue.csv', sep=',', encoding='utf-8', index=False)\n",
      "## Pandas — Exploratory Data Analysis\n",
      "[Download this \"Pandas — Exploratory Data Analysis\" notebook and other relevant files here](https://melaniewalsh.org/Functions-More-Pandas.zip)\n",
      "<img src='../images/Pudding-Github-data.png' width=100%, border=2>\n",
      "\n",
      "## Data Biography\n",
      "The very first step of a responsible cultural analytics/digital humanities EDA, however, doesn't require any code or computation at all. The very first step is to understand the data's \"biography,\" as Heather Krause lays it out in [\"Data Biographies\"](https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/). Your assignment for Thursday is to make sure you can answer these questions. It will be helpful to read [\"FAQ for the “Film Dialogue, By Gender” Project](https://medium.com/@matthew_daniels/faq-for-the-film-dialogue-by-gender-project-40078209f751).\n",
      "### Where did the data come from?\n",
      "\n",
      "### Who collected the data?\n",
      "\n",
      "### How was data collected? And how was \"gender\" calculated?\n",
      "\n",
      "### Why was the data collected?\n",
      "\n",
      "## Import Pandas\n",
      "The rest of our EDA will be accomplished with the help of the Python library Pandas.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "We will read in our Pudding film dialogue dataset, which we created by merging together a few different datasets that were published by Hannah Andersen and Matt Daniels. This CSV file includes the word **salty**—'../data/Pudding/Pudding-Film-Dialogue-**Salty**.csv'—because it contains a few artificially added problems and errors. It's inspired by Matthew Lincoln's R Package for artificially creating messy data, [salty](https://rdrr.io/cran/salty/).\n",
      "pudding_film_df = pd.read_csv('../data/Pudding/Pudding-Film-Dialogue-Salty.csv', delimiter=',', encoding='utf-8')\n",
      "pudding_film_df.sample(10)\n",
      "## Examine the Data\n",
      "How many rows and columns are in our dataset? What types of data are here? What are the names of the columns?\n",
      "pudding_film_df.shape\n",
      "pudding_film_df.dtypes\n",
      "pudding_film_df.columns\n",
      "## `.describe()`\n",
      "The `.describe()` method will give you a summary table of all the numerical values in your dataframe.\n",
      "pudding_film_df.describe()\n",
      "Do you notice any outliers, anomalies, or potential problems here?\n",
      "pudding_film_df[pudding_film_df['age'] == 2013]\n",
      "## Rename and Drop Columns\n",
      "EDA often includes renaming and dropping columns to make the data easier to work with and more legible.\n",
      "pudding_film_df = pudding_film_df.rename(columns={'imdb_character_name': 'character', 'year': 'release_year'})\n",
      "pudding_film_df.head()\n",
      "pudding_film_df = pudding_film_df.drop(columns='imdb_id')\n",
      "## Check for Duplicates or Missing Values (`NaN`)\n",
      "## Duplicates\n",
      "You can check for duplicate rows by using the `.duplicated()` method and seting the parameter `keep=False` (which will display both duplicated values as opposed to only the first value `keep='first'` or the last value `keep='last'`).\n",
      "pudding_film_df.duplicated(keep=False)\n",
      "The output above is reporting whether each row in the dataset is a duplicate `True` or not `False`. We can use the `.duplicated()` method inside a filter to isolate only the rows in the dataframe that are exact duplicates.\n",
      "pudding_film_df[pudding_film_df.duplicated(keep=False)]\n",
      "You can drop duplicates from the dataframe with the `.drop_duplicates()` method and choose to \"keep\" the \"first\" instance of the duplicate or the \"last\" instance.\n",
      "pudding_film_df = pudding_film_df.drop_duplicates(keep='first')\n",
      "Now if we check the data for duplicates again, they should be all gone.\n",
      "pudding_film_df[pudding_film_df.duplicated(keep=False)]\n",
      "## Missing/Null Values\n",
      "Missing values in a Pandas dataframe are interpreted as a special `NaN` value. This is important to remember for a few reasons.\n",
      "\n",
      "First, if we want to find out how many rows in our dataset contain blank or missing values, we'll need to use a special `.isnull()` method that checks for `NaN` values. If we check for missing values in our \"character\" column, for example, we will discover that there are two rows that don't have any character names.\n",
      "pudding_film_df['character'].isnull() == True\n",
      "pudding_film_df[pudding_film_df['character'].isnull() == True]\n",
      "This is important information for the sake of better understanding our dataset. But it's also important because `NaN` values can mess up later calculations or transformations that you might attempt. For this reason, we're going to replace or \"fill\" these `NaN` values with the string \"No Character Data\" by using the `.fillna()` method.\n",
      "pudding_film_df['character'] = pudding_film_df['character'].fillna('No Character Data')\n",
      "pudding_film_df[pudding_film_df['character'].isnull() == True]\n",
      "pudding_film_df[pudding_film_df['character'] == 'No Character Data']\n",
      "With the `.isnull()` method, we can also discover that there are thousands of rows that don't contain any box office gross information as well as thousands of characters that don't have any associated age information.\n",
      "pudding_film_df[pudding_film_df['gross'].isnull() == True]\n",
      "pudding_film_df[pudding_film_df['age'].isnull() == True]\n",
      "## Check Distributions of Data\n",
      "## `.hist()`\n",
      "One quick way to get a sense of the distributions of your data is with the [`hist()` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html#pandas.DataFrame.hist), which creates a \"histogram\" of every numerical category in your dataframe. A histogram is a chart the represents the frequency of each value. A histogram looks a lot like a bar chart — and it *is* a lot like a bar chart — except a histogram represents quantitative values as opposed to qualitative values.\n",
      "pudding_film_df.hist()\n",
      "If you use the parameter `figsize(x_size, y_size)` and specify specific sizes, you can make the grid of histogram charts bigger.\n",
      "pudding_film_df.hist(figsize=(10,10))\n",
      "You can also select individual columns and plot histograms based on that column. \n",
      "pudding_film_df['age'].hist()\n",
      "If you add in the `range= (x_size, y_size)` parameter, you can focus your histogram on a specific area.\n",
      "pudding_film_df['age'].hist(range=(0, 100))\n",
      "pudding_film_df['gross'].hist()\n",
      "You can also specify how finely to group the data with the `bins` parameter. You might think about it like putting the data into a bunch of physical bins 🗑️  If we set the `bins` parameter to 100, we can a more nuanced view of the box office gross distribution.\n",
      "pudding_film_df['gross'].hist(bins=100, range=(0, 1000))\n",
      "pudding_film_df['release_year'].hist()\n",
      "pudding_film_df['release_year'].hist(bins=50, range=(1940, 2020))\n",
      "If you specifically select a column that contains qualitative data as opposed to quantitative data, you can also force Pandas to make a histogram.\n",
      "pudding_film_df['gender'].hist()\n",
      "## Clean and Transform Data\n",
      "## Pandas `.str` Methods\n",
      "| **Pandas String Method** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `pandas_column.str.lower()`         | makes the string in each row lowercase                                                                                |\n",
      "| `pandas_column.str.upper()`         | makes the string in each row uppercase                                                |\n",
      "| `pandas_column.str.title()`         | makes the string in each row titlecase                                                |\n",
      "| `pandas_column.str.replace('old string', 'new string')`      | replaces `old string` with `new string` for each row |\n",
      "| `pandas_column.str.contains('some string')`      | tests whether string in each row contains \"some string\" |\n",
      "| `pandas_column.str.split('delim')`          | returns a list of substrings separated by the given delimiter |\n",
      "| `pandas_column.str.join(list)`         | opposite of split(), joins the elements in the given list together using the string                                                                        |\n",
      "                                                            \n",
      "Remember all the special things that you can do with string data aka [string methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html)? Well Pandas columns that contain string data also have special [Pandas string methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#string-methods). Many of them are the same or almost the same as regular Python string methods, except  they will transform every single string value in a column, and you have to add the code `.str` to the method chain.\n",
      "For example, to transform every character's name in the \"character\" column from lowercase to uppercase, you can add `.str.upper()` to `pudding_film_df['character']`\n",
      "pudding_film_df['character'] = pudding_film_df['character'].str.upper()\n",
      "pudding_film_df.head()\n",
      "To transform every character's name in the \"character\" column back to lowercase, you can add`.str.lower()` to `pudding_film_df['character']`\n",
      "pudding_film_df['character'] = pudding_film_df['character'].str.lower()\n",
      "pudding_film_df.sample(10)\n",
      "If we want to replace the gender columns's single letter abbreviation for \"male\" / \"female\" (sex) with \"man\" / \"woman\" (gender identity), we could use the `.str.replace()` method. \n",
      "pudding_film_df['gender'] = pudding_film_df['gender'].str.replace('m', 'man')\n",
      "pudding_film_df['gender'] = pudding_film_df['gender'].str.replace('f', 'woman')\n",
      "pudding_film_df.sample(10)\n",
      "We can also use the `.str.contains()` to search for particular words or phrases in a column, such as \"Star Wars\" or \"Mean Girls\".\n",
      "pudding_film_df[pudding_film_df['title'].str.contains('Star Wars')]\n",
      "pudding_film_df[pudding_film_df['title'].str.contains('Mean Girls')]\n",
      "## Applying Functions\n",
      "With the `.apply()` method, you can run a function on every single row in a Pandas column or dataframe.\n",
      "def make_text_lower_case(text):\n",
      "    lower_case_text = text.upper()\n",
      "    return lower_case_text\n",
      "def make_text_upper_case(text):\n",
      "    upper_case_text = text.upper()\n",
      "    return upper_case_text\n",
      "def make_text_title_case(text):\n",
      "    title_case_text = text.title()\n",
      "    return title_case_text\n",
      "make_text_upper_case(\"betty\")\n",
      "pudding_film_df['character'].apply(make_text_upper_case)\n",
      "pudding_film_df['character'].apply(make_text_title_case)\n",
      "pudding_film_df['character'] = pudding_film_df['character'].apply(make_text_title_case)\n",
      "pudding_film_df.sample(10)\n",
      "def clarify_gender(text):\n",
      "    gender = ''\n",
      "    if text == 'woman':\n",
      "        gender = 'Woman'\n",
      "    elif text == 'man':\n",
      "        gender = 'Man'\n",
      "    return gender\n",
      "def calculate_after_2000(year):\n",
      "    is_millennium = ''\n",
      "    if year >= 2000:\n",
      "        is_millennium = True\n",
      "    elif year < 2000:\n",
      "        is_millennium = False\n",
      "    return is_millennium\n",
      "pudding_film_df['gender'] = pudding_film_df['gender'].apply(clarify_gender)\n",
      "pudding_film_df.head()\n",
      "pudding_film_df['after_2000'] = pudding_film_df['release_year'].apply(calculate_after_2000)\n",
      "pudding_film_df.head()\n",
      "## Write to CSV\n",
      "To write a dataframe to a new CSV file, you can add the `.to_csv()` method after the name of your dataframe.\n",
      "\n",
      "`your_dataframe.to_csv('Your-Desired-File-Name', encoding='utf-8', sep=',', index=False)`\n",
      "pudding_film_df.to_csv('../data/Pudding/Pudding-Film-Dialogue-Clean.csv', encoding='utf-8', sep=',', index=False)\n",
      "\n",
      "# Pandas\n",
      "In this lesson, we're going to learn about [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html), a powerful Python library for working with tabular data like CSV files. Things that were kind of clunky to accomplish with the CSV module are much easier to accomplish with Pandas.\n",
      "\n",
      "Here are some of the things you can do with Pandas:\n",
      "\n",
      "* Easily read and write CSV files\n",
      "* Sort and filter data\n",
      "* Analyze data\n",
      "* Combine datasets\n",
      "* Create data visualizations\n",
      "## Our Dataset\n",
      "<img src=\"../images/Slave-Voyages.png\" width=100%, border=2 border=2>\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson,\n",
      "<a href=\"https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death\">“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>\n",
      "The dataset that we're going to be working with in this lesson is taken from [The Trans-Atlantic Slave Trade Database](https://www.slavevoyages.org/voyage/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of enslaved men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "\n",
      "We're working with this data for a number of reasons. First, the *Slave Voyages* project is a major data-driven contribution to the history of slavery and to the field of the digital humanities. Second, working with this data makes clear that, as Johnson writes, computation and data alone cannot capture \"the violent quandary\" of slavery or bring about justice. As we learn how to manipulate and analyze this dataset with Pandas, we should be reminded, at every step, of the challenges that are involved in using this data responsibly and intentionally.\n",
      "## Import Pandas\n",
      "To use the Pandas library, we first need to `import` it, as below. This `import` statement not only imports the library but also gives it a nickname: `as` \"pd\" instead of \"pandas.\" You'll run across this alias convention a lot. It's a way to save time and not have to write the entire word \"pandas\" over and over again.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "To read in a CSV file, you use the function `pd.read_csv()` and insert the name of your desired file path. You can also add the \"delimiter\" argument to specify the delimiter for your file.\n",
      "slave_voyages_df = pd.read_csv('../data/Slave-Voyages-Trans-Atlantic-North-America.csv', delimiter=\",\")\n",
      "```{tip} \n",
      "If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "```\n",
      "**Pro tip!** If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "help(pd.read_csv)\n",
      "When you read in a CSV file with Pandas, you transform it into a Pandas object called a dataframe.\n",
      "type(slave_voyages_df)\n",
      "## Display the Data\n",
      "In a Jupyter notebook, you can display a dataframe simply by running a cell with the name of the dataframe. The `NaN` value is the Pandas value for any missing data (`NaN` values have special properties that we'll get to later).\n",
      "slave_voyages_df\n",
      "You should always eyeball your dataset either by looking at the first few rows or by looking at a few random rows. To look at the first few rows, you can use a method called `.head()` (very similar to the `head` command on the command line).\n",
      "slave_voyages_df.head(10)\n",
      "To look at a random sample of rows, you can use the `.sample()` method. You might run this cell a few times to make sure everything looks ok.\n",
      "slave_voyages_df.sample(10)\n",
      "## Examine the Data\n",
      "You can check the dataframe's \"shape\" or how many rows vs columns it contains with `.shape` (number of rows, number of columns)\n",
      "slave_voyages_df.shape\n",
      "You can (and always should) check the data types that each column contains with `.dtypes`\n",
      "\n",
      "\n",
      "| **Pandas dtypes** |                                                                                    |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `object`         | string                                                                               |\n",
      "| `float64`         | float                                               |\n",
      "| `int64`       | integer                                                        |\n",
      "| `datetime64`       |  date time              \n",
      "slave_voyages_df.dtypes\n",
      "It's important to always check the data types in your dataframe. For example, sometimes numeric values, which you want to do mathematical calculations with, will accidentally be interpreted as a string object. To perform calculations on this data, you would need to first convert that column from a string to an integer.\n",
      "\n",
      "The data types in our Trans-Atlantic Slave Trade dataset look ok, except the column \"year_of_arrival,\" which is currently interpreted as an integer value when ideally it should be a datetime value.\n",
      "You can also check the column names of your dataframe with `.columns`\n",
      "slave_voyages_df.columns\n",
      "## Rename and Drop Columns\n",
      "Let's say we wanted to rename the \"flag\" column as \"national_affiliation.\" One way to approach this goal would be to make a new column identical to the \"flag\" column but with a different name.\n",
      "\n",
      "Making a new column with Pandas is very similar to making a new Python variable. You simply type the name of the dataframe with square brackets `[]` and include the name of your new desired column that doesn't exist yet. Then you assign it the value of the already existing flag column.\n",
      "slave_voyages_df['national_affiliation'] = slave_voyages_df['flag']\n",
      "If you scroll all the way to the end of the dataframe, you can see that we added a new column called \"national_affiliation\" that's exactly the same as the flag column.\n",
      "slave_voyages_df.head()\n",
      "But what if we don't want multiple columns with the same information? What if we just want to rename the \"flag\" column?\n",
      "First, let's get rid of the \"national_affiliation\" column so we can try again. To remove columns, you can use the `.drop()` method and the `columns=` parameter, followed by the column you'd like to drop.\n",
      "slave_voyages_df = slave_voyages_df.drop(columns='national_affiliation')\n",
      "slave_voyages_df.columns\n",
      "A better way to rename columns is with the `.rename()` method and the `columns=` parameter that includes the original name of the column followed by a colon and the new desired name of the column.\n",
      "slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "Renaming the \"flag\" column as we did above will only momentarily change that column's name. If we display our dataframe, we'll see that the column name has *not* changed permamently.\n",
      "slave_voyages_df.head(5)\n",
      "To save changes in your dataframe, you need to keep re-assigning your dataframe, as below.\n",
      "slave_voyages_df = slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "slave_voyages_df.head(5)\n",
      "## Select Columns\n",
      "To isolate a column with the csv module was kind of a pain. We had to loop through each row and index a certain value in each row's list. But with Pandas it's very simple to select a column. You can do so by typing the name of the dataframe accompanied by square brackets `[]` with the name of the column in quotation marks.\n",
      "slave_voyages_df['vessel_name']\n",
      "type(slave_voyages_df['vessel_name'])\n",
      "Each column in a dataframe is technically a Pandas object called a \"Series.\" We won't worry too much about the \"Series\" object right now, except to note that a Series object does not display as nicely as a DataFrame. To select a column as a dataframe, you can use two square brackets `[[]]`\n",
      "slave_voyages_df[['vessel_name']]\n",
      "type(slave_voyages_df[['vessel_name']])\n",
      "With two square brackets, you can also select and isolate multiple columns at the same time.\n",
      "slave_voyages_df[['vessel_name', 'national_affiliation', 'year_of_arrival']]\n",
      "## Filter Data\n",
      "You can also filter data based on different criteria. For example, let's say we wanted to look at the slave voyages whose national affiliation was American. You can evaluate each row in a column by isolating that column and then using the comparison operators that we discussed a few lessons ago, such as equals `==`.\n",
      "slave_voyages_df['national_affiliation'] == \"U.S.A.\"\n",
      "The output above is an evaluation of whether each row in the \"flag\" column equals \"U.S.A.\" or not. This function becomes much more helpful when we use this comparison as a selection criteria in its own right.\n",
      "\n",
      "Just as we can select a column by placing its name inside square brackets, we can select filtered data by placing a filter inside square brackets `[]`. Essentially, the line below is saying give me all the rows in the dataframe that match `slave_voyages_df['flag'] == \"U.S.A.\"`\n",
      "slave_voyages_df[slave_voyages_df['national_affiliation'] == \"U.S.A.\"]\n",
      "You can do the same thing with integers and other comparison operators. For example, if we wanted to filter for only the slave voyages that arrived after 1830, we could do so with `slave_voyages_df['year_of_arrival'] > 1830`\n",
      "slave_voyages_df['year_of_arrival'] > 1830\n",
      "slave_voyages_df[slave_voyages_df['year_of_arrival'] > 1830]\n",
      "## Sort Columns\n",
      "You can sort a dataframe with the `.sort_values()` method, inside of which you include the parameter `by=` and indicate the name of the column you want to sort by (written in quotation marks).\n",
      "slave_voyages_df.sort_values(by='total_disembarked')\n",
      "Take a look at the way the dataframe above is sorted. Do you notice that something seems a bit off?\n",
      "\n",
      "By default, Pandas will sort in \"ascending\" order, from the smallest value to the largest value. If you want to sort the largest values first, you need to include another parameter `ascending=False`.\n",
      "slave_voyages_df.sort_values(by='total_disembarked', ascending=False)\n",
      "If you want to sort a Series object, you don't need to use the `by=` paramter.\n",
      "slave_voyages_df['total_disembarked'].sort_values(ascending=False)\n",
      "## Count Values in Columns\n",
      "To count the unique values in a column, you can use the `.value_counts()` method.\n",
      "slave_voyages_df['national_affiliation'].value_counts()\n",
      "slave_voyages_df['vessel_name'].value_counts()\n",
      "slave_voyages_df['place_of_slave_disembarkation'].value_counts()\n",
      "## Calculate Columns\n",
      "| Pandas calculations | Explanation                         |\n",
      "|----------|-------------------------------------|\n",
      "| `.count()`    | Number of observations    |\n",
      "| `.sum()`      | Sum of values                       |\n",
      "| `.mean()`     | Mean of values                      |\n",
      "| `.median()`   | Median of values         |\n",
      "| `.min()`      | Minimum                             |\n",
      "| `.max()`      | Maximum                             |\n",
      "| `.mode()`     | Mode                                |\n",
      "| `.std()`      | Unbiased standard deviation         |\n",
      "\n",
      "\n",
      "You can do different calculations on columns with built-in Pandas functions, such as `.sum()` and `.max()\n",
      "slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum() - slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_disembarked'].mean()\n",
      "slave_voyages_df['total_disembarked'].max()\n",
      "## Groupby Columns\n",
      "One powerful Pandas function is called `.groupby()`, which allows you to aggregate data and perform calculations on it. For example, if we wanted to see how many enslaved people were transported by each nation, we could group by, or aggregate our data based on, \"national_affiliation\". The first step to using groupby is to type the name of your dataframe followed by `.groupby()` with the column you'd like to aggregate based on. \n",
      "slave_voyages_df.groupby('national_affiliation')\n",
      "This action will created a \"groupby\" object, which you won't be able to access unless you perform a calculation on it, such as `.sum()`\n",
      "slave_voyages_df.groupby('national_affiliation').sum()\n",
      "If we want to isolate only the \"total_disembarked\" column from this groupby calculation, then we can add in \"total_disembarked\" in square brackets `[]`\n",
      "slave_voyages_df.groupby('national_affiliation')['total_disembarked'].sum().sort_values(ascending=False)\n",
      "slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "Let's save this groupby calculation into a new variable called `national_totals`\n",
      "national_totals = slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "## Make Plots \n",
      "We're going to talk about making plots and data visualizations more extensively later. But for now we're going to introduce the fact that you can make simple plots simply by using the [`.plot()` method](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html) that's built into Pandas.\n",
      "national_totals.plot(kind='bar')\n",
      "national_totals.plot(kind='barh')\n",
      "## Time Series\n",
      "slave_voyages_df['year_of_arrival'].dtypes\n",
      "Transform an integer into a datetime\n",
      "slave_voyages_df['year_of_arrival'] = pd.to_datetime(slave_voyages_df['year_of_arrival'], format='%Y',errors='coerce')\n",
      "Number of voyages over time\n",
      "slave_voyages_df['year_of_arrival'].value_counts().plot()\n",
      "Total number of enslaved people disembarked over time\n",
      "slave_voyages_df.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "usa_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='U.S.A.']\n",
      "usa_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "gb_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='Great Britain']\n",
      "gb_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "# Pandas\n",
      "In this lesson, we're going to learn about [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html), a powerful Python library for working with tabular data like CSV files. Things that were kind of clunky to accomplish with the CSV module are much easier to accomplish with Pandas.\n",
      "\n",
      "Here are some of the things you can do with Pandas:\n",
      "\n",
      "* Easily read and write CSV files\n",
      "* Sort and filter data\n",
      "* Analyze data\n",
      "* Combine datasets\n",
      "* Create data visualizations\n",
      "## Our Dataset\n",
      "<img src=\"../images/Slave-Voyages.png\" width=100%>\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson,\n",
      "<a href=\"https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death\">“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>\n",
      "The dataset that we're going to be working with in this lesson is taken from [The Trans-Atlantic Slave Trade Database](https://www.slavevoyages.org/voyage/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of enslaved men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "\n",
      "We're working with this data for a number of reasons. First, the *Slave Voyages* project is a major data-driven contribution to the history of slavery and to the field of the digital humanities. Second, working with this data makes clear that, as Johnson writes, computation and data alone cannot capture \"the violent quandary\" of slavery or bring about justice. As we learn how to manipulate and analyze this dataset with Pandas, we should be reminded, at every step, of the challenges that are involved in using this data responsibly and intentionally.\n",
      "## Import Pandas\n",
      "To use the Pandas library, we first need to `import` it, as below. This `import` statement not only imports the library but also gives it a nickname: `as` \"pd\" instead of \"pandas.\" You'll run across this alias convention a lot. It's a way to save time and not have to write the entire word \"pandas\" over and over again.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "To read in a CSV file, you use the function `pd.read_csv()` and insert the name of your desired file path. You can also add the \"delimiter\" argument to specify the delimiter for your file.\n",
      "slave_voyages_df = pd.read_csv('../data/Slave-Voyages-Trans-Atlantic-North-America.csv', delimiter=\",\")\n",
      "```{tip} \n",
      "If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "```\n",
      "**Pro tip!** If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "help(pd.read_csv)\n",
      "When you read in a CSV file with Pandas, you transform it into a Pandas object called a dataframe.\n",
      "type(slave_voyages_df)\n",
      "## Display the Data\n",
      "In a Jupyter notebook, you can display a dataframe simply by running a cell with the name of the dataframe. The `NaN` value is the Pandas value for any missing data (`NaN` values have special properties that we'll get to later).\n",
      "slave_voyages_df\n",
      "You should always eyeball your dataset either by looking at the first few rows or by looking at a few random rows. To look at the first few rows, you can use a method called `.head()` (very similar to the `head` command on the command line).\n",
      "slave_voyages_df.head(10)\n",
      "To look at a random sample of rows, you can use the `.sample()` method. You might run this cell a few times to make sure everything looks ok.\n",
      "slave_voyages_df.sample(10)\n",
      "## Examine the Data\n",
      "You can check the dataframe's \"shape\" or how many rows vs columns it contains with `.shape` (number of rows, number of columns)\n",
      "slave_voyages_df.shape\n",
      "You can (and always should) check the data types that each column contains with `.dtypes`\n",
      "\n",
      "\n",
      "| **Pandas dtypes** |                                                                                    |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `object`         | string                                                                               |\n",
      "| `float64`         | float                                               |\n",
      "| `int64`       | integer                                                        |\n",
      "| `datetime64`       |  date time              \n",
      "slave_voyages_df.dtypes\n",
      "It's important to always check the data types in your dataframe. For example, sometimes numeric values, which you want to do mathematical calculations with, will accidentally be interpreted as a string object. To perform calculations on this data, you would need to first convert that column from a string to an integer.\n",
      "\n",
      "The data types in our Trans-Atlantic Slave Trade dataset look ok, except the column \"year_of_arrival,\" which is currently interpreted as an integer value when ideally it should be a datetime value.\n",
      "You can also check the column names of your dataframe with `.columns`\n",
      "slave_voyages_df.columns\n",
      "## Rename and Drop Columns\n",
      "Let's say we wanted to rename the \"flag\" column as \"national_affiliation.\" One way to approach this goal would be to make a new column identical to the \"flag\" column but with a different name.\n",
      "\n",
      "Making a new column with Pandas is very similar to making a new Python variable. You simply type the name of the dataframe with square brackets `[]` and include the name of your new desired column that doesn't exist yet. Then you assign it the value of the already existing flag column.\n",
      "slave_voyages_df['national_affiliation'] = slave_voyages_df['flag']\n",
      "If you scroll all the way to the end of the dataframe, you can see that we added a new column called \"national_affiliation\" that's exactly the same as the flag column.\n",
      "slave_voyages_df.head()\n",
      "But what if we don't want multiple columns with the same information? What if we just want to rename the \"flag\" column?\n",
      "First, let's get rid of the \"national_affiliation\" column so we can try again. To remove columns, you can use the `.drop()` method and the `columns=` parameter, followed by the column you'd like to drop.\n",
      "slave_voyages_df = slave_voyages_df.drop(columns='national_affiliation')\n",
      "slave_voyages_df.columns\n",
      "A better way to rename columns is with the `.rename()` method and the `columns=` parameter that includes the original name of the column followed by a colon and the new desired name of the column.\n",
      "slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "Renaming the \"flag\" column as we did above will only momentarily change that column's name. If we display our dataframe, we'll see that the column name has *not* changed permamently.\n",
      "slave_voyages_df.head(5)\n",
      "To save changes in your dataframe, you need to keep re-assigning your dataframe, as below.\n",
      "slave_voyages_df = slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "slave_voyages_df.head(5)\n",
      "## Select Columns\n",
      "To isolate a column with the csv module was kind of a pain. We had to loop through each row and index a certain value in each row's list. But with Pandas it's very simple to select a column. You can do so by typing the name of the dataframe accompanied by square brackets `[]` with the name of the column in quotation marks.\n",
      "slave_voyages_df['vessel_name']\n",
      "type(slave_voyages_df['vessel_name'])\n",
      "Each column in a dataframe is technically a Pandas object called a \"Series.\" We won't worry too much about the \"Series\" object right now, except to note that a Series object does not display as nicely as a DataFrame. To select a column as a dataframe, you can use two square brackets `[[]]`\n",
      "slave_voyages_df[['vessel_name']]\n",
      "type(slave_voyages_df[['vessel_name']])\n",
      "With two square brackets, you can also select and isolate multiple columns at the same time.\n",
      "slave_voyages_df[['vessel_name', 'national_affiliation', 'year_of_arrival']]\n",
      "## Filter Data\n",
      "You can also filter data based on different criteria. For example, let's say we wanted to look at the slave voyages whose national affiliation was American. You can evaluate each row in a column by isolating that column and then using the comparison operators that we discussed a few lessons ago, such as equals `==`.\n",
      "slave_voyages_df['national_affiliation'] == \"U.S.A.\"\n",
      "The output above is an evaluation of whether each row in the \"flag\" column equals \"U.S.A.\" or not. This function becomes much more helpful when we use this comparison as a selection criteria in its own right.\n",
      "\n",
      "Just as we can select a column by placing its name inside square brackets, we can select filtered data by placing a filter inside square brackets `[]`. Essentially, the line below is saying give me all the rows in the dataframe that match `slave_voyages_df['flag'] == \"U.S.A.\"`\n",
      "slave_voyages_df[slave_voyages_df['national_affiliation'] == \"U.S.A.\"]\n",
      "You can do the same thing with integers and other comparison operators. For example, if we wanted to filter for only the slave voyages that arrived after 1830, we could do so with `slave_voyages_df['year_of_arrival'] > 1830`\n",
      "slave_voyages_df['year_of_arrival'] > 1830\n",
      "slave_voyages_df[slave_voyages_df['year_of_arrival'] > 1830]\n",
      "## Sort Columns\n",
      "You can sort a dataframe with the `.sort_values()` method, inside of which you include the parameter `by=` and indicate the name of the column you want to sort by (written in quotation marks).\n",
      "slave_voyages_df.sort_values(by='total_disembarked')\n",
      "Take a look at the way the dataframe above is sorted. Do you notice that something seems a bit off?\n",
      "\n",
      "By default, Pandas will sort in \"ascending\" order, from the smallest value to the largest value. If you want to sort the largest values first, you need to include another parameter `ascending=False`.\n",
      "slave_voyages_df.sort_values(by='total_disembarked', ascending=False)\n",
      "If you want to sort a Series object, you don't need to use the `by=` paramter.\n",
      "slave_voyages_df['total_disembarked'].sort_values(ascending=False)\n",
      "## Count Values in Columns\n",
      "To count the unique values in a column, you can use the `.value_counts()` method.\n",
      "slave_voyages_df['national_affiliation'].value_counts()\n",
      "slave_voyages_df['vessel_name'].value_counts()\n",
      "slave_voyages_df['place_of_slave_disembarkation'].value_counts()\n",
      "## Calculate Columns\n",
      "| Pandas calculations | Explanation                         |\n",
      "|----------|-------------------------------------|\n",
      "| `.count()`    | Number of observations    |\n",
      "| `.sum()`      | Sum of values                       |\n",
      "| `.mean()`     | Mean of values                      |\n",
      "| `.median()`   | Median of values         |\n",
      "| `.min()`      | Minimum                             |\n",
      "| `.max()`      | Maximum                             |\n",
      "| `.mode()`     | Mode                                |\n",
      "| `.std()`      | Unbiased standard deviation         |\n",
      "\n",
      "\n",
      "You can do different calculations on columns with built-in Pandas functions, such as `.sum()` and `.max()\n",
      "slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum() - slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_disembarked'].mean()\n",
      "slave_voyages_df['total_disembarked'].max()\n",
      "## Groupby Columns\n",
      "One powerful Pandas function is called `.groupby()`, which allows you to aggregate data and perform calculations on it. For example, if we wanted to see how many enslaved people were transported by each nation, we could group by, or aggregate our data based on, \"national_affiliation\". The first step to using groupby is to type the name of your dataframe followed by `.groupby()` with the column you'd like to aggregate based on. \n",
      "slave_voyages_df.groupby('national_affiliation')\n",
      "This action will created a \"groupby\" object, which you won't be able to access unless you perform a calculation on it, such as `.sum()`\n",
      "slave_voyages_df.groupby('national_affiliation').sum()\n",
      "If we want to isolate only the \"total_disembarked\" column from this groupby calculation, then we can add in \"total_disembarked\" in square brackets `[]`\n",
      "slave_voyages_df.groupby('national_affiliation')['total_disembarked'].sum().sort_values(ascending=False)\n",
      "slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "Let's save this groupby calculation into a new variable called `national_totals`\n",
      "national_totals = slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "## Make Plots \n",
      "We're going to talk about making plots and data visualizations more extensively later. But for now we're going to introduce the fact that you can make simple plots simply by using the [`.plot()` method](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html) that's built into Pandas.\n",
      "national_totals.plot(kind='bar')\n",
      "national_totals.plot(kind='barh')\n",
      "## Time Series\n",
      "slave_voyages_df['year_of_arrival'].dtypes\n",
      "Transform an integer into a datetime\n",
      "slave_voyages_df['year_of_arrival'] = pd.to_datetime(slave_voyages_df['year_of_arrival'], format='%Y',errors='coerce')\n",
      "Number of voyages over time\n",
      "slave_voyages_df['year_of_arrival'].value_counts().plot()\n",
      "Total number of enslaved people disembarked over time\n",
      "slave_voyages_df.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "usa_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='U.S.A.']\n",
      "usa_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "gb_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='Great Britain']\n",
      "gb_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "# Data Analysis\n",
      "\n",
      "# Pandas — More Targeted Analysis\n",
      "[Download this \"Pandas — More Targeted Analysis\" notebook and other relevant files here](https://melaniewalsh.org/Functions-More-Pandas.zip)\n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100% border=2>\n",
      "\n",
      "import pandas as pd\n",
      "pudding_film_df = pd.read_csv('../data/Pudding/Pudding-Film-Dialogue-Clean.csv', encoding='utf-8', sep=',')\n",
      "pudding_film_df.head()\n",
      "## Filter Data\n",
      "Let's create two filtered dataframes for all the men characters and all the women characters.\n",
      "men_film_df = pudding_film_df[pudding_film_df['gender'] == 'Man']\n",
      "women_film_df = pudding_film_df[pudding_film_df['gender'] == 'Woman']\n",
      "## Men in Film\n",
      "How many average words do the men in this dataset speak? What's the average age for the men characters?\n",
      "men_film_df.describe()\n",
      "Based on this description, the average number of words spoken is 932. The average age is 44 years old.\n",
      "Now let's find the movies with the highest proportion of men speaking.\n",
      "men_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "men_titles_aggregated = men_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "men_titles_aggregated.sort_values(by='proportion_of_dialogue', ascending=False)[:20]\n",
      "## Women in Film\n",
      "How many average words do the women in this dataset speak? What's the average age for the women characters?\n",
      "women_film_df.describe()\n",
      "Based on this description, the average number of words spoken is 850. The average age is 37 years old.\n",
      "women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "women_titles_aggregated = women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "women_titles_aggregated.sort_values(by='proportion_of_dialogue', ascending=False)[:20]\n",
      "## Create an Interactive Data Visualization\n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100% border=2>\n",
      "To create the fancy data visualizations in their piece, The Pudding used an entirely different programming language called JavaScript, the most common programming language for the web. They specifically used a data viz library called [d3](https://observablehq.com/@d3/gallery) and a \"scrollytelling\" library called [Scrollama](https://pudding.cool/process/introducing-scrollama/).\n",
      "Just for fun, I thought we could try approximate one of their data visualizations with a Python interactive data visualization library called Bokeh. If this code doesn't work for you, don't worry about it. It's just for fun. \n",
      "To install Bokeh, run the cell below:\n",
      "!pip install bokeh\n",
      "We'll need to import a bunch of specific modules from Bokeh.\n",
      "from bokeh.plotting import figure, show\n",
      "from bokeh.models import ColumnDataSource\n",
      "from bokeh.models.tools import HoverTool\n",
      "from bokeh.io import output_notebook, show\n",
      "from bokeh.plotting import output_file, save\n",
      "from bokeh.palettes import RdBu\n",
      "from bokeh.transform import linear_cmap\n",
      "We also need to set it up to work in a Jupyter notebook.\n",
      "output_notebook()\n",
      "women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "women_film_groupby = women_film_df.groupby('title')[['proportion_of_dialogue']].sum()\n",
      "# Set up the source data that will suppply the x,y columns and the film title hover text\n",
      "\n",
      "source = ColumnDataSource(\n",
      "    data=dict(\n",
      "    x= women_film_groupby['proportion_of_dialogue'],\n",
      "    y= [0 for title in women_film_groupby.index],\n",
      "    title= women_film_groupby.index\n",
      "                             ))\n",
      "# Set the hover tool tip to the film title\n",
      "\n",
      "TOOLTIPS = [(\"Title\", \"@title\")]\n",
      "\n",
      "# Set up the data viz figure including width, height and labels\n",
      "\n",
      "p = figure(tooltips=TOOLTIPS, plot_width=2000,plot_height=500,\n",
      "           title=\"Film Dialogue Gender Breakdown\", x_axis_label = 'Proportion of Men —> Women Speaking')\n",
      "\n",
      "p.title.text_font_size = '18pt'\n",
      "p.yaxis.bounds = [0, 0]\n",
      "\n",
      "# Create a red to blue color palette\n",
      "\n",
      "mapper = linear_cmap(field_name='x', palette=RdBu[4], low=0, high=1)\n",
      "\n",
      "# Supply inidivudal points values\n",
      "p.circle(x='x',\n",
      "         y='y',\n",
      "         size = 20,\n",
      "         source=source, \n",
      "         alpha=0.7,\n",
      "        color= mapper)\n",
      "\n",
      "# Output file as an html file as well as show it in this Jupyter notebook\n",
      "output_file('film-dialogue.html')\n",
      "\n",
      "show(p)\n",
      "[Explore our interactive data viz here](film-dialogue.html)\n",
      "# Pandas — Merge Datasets\n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100% border=2 border=2>\n",
      "\n",
      "<img src='../images/Pudding-Github-data.png' width=100%, border=2 border=2>\n",
      "\n",
      "import pandas as pd\n",
      "The Pudding published one CSV file called \"meta_data7.csv\" that contains, among things, the title of each movie, the year of its release, and its box office gross. Let's read in this CSV file and make it into a dataframe.\n",
      "metadata = pd.read_csv('../data/Pudding/meta_data7.csv', delimiter=',', encoding='utf-8')\n",
      "metadata\n",
      "We're going to drop the column \"lines_data\", which contains information about when during the film each character speaks.\n",
      "metadata = metadata.drop(columns='lines_data')\n",
      "The Pudding published another CSV file called \"character_list5.csv\" that contains, among other things, the name, gender, and age of each character as well as the number of words the character speaks.\n",
      "characters = pd.read_csv('../data/Pudding/character_list5.csv', delimiter=',',encoding='utf-8')\n",
      "characters\n",
      "As you can see, the characters dataframe doesn't include the actual title of the movie in which the character appears or the movie's release year or box office gross. The metadata datafram doesn't contain any information about the characters. We want that info all in one place. So how can we combine all of this data together?\n",
      "## `pd.merge()`\n",
      "If you look closely, there's one column that both datasets share in common: \"script_id\". If two datasets share at least one column in common, then you can merge them together based on this column. You can use the `pd.merge()` function and then type in the name of the first dataframe, the name of the second dataframe, and the shared column to be merged on.\n",
      "pd.merge(characters, metadata, on='script_id')\n",
      "merged_movie_character = pd.merge(characters, metadata, on='script_id')\n",
      "## Calculate Dialogue Proportions\n",
      "We're going to add one more column to this dataset before the next lesson. We're going to calculate the proportion of words spoken in each film by each character. To do so, we're going to `.groupby()` the movie's title and calculate the sum total number of words spoken in each movie. \n",
      "merged_movie_character.groupby(['title'])[['words']].sum()\n",
      "If we use the `.transform()`, we can turn this groupby into a single column of data.\n",
      "merged_movie_character.groupby(['title'])[['words']].transform(sum)\n",
      "total_movie_words = merged_movie_character.groupby(['title'])[['words']].transform(sum)\n",
      "Then we're going to divide the total number of words spoken by each character by the total number of words spoken in each film.\n",
      "total_character_words = merged_movie_character[['words']]\n",
      "total_character_words / total_movie_words\n",
      "dialogue_proportion = total_character_words / total_movie_words\n",
      "Then we're going to add it as a new column.\n",
      "merged_movie_character['proportion_of_dialogue'] = dialogue_proportion\n",
      "merged_movie_character\n",
      "## Write to CSV File\n",
      "Finally, we're going to output this merged and more comprehensive dataset to a CSV file by using the `.to_csv()` method. We set the `index` parameter to `False` to remove the index column (the numbers in the left-most column).\n",
      "merged_movie_character.to_csv('../data/Pudding/Merged-Pudding-Film-Dialogue.csv', sep=',', encoding='utf-8', index=False)\n",
      "## Pandas — Exploratory Data Analysis\n",
      "[Download this \"Pandas — Exploratory Data Analysis\" notebook and other relevant files here](https://melaniewalsh.org/Functions-More-Pandas.zip)\n",
      "<img src='../images/Pudding-Github-data.png' width=100%, border=2>\n",
      "\n",
      "## Data Biography\n",
      "The very first step of a responsible cultural analytics/digital humanities EDA, however, doesn't require any code or computation at all. The very first step is to understand the data's \"biography,\" as Heather Krause lays it out in [\"Data Biographies\"](https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/). Your assignment for Thursday is to make sure you can answer these questions. It will be helpful to read [\"FAQ for the “Film Dialogue, By Gender” Project](https://medium.com/@matthew_daniels/faq-for-the-film-dialogue-by-gender-project-40078209f751).\n",
      "### Where did the data come from?\n",
      "\n",
      "### Who collected the data?\n",
      "\n",
      "### How was data collected? And how was \"gender\" calculated?\n",
      "\n",
      "### Why was the data collected?\n",
      "\n",
      "## Import Pandas\n",
      "The rest of our EDA will be accomplished with the help of the Python library Pandas.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "We will read in our Pudding film dialogue dataset, which we created by merging together a few different datasets that were published by Hannah Andersen and Matt Daniels. This CSV file includes the word **salty**—'../data/Pudding/Pudding-Film-Dialogue-**Salty**.csv'—because it contains a few artificially added problems and errors. It's inspired by Matthew Lincoln's R Package for artificially creating messy data, [salty](https://rdrr.io/cran/salty/).\n",
      "pudding_film_df = pd.read_csv('../data/Pudding/Pudding-Film-Dialogue-Salty.csv', delimiter=',', encoding='utf-8')\n",
      "pudding_film_df.sample(10)\n",
      "## Examine the Data\n",
      "How many rows and columns are in our dataset? What types of data are here? What are the names of the columns?\n",
      "pudding_film_df.shape\n",
      "pudding_film_df.dtypes\n",
      "pudding_film_df.columns\n",
      "## `.describe()`\n",
      "The `.describe()` method will give you a summary table of all the numerical values in your dataframe.\n",
      "pudding_film_df.describe()\n",
      "Do you notice any outliers, anomalies, or potential problems here?\n",
      "pudding_film_df[pudding_film_df['age'] == 2013]\n",
      "## Rename and Drop Columns\n",
      "EDA often includes renaming and dropping columns to make the data easier to work with and more legible.\n",
      "pudding_film_df = pudding_film_df.rename(columns={'imdb_character_name': 'character', 'year': 'release_year'})\n",
      "pudding_film_df.head()\n",
      "pudding_film_df = pudding_film_df.drop(columns='imdb_id')\n",
      "## Check for Duplicates or Missing Values (`NaN`)\n",
      "## Duplicates\n",
      "You can check for duplicate rows by using the `.duplicated()` method and seting the parameter `keep=False` (which will display both duplicated values as opposed to only the first value `keep='first'` or the last value `keep='last'`).\n",
      "pudding_film_df.duplicated(keep=False)\n",
      "The output above is reporting whether each row in the dataset is a duplicate `True` or not `False`. We can use the `.duplicated()` method inside a filter to isolate only the rows in the dataframe that are exact duplicates.\n",
      "pudding_film_df[pudding_film_df.duplicated(keep=False)]\n",
      "You can drop duplicates from the dataframe with the `.drop_duplicates()` method and choose to \"keep\" the \"first\" instance of the duplicate or the \"last\" instance.\n",
      "pudding_film_df = pudding_film_df.drop_duplicates(keep='first')\n",
      "Now if we check the data for duplicates again, they should be all gone.\n",
      "pudding_film_df[pudding_film_df.duplicated(keep=False)]\n",
      "## Missing/Null Values\n",
      "Missing values in a Pandas dataframe are interpreted as a special `NaN` value. This is important to remember for a few reasons.\n",
      "\n",
      "First, if we want to find out how many rows in our dataset contain blank or missing values, we'll need to use a special `.isnull()` method that checks for `NaN` values. If we check for missing values in our \"character\" column, for example, we will discover that there are two rows that don't have any character names.\n",
      "pudding_film_df['character'].isnull() == True\n",
      "pudding_film_df[pudding_film_df['character'].isnull() == True]\n",
      "This is important information for the sake of better understanding our dataset. But it's also important because `NaN` values can mess up later calculations or transformations that you might attempt. For this reason, we're going to replace or \"fill\" these `NaN` values with the string \"No Character Data\" by using the `.fillna()` method.\n",
      "pudding_film_df['character'] = pudding_film_df['character'].fillna('No Character Data')\n",
      "pudding_film_df[pudding_film_df['character'].isnull() == True]\n",
      "pudding_film_df[pudding_film_df['character'] == 'No Character Data']\n",
      "With the `.isnull()` method, we can also discover that there are thousands of rows that don't contain any box office gross information as well as thousands of characters that don't have any associated age information.\n",
      "pudding_film_df[pudding_film_df['gross'].isnull() == True]\n",
      "pudding_film_df[pudding_film_df['age'].isnull() == True]\n",
      "## Check Distributions of Data\n",
      "## `.hist()`\n",
      "One quick way to get a sense of the distributions of your data is with the [`hist()` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.hist.html#pandas.DataFrame.hist), which creates a \"histogram\" of every numerical category in your dataframe. A histogram is a chart the represents the frequency of each value. A histogram looks a lot like a bar chart — and it *is* a lot like a bar chart — except a histogram represents quantitative values as opposed to qualitative values.\n",
      "pudding_film_df.hist()\n",
      "If you use the parameter `figsize(x_size, y_size)` and specify specific sizes, you can make the grid of histogram charts bigger.\n",
      "pudding_film_df.hist(figsize=(10,10))\n",
      "You can also select individual columns and plot histograms based on that column. \n",
      "pudding_film_df['age'].hist()\n",
      "If you add in the `range= (x_size, y_size)` parameter, you can focus your histogram on a specific area.\n",
      "pudding_film_df['age'].hist(range=(0, 100))\n",
      "pudding_film_df['gross'].hist()\n",
      "You can also specify how finely to group the data with the `bins` parameter. You might think about it like putting the data into a bunch of physical bins 🗑️  If we set the `bins` parameter to 100, we can a more nuanced view of the box office gross distribution.\n",
      "pudding_film_df['gross'].hist(bins=100, range=(0, 1000))\n",
      "pudding_film_df['release_year'].hist()\n",
      "pudding_film_df['release_year'].hist(bins=50, range=(1940, 2020))\n",
      "If you specifically select a column that contains qualitative data as opposed to quantitative data, you can also force Pandas to make a histogram.\n",
      "pudding_film_df['gender'].hist()\n",
      "## Clean and Transform Data\n",
      "## Pandas `.str` Methods\n",
      "| **Pandas String Method** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `pandas_column.str.lower()`         | makes the string in each row lowercase                                                                                |\n",
      "| `pandas_column.str.upper()`         | makes the string in each row uppercase                                                |\n",
      "| `pandas_column.str.title()`         | makes the string in each row titlecase                                                |\n",
      "| `pandas_column.str.replace('old string', 'new string')`      | replaces `old string` with `new string` for each row |\n",
      "| `pandas_column.str.contains('some string')`      | tests whether string in each row contains \"some string\" |\n",
      "| `pandas_column.str.split('delim')`          | returns a list of substrings separated by the given delimiter |\n",
      "| `pandas_column.str.join(list)`         | opposite of split(), joins the elements in the given list together using the string                                                                        |\n",
      "                                                            \n",
      "Remember all the special things that you can do with string data aka [string methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html)? Well Pandas columns that contain string data also have special [Pandas string methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#string-methods). Many of them are the same or almost the same as regular Python string methods, except  they will transform every single string value in a column, and you have to add the code `.str` to the method chain.\n",
      "For example, to transform every character's name in the \"character\" column from lowercase to uppercase, you can add `.str.upper()` to `pudding_film_df['character']`\n",
      "pudding_film_df['character'] = pudding_film_df['character'].str.upper()\n",
      "pudding_film_df.head()\n",
      "To transform every character's name in the \"character\" column back to lowercase, you can add`.str.lower()` to `pudding_film_df['character']`\n",
      "pudding_film_df['character'] = pudding_film_df['character'].str.lower()\n",
      "pudding_film_df.sample(10)\n",
      "If we want to replace the gender columns's single letter abbreviation for \"male\" / \"female\" (sex) with \"man\" / \"woman\" (gender identity), we could use the `.str.replace()` method. \n",
      "pudding_film_df['gender'] = pudding_film_df['gender'].str.replace('m', 'man')\n",
      "pudding_film_df['gender'] = pudding_film_df['gender'].str.replace('f', 'woman')\n",
      "pudding_film_df.sample(10)\n",
      "We can also use the `.str.contains()` to search for particular words or phrases in a column, such as \"Star Wars\" or \"Mean Girls\".\n",
      "pudding_film_df[pudding_film_df['title'].str.contains('Star Wars')]\n",
      "pudding_film_df[pudding_film_df['title'].str.contains('Mean Girls')]\n",
      "## Applying Functions\n",
      "With the `.apply()` method, you can run a function on every single row in a Pandas column or dataframe.\n",
      "def make_text_lower_case(text):\n",
      "    lower_case_text = text.upper()\n",
      "    return lower_case_text\n",
      "def make_text_upper_case(text):\n",
      "    upper_case_text = text.upper()\n",
      "    return upper_case_text\n",
      "def make_text_title_case(text):\n",
      "    title_case_text = text.title()\n",
      "    return title_case_text\n",
      "make_text_upper_case(\"betty\")\n",
      "pudding_film_df['character'].apply(make_text_upper_case)\n",
      "pudding_film_df['character'].apply(make_text_title_case)\n",
      "pudding_film_df['character'] = pudding_film_df['character'].apply(make_text_title_case)\n",
      "pudding_film_df.sample(10)\n",
      "def clarify_gender(text):\n",
      "    gender = ''\n",
      "    if text == 'woman':\n",
      "        gender = 'Woman'\n",
      "    elif text == 'man':\n",
      "        gender = 'Man'\n",
      "    return gender\n",
      "def calculate_after_2000(year):\n",
      "    is_millennium = ''\n",
      "    if year >= 2000:\n",
      "        is_millennium = True\n",
      "    elif year < 2000:\n",
      "        is_millennium = False\n",
      "    return is_millennium\n",
      "pudding_film_df['gender'] = pudding_film_df['gender'].apply(clarify_gender)\n",
      "pudding_film_df.head()\n",
      "pudding_film_df['after_2000'] = pudding_film_df['release_year'].apply(calculate_after_2000)\n",
      "pudding_film_df.head()\n",
      "## Write to CSV\n",
      "To write a dataframe to a new CSV file, you can add the `.to_csv()` method after the name of your dataframe.\n",
      "\n",
      "`your_dataframe.to_csv('Your-Desired-File-Name', encoding='utf-8', sep=',', index=False)`\n",
      "pudding_film_df.to_csv('../data/Pudding/Pudding-Film-Dialogue-Clean.csv', encoding='utf-8', sep=',', index=False)\n",
      "\n",
      "# Pandas\n",
      "In this lesson, we're going to learn about [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html), a powerful Python library for working with tabular data like CSV files. Things that were kind of clunky to accomplish with the CSV module are much easier to accomplish with Pandas.\n",
      "\n",
      "Here are some of the things you can do with Pandas:\n",
      "\n",
      "* Easily read and write CSV files\n",
      "* Sort and filter data\n",
      "* Analyze data\n",
      "* Combine datasets\n",
      "* Create data visualizations\n",
      "## Our Dataset\n",
      "<img src=\"../images/Slave-Voyages.png\" width=100%, border=2 border=2>\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson,\n",
      "<a href=\"https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death\">“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>\n",
      "The dataset that we're going to be working with in this lesson is taken from [The Trans-Atlantic Slave Trade Database](https://www.slavevoyages.org/voyage/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of enslaved men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "\n",
      "We're working with this data for a number of reasons. First, the *Slave Voyages* project is a major data-driven contribution to the history of slavery and to the field of the digital humanities. Second, working with this data makes clear that, as Johnson writes, computation and data alone cannot capture \"the violent quandary\" of slavery or bring about justice. As we learn how to manipulate and analyze this dataset with Pandas, we should be reminded, at every step, of the challenges that are involved in using this data responsibly and intentionally.\n",
      "## Import Pandas\n",
      "To use the Pandas library, we first need to `import` it, as below. This `import` statement not only imports the library but also gives it a nickname: `as` \"pd\" instead of \"pandas.\" You'll run across this alias convention a lot. It's a way to save time and not have to write the entire word \"pandas\" over and over again.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "To read in a CSV file, you use the function `pd.read_csv()` and insert the name of your desired file path. You can also add the \"delimiter\" argument to specify the delimiter for your file.\n",
      "slave_voyages_df = pd.read_csv('../data/Slave-Voyages-Trans-Atlantic-North-America.csv', delimiter=\",\")\n",
      "```{tip} \n",
      "If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "```\n",
      "**Pro tip!** If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "help(pd.read_csv)\n",
      "When you read in a CSV file with Pandas, you transform it into a Pandas object called a dataframe.\n",
      "type(slave_voyages_df)\n",
      "## Display the Data\n",
      "In a Jupyter notebook, you can display a dataframe simply by running a cell with the name of the dataframe. The `NaN` value is the Pandas value for any missing data (`NaN` values have special properties that we'll get to later).\n",
      "slave_voyages_df\n",
      "You should always eyeball your dataset either by looking at the first few rows or by looking at a few random rows. To look at the first few rows, you can use a method called `.head()` (very similar to the `head` command on the command line).\n",
      "slave_voyages_df.head(10)\n",
      "To look at a random sample of rows, you can use the `.sample()` method. You might run this cell a few times to make sure everything looks ok.\n",
      "slave_voyages_df.sample(10)\n",
      "## Examine the Data\n",
      "You can check the dataframe's \"shape\" or how many rows vs columns it contains with `.shape` (number of rows, number of columns)\n",
      "slave_voyages_df.shape\n",
      "You can (and always should) check the data types that each column contains with `.dtypes`\n",
      "\n",
      "\n",
      "| **Pandas dtypes** |                                                                                    |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `object`         | string                                                                               |\n",
      "| `float64`         | float                                               |\n",
      "| `int64`       | integer                                                        |\n",
      "| `datetime64`       |  date time              \n",
      "slave_voyages_df.dtypes\n",
      "It's important to always check the data types in your dataframe. For example, sometimes numeric values, which you want to do mathematical calculations with, will accidentally be interpreted as a string object. To perform calculations on this data, you would need to first convert that column from a string to an integer.\n",
      "\n",
      "The data types in our Trans-Atlantic Slave Trade dataset look ok, except the column \"year_of_arrival,\" which is currently interpreted as an integer value when ideally it should be a datetime value.\n",
      "You can also check the column names of your dataframe with `.columns`\n",
      "slave_voyages_df.columns\n",
      "## Rename and Drop Columns\n",
      "Let's say we wanted to rename the \"flag\" column as \"national_affiliation.\" One way to approach this goal would be to make a new column identical to the \"flag\" column but with a different name.\n",
      "\n",
      "Making a new column with Pandas is very similar to making a new Python variable. You simply type the name of the dataframe with square brackets `[]` and include the name of your new desired column that doesn't exist yet. Then you assign it the value of the already existing flag column.\n",
      "slave_voyages_df['national_affiliation'] = slave_voyages_df['flag']\n",
      "If you scroll all the way to the end of the dataframe, you can see that we added a new column called \"national_affiliation\" that's exactly the same as the flag column.\n",
      "slave_voyages_df.head()\n",
      "But what if we don't want multiple columns with the same information? What if we just want to rename the \"flag\" column?\n",
      "First, let's get rid of the \"national_affiliation\" column so we can try again. To remove columns, you can use the `.drop()` method and the `columns=` parameter, followed by the column you'd like to drop.\n",
      "slave_voyages_df = slave_voyages_df.drop(columns='national_affiliation')\n",
      "slave_voyages_df.columns\n",
      "A better way to rename columns is with the `.rename()` method and the `columns=` parameter that includes the original name of the column followed by a colon and the new desired name of the column.\n",
      "slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "Renaming the \"flag\" column as we did above will only momentarily change that column's name. If we display our dataframe, we'll see that the column name has *not* changed permamently.\n",
      "slave_voyages_df.head(5)\n",
      "To save changes in your dataframe, you need to keep re-assigning your dataframe, as below.\n",
      "slave_voyages_df = slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "slave_voyages_df.head(5)\n",
      "## Select Columns\n",
      "To isolate a column with the csv module was kind of a pain. We had to loop through each row and index a certain value in each row's list. But with Pandas it's very simple to select a column. You can do so by typing the name of the dataframe accompanied by square brackets `[]` with the name of the column in quotation marks.\n",
      "slave_voyages_df['vessel_name']\n",
      "type(slave_voyages_df['vessel_name'])\n",
      "Each column in a dataframe is technically a Pandas object called a \"Series.\" We won't worry too much about the \"Series\" object right now, except to note that a Series object does not display as nicely as a DataFrame. To select a column as a dataframe, you can use two square brackets `[[]]`\n",
      "slave_voyages_df[['vessel_name']]\n",
      "type(slave_voyages_df[['vessel_name']])\n",
      "With two square brackets, you can also select and isolate multiple columns at the same time.\n",
      "slave_voyages_df[['vessel_name', 'national_affiliation', 'year_of_arrival']]\n",
      "## Filter Data\n",
      "You can also filter data based on different criteria. For example, let's say we wanted to look at the slave voyages whose national affiliation was American. You can evaluate each row in a column by isolating that column and then using the comparison operators that we discussed a few lessons ago, such as equals `==`.\n",
      "slave_voyages_df['national_affiliation'] == \"U.S.A.\"\n",
      "The output above is an evaluation of whether each row in the \"flag\" column equals \"U.S.A.\" or not. This function becomes much more helpful when we use this comparison as a selection criteria in its own right.\n",
      "\n",
      "Just as we can select a column by placing its name inside square brackets, we can select filtered data by placing a filter inside square brackets `[]`. Essentially, the line below is saying give me all the rows in the dataframe that match `slave_voyages_df['flag'] == \"U.S.A.\"`\n",
      "slave_voyages_df[slave_voyages_df['national_affiliation'] == \"U.S.A.\"]\n",
      "You can do the same thing with integers and other comparison operators. For example, if we wanted to filter for only the slave voyages that arrived after 1830, we could do so with `slave_voyages_df['year_of_arrival'] > 1830`\n",
      "slave_voyages_df['year_of_arrival'] > 1830\n",
      "slave_voyages_df[slave_voyages_df['year_of_arrival'] > 1830]\n",
      "## Sort Columns\n",
      "You can sort a dataframe with the `.sort_values()` method, inside of which you include the parameter `by=` and indicate the name of the column you want to sort by (written in quotation marks).\n",
      "slave_voyages_df.sort_values(by='total_disembarked')\n",
      "Take a look at the way the dataframe above is sorted. Do you notice that something seems a bit off?\n",
      "\n",
      "By default, Pandas will sort in \"ascending\" order, from the smallest value to the largest value. If you want to sort the largest values first, you need to include another parameter `ascending=False`.\n",
      "slave_voyages_df.sort_values(by='total_disembarked', ascending=False)\n",
      "If you want to sort a Series object, you don't need to use the `by=` paramter.\n",
      "slave_voyages_df['total_disembarked'].sort_values(ascending=False)\n",
      "## Count Values in Columns\n",
      "To count the unique values in a column, you can use the `.value_counts()` method.\n",
      "slave_voyages_df['national_affiliation'].value_counts()\n",
      "slave_voyages_df['vessel_name'].value_counts()\n",
      "slave_voyages_df['place_of_slave_disembarkation'].value_counts()\n",
      "## Calculate Columns\n",
      "| Pandas calculations | Explanation                         |\n",
      "|----------|-------------------------------------|\n",
      "| `.count()`    | Number of observations    |\n",
      "| `.sum()`      | Sum of values                       |\n",
      "| `.mean()`     | Mean of values                      |\n",
      "| `.median()`   | Median of values         |\n",
      "| `.min()`      | Minimum                             |\n",
      "| `.max()`      | Maximum                             |\n",
      "| `.mode()`     | Mode                                |\n",
      "| `.std()`      | Unbiased standard deviation         |\n",
      "\n",
      "\n",
      "You can do different calculations on columns with built-in Pandas functions, such as `.sum()` and `.max()\n",
      "slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum() - slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_disembarked'].mean()\n",
      "slave_voyages_df['total_disembarked'].max()\n",
      "## Groupby Columns\n",
      "One powerful Pandas function is called `.groupby()`, which allows you to aggregate data and perform calculations on it. For example, if we wanted to see how many enslaved people were transported by each nation, we could group by, or aggregate our data based on, \"national_affiliation\". The first step to using groupby is to type the name of your dataframe followed by `.groupby()` with the column you'd like to aggregate based on. \n",
      "slave_voyages_df.groupby('national_affiliation')\n",
      "This action will created a \"groupby\" object, which you won't be able to access unless you perform a calculation on it, such as `.sum()`\n",
      "slave_voyages_df.groupby('national_affiliation').sum()\n",
      "If we want to isolate only the \"total_disembarked\" column from this groupby calculation, then we can add in \"total_disembarked\" in square brackets `[]`\n",
      "slave_voyages_df.groupby('national_affiliation')['total_disembarked'].sum().sort_values(ascending=False)\n",
      "slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "Let's save this groupby calculation into a new variable called `national_totals`\n",
      "national_totals = slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "## Make Plots \n",
      "We're going to talk about making plots and data visualizations more extensively later. But for now we're going to introduce the fact that you can make simple plots simply by using the [`.plot()` method](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html) that's built into Pandas.\n",
      "national_totals.plot(kind='bar')\n",
      "national_totals.plot(kind='barh')\n",
      "## Time Series\n",
      "slave_voyages_df['year_of_arrival'].dtypes\n",
      "Transform an integer into a datetime\n",
      "slave_voyages_df['year_of_arrival'] = pd.to_datetime(slave_voyages_df['year_of_arrival'], format='%Y',errors='coerce')\n",
      "Number of voyages over time\n",
      "slave_voyages_df['year_of_arrival'].value_counts().plot()\n",
      "Total number of enslaved people disembarked over time\n",
      "slave_voyages_df.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "usa_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='U.S.A.']\n",
      "usa_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "gb_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='Great Britain']\n",
      "gb_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "# Pandas\n",
      "In this lesson, we're going to learn about [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html), a powerful Python library for working with tabular data like CSV files. Things that were kind of clunky to accomplish with the CSV module are much easier to accomplish with Pandas.\n",
      "\n",
      "Here are some of the things you can do with Pandas:\n",
      "\n",
      "* Easily read and write CSV files\n",
      "* Sort and filter data\n",
      "* Analyze data\n",
      "* Combine datasets\n",
      "* Create data visualizations\n",
      "## Our Dataset\n",
      "<img src=\"../images/Slave-Voyages.png\" width=100%>\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson,\n",
      "<a href=\"https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death\">“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>\n",
      "The dataset that we're going to be working with in this lesson is taken from [The Trans-Atlantic Slave Trade Database](https://www.slavevoyages.org/voyage/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of enslaved men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "\n",
      "We're working with this data for a number of reasons. First, the *Slave Voyages* project is a major data-driven contribution to the history of slavery and to the field of the digital humanities. Second, working with this data makes clear that, as Johnson writes, computation and data alone cannot capture \"the violent quandary\" of slavery or bring about justice. As we learn how to manipulate and analyze this dataset with Pandas, we should be reminded, at every step, of the challenges that are involved in using this data responsibly and intentionally.\n",
      "## Import Pandas\n",
      "To use the Pandas library, we first need to `import` it, as below. This `import` statement not only imports the library but also gives it a nickname: `as` \"pd\" instead of \"pandas.\" You'll run across this alias convention a lot. It's a way to save time and not have to write the entire word \"pandas\" over and over again.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "To read in a CSV file, you use the function `pd.read_csv()` and insert the name of your desired file path. You can also add the \"delimiter\" argument to specify the delimiter for your file.\n",
      "slave_voyages_df = pd.read_csv('../data/Slave-Voyages-Trans-Atlantic-North-America.csv', delimiter=\",\")\n",
      "```{tip} \n",
      "If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "```\n",
      "**Pro tip!** If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "help(pd.read_csv)\n",
      "When you read in a CSV file with Pandas, you transform it into a Pandas object called a dataframe.\n",
      "type(slave_voyages_df)\n",
      "## Display the Data\n",
      "In a Jupyter notebook, you can display a dataframe simply by running a cell with the name of the dataframe. The `NaN` value is the Pandas value for any missing data (`NaN` values have special properties that we'll get to later).\n",
      "slave_voyages_df\n",
      "You should always eyeball your dataset either by looking at the first few rows or by looking at a few random rows. To look at the first few rows, you can use a method called `.head()` (very similar to the `head` command on the command line).\n",
      "slave_voyages_df.head(10)\n",
      "To look at a random sample of rows, you can use the `.sample()` method. You might run this cell a few times to make sure everything looks ok.\n",
      "slave_voyages_df.sample(10)\n",
      "## Examine the Data\n",
      "You can check the dataframe's \"shape\" or how many rows vs columns it contains with `.shape` (number of rows, number of columns)\n",
      "slave_voyages_df.shape\n",
      "You can (and always should) check the data types that each column contains with `.dtypes`\n",
      "\n",
      "\n",
      "| **Pandas dtypes** |                                                                                    |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `object`         | string                                                                               |\n",
      "| `float64`         | float                                               |\n",
      "| `int64`       | integer                                                        |\n",
      "| `datetime64`       |  date time              \n",
      "slave_voyages_df.dtypes\n",
      "It's important to always check the data types in your dataframe. For example, sometimes numeric values, which you want to do mathematical calculations with, will accidentally be interpreted as a string object. To perform calculations on this data, you would need to first convert that column from a string to an integer.\n",
      "\n",
      "The data types in our Trans-Atlantic Slave Trade dataset look ok, except the column \"year_of_arrival,\" which is currently interpreted as an integer value when ideally it should be a datetime value.\n",
      "You can also check the column names of your dataframe with `.columns`\n",
      "slave_voyages_df.columns\n",
      "## Rename and Drop Columns\n",
      "Let's say we wanted to rename the \"flag\" column as \"national_affiliation.\" One way to approach this goal would be to make a new column identical to the \"flag\" column but with a different name.\n",
      "\n",
      "Making a new column with Pandas is very similar to making a new Python variable. You simply type the name of the dataframe with square brackets `[]` and include the name of your new desired column that doesn't exist yet. Then you assign it the value of the already existing flag column.\n",
      "slave_voyages_df['national_affiliation'] = slave_voyages_df['flag']\n",
      "If you scroll all the way to the end of the dataframe, you can see that we added a new column called \"national_affiliation\" that's exactly the same as the flag column.\n",
      "slave_voyages_df.head()\n",
      "But what if we don't want multiple columns with the same information? What if we just want to rename the \"flag\" column?\n",
      "First, let's get rid of the \"national_affiliation\" column so we can try again. To remove columns, you can use the `.drop()` method and the `columns=` parameter, followed by the column you'd like to drop.\n",
      "slave_voyages_df = slave_voyages_df.drop(columns='national_affiliation')\n",
      "slave_voyages_df.columns\n",
      "A better way to rename columns is with the `.rename()` method and the `columns=` parameter that includes the original name of the column followed by a colon and the new desired name of the column.\n",
      "slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "Renaming the \"flag\" column as we did above will only momentarily change that column's name. If we display our dataframe, we'll see that the column name has *not* changed permamently.\n",
      "slave_voyages_df.head(5)\n",
      "To save changes in your dataframe, you need to keep re-assigning your dataframe, as below.\n",
      "slave_voyages_df = slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "slave_voyages_df.head(5)\n",
      "## Select Columns\n",
      "To isolate a column with the csv module was kind of a pain. We had to loop through each row and index a certain value in each row's list. But with Pandas it's very simple to select a column. You can do so by typing the name of the dataframe accompanied by square brackets `[]` with the name of the column in quotation marks.\n",
      "slave_voyages_df['vessel_name']\n",
      "type(slave_voyages_df['vessel_name'])\n",
      "Each column in a dataframe is technically a Pandas object called a \"Series.\" We won't worry too much about the \"Series\" object right now, except to note that a Series object does not display as nicely as a DataFrame. To select a column as a dataframe, you can use two square brackets `[[]]`\n",
      "slave_voyages_df[['vessel_name']]\n",
      "type(slave_voyages_df[['vessel_name']])\n",
      "With two square brackets, you can also select and isolate multiple columns at the same time.\n",
      "slave_voyages_df[['vessel_name', 'national_affiliation', 'year_of_arrival']]\n",
      "## Filter Data\n",
      "You can also filter data based on different criteria. For example, let's say we wanted to look at the slave voyages whose national affiliation was American. You can evaluate each row in a column by isolating that column and then using the comparison operators that we discussed a few lessons ago, such as equals `==`.\n",
      "slave_voyages_df['national_affiliation'] == \"U.S.A.\"\n",
      "The output above is an evaluation of whether each row in the \"flag\" column equals \"U.S.A.\" or not. This function becomes much more helpful when we use this comparison as a selection criteria in its own right.\n",
      "\n",
      "Just as we can select a column by placing its name inside square brackets, we can select filtered data by placing a filter inside square brackets `[]`. Essentially, the line below is saying give me all the rows in the dataframe that match `slave_voyages_df['flag'] == \"U.S.A.\"`\n",
      "slave_voyages_df[slave_voyages_df['national_affiliation'] == \"U.S.A.\"]\n",
      "You can do the same thing with integers and other comparison operators. For example, if we wanted to filter for only the slave voyages that arrived after 1830, we could do so with `slave_voyages_df['year_of_arrival'] > 1830`\n",
      "slave_voyages_df['year_of_arrival'] > 1830\n",
      "slave_voyages_df[slave_voyages_df['year_of_arrival'] > 1830]\n",
      "## Sort Columns\n",
      "You can sort a dataframe with the `.sort_values()` method, inside of which you include the parameter `by=` and indicate the name of the column you want to sort by (written in quotation marks).\n",
      "slave_voyages_df.sort_values(by='total_disembarked')\n",
      "Take a look at the way the dataframe above is sorted. Do you notice that something seems a bit off?\n",
      "\n",
      "By default, Pandas will sort in \"ascending\" order, from the smallest value to the largest value. If you want to sort the largest values first, you need to include another parameter `ascending=False`.\n",
      "slave_voyages_df.sort_values(by='total_disembarked', ascending=False)\n",
      "If you want to sort a Series object, you don't need to use the `by=` paramter.\n",
      "slave_voyages_df['total_disembarked'].sort_values(ascending=False)\n",
      "## Count Values in Columns\n",
      "To count the unique values in a column, you can use the `.value_counts()` method.\n",
      "slave_voyages_df['national_affiliation'].value_counts()\n",
      "slave_voyages_df['vessel_name'].value_counts()\n",
      "slave_voyages_df['place_of_slave_disembarkation'].value_counts()\n",
      "## Calculate Columns\n",
      "| Pandas calculations | Explanation                         |\n",
      "|----------|-------------------------------------|\n",
      "| `.count()`    | Number of observations    |\n",
      "| `.sum()`      | Sum of values                       |\n",
      "| `.mean()`     | Mean of values                      |\n",
      "| `.median()`   | Median of values         |\n",
      "| `.min()`      | Minimum                             |\n",
      "| `.max()`      | Maximum                             |\n",
      "| `.mode()`     | Mode                                |\n",
      "| `.std()`      | Unbiased standard deviation         |\n",
      "\n",
      "\n",
      "You can do different calculations on columns with built-in Pandas functions, such as `.sum()` and `.max()\n",
      "slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum() - slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_disembarked'].mean()\n",
      "slave_voyages_df['total_disembarked'].max()\n",
      "## Groupby Columns\n",
      "One powerful Pandas function is called `.groupby()`, which allows you to aggregate data and perform calculations on it. For example, if we wanted to see how many enslaved people were transported by each nation, we could group by, or aggregate our data based on, \"national_affiliation\". The first step to using groupby is to type the name of your dataframe followed by `.groupby()` with the column you'd like to aggregate based on. \n",
      "slave_voyages_df.groupby('national_affiliation')\n",
      "This action will created a \"groupby\" object, which you won't be able to access unless you perform a calculation on it, such as `.sum()`\n",
      "slave_voyages_df.groupby('national_affiliation').sum()\n",
      "If we want to isolate only the \"total_disembarked\" column from this groupby calculation, then we can add in \"total_disembarked\" in square brackets `[]`\n",
      "slave_voyages_df.groupby('national_affiliation')['total_disembarked'].sum().sort_values(ascending=False)\n",
      "slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "Let's save this groupby calculation into a new variable called `national_totals`\n",
      "national_totals = slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "## Make Plots \n",
      "We're going to talk about making plots and data visualizations more extensively later. But for now we're going to introduce the fact that you can make simple plots simply by using the [`.plot()` method](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html) that's built into Pandas.\n",
      "national_totals.plot(kind='bar')\n",
      "national_totals.plot(kind='barh')\n",
      "## Time Series\n",
      "slave_voyages_df['year_of_arrival'].dtypes\n",
      "Transform an integer into a datetime\n",
      "slave_voyages_df['year_of_arrival'] = pd.to_datetime(slave_voyages_df['year_of_arrival'], format='%Y',errors='coerce')\n",
      "Number of voyages over time\n",
      "slave_voyages_df['year_of_arrival'].value_counts().plot()\n",
      "Total number of enslaved people disembarked over time\n",
      "slave_voyages_df.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "usa_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='U.S.A.']\n",
      "usa_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "gb_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='Great Britain']\n",
      "gb_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "## Final Project — Cultural Data Analysis\n",
      "Due Monday, May 18th by 5pm\n",
      "## Report and Findings (3-5 page paper):\n",
      "\n",
      "\n",
      "## Final Project — Cultural Data Analysis\n",
      "Due Monday, May 18th by 5pm\n",
      "## Report and Findings (3-5 page paper):\n",
      "\n",
      "\n",
      "## Lists & For Loops\n",
      "For this lesson, we're again going to draw on Anelise Shrout's [Bellevue Almshouse data](https://docs.google.com/spreadsheets/d/1uf8uaqicknrn0a6STWrVfVMScQQMtzYf5I_QyhB9r7I/edit##gid=2057113261).\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "## Lists\n",
      "Last lesson, when we worked with conditionals, we made a bunch of individual variables about each individual Irish immigrant. Often it's much more advantageous, however, to create a *collection* of values and hold them in a single place. One of the most common Python data collections is called a \"list.\" For example, rather than all these separate variables...\n",
      "**Person 1**\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "**Person 2**\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "**Person 3**\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "**Person 4**\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "...we can create a *list* of all the Irish immigrants' names and hold them in a single place.\n",
      "names = ['Mary Gallagher', 'John Sanin(?)', 'Anthony Clark', 'Margaret Farrell']\n",
      "type(names)\n",
      "ages = [28, 19, 60, 30]\n",
      "type(ages)\n",
      "A list is always enclosed by square brackets `[]` and accepts items in a row separated by commas `,`. A list can contain any combination of Python data types.\n",
      "### Index\n",
      "You can index a list just like you would index a string. For example, if we wanted to pull out the first item in our `names` list, we could put square brackets and our desired index number immediately after the list. Just like with strings, the Python index begins with 0.\n",
      "names[0]\n",
      "names[3]\n",
      "### Slice\n",
      "You can also slice lists just like you would a string.\n",
      "names[:2]\n",
      "### List Methods\n",
      "Lists also have a number of special methods that can be used with them, such as a method for adding items to a list, which is an extremely common one.\n",
      "| **List Method** | **Explanation**                                                                                   |\n",
      "|-------------|---------------------------------------------------------------------------------------------------|\n",
      "| `list.append(another_item)`          | adds new item to end of list                                                                                |\n",
      "| `list.extend(another_list)`        | adds items from another_list to list                                                |\n",
      "| `list.remove(item)`        | removes first instance of item                                                       |\n",
      "| `list.reverse()`       | reverses order of list                                                                                 |                                                     |\n",
      "###### Add Items To List\n",
      "names.append(\"Lawrence Feeney\")\n",
      "names\n",
      "###### Extend List With Another List\n",
      "names.extend(ages)\n",
      "names\n",
      "## For Loops\n",
      "One of the best ways to work with a list is with something called \"`for` loops.\" This is a way of considering each item in the list, also known as \"iterating\" through the list.\n",
      "names = ['Mary Gallagher', 'John Sanin(?)', 'Anthony Clark', 'Margaret Farrell']\n",
      "for name in names:\n",
      "    print(name)\n",
      "A `for` loop contains the English word `for` followed by a variable name for each item in the list that you want to consider (it could be anything!) followed by the English word in` followed by a colon `:`\n",
      "for name in names:\n",
      "    print(f\"Person's name is {name}\")\n",
      "for x in names:\n",
      "    print(f\"Person's name is {x}\")\n",
      "ages = [28, 19, 60, 30]\n",
      "for age in ages:\n",
      "    if age > 30:\n",
      "        print(\"Person is less than 30 years old\")\n",
      "    else:\n",
      "        print(\"Person is more than 30 years old\")\n",
      "## Group Exercise / HW Assignment 3 (Part II)\n",
      "###### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "###### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "###### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "###### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Make a list that contains each of the above Irish immigrants' professions and assign to a variable called `professions`\n",
      "##Your Code Here\n",
      "**2.** Extract the second item in the list `professions`. Hint: remember how the Python index works!\n",
      "##Your Code Here\n",
      "**3.** Add the item \"spinster\" to your `professions` list, then print the list.\n",
      "##Your Code Here\n",
      "##Your Code Here\n",
      "**4.** Make a `for` loop that considers each item in the `professions` list and prints \"Person's profession is ___\"\n",
      "##Your Code Here\n",
      "    ##Your Code Here\n",
      "**5.** Make a list that contains each of the above Irish immigrants' child statuses and assign to a variable called `child_status`. You can make Margaret Farrell's child status an empty string `''`.\n",
      "##Your Code Here\n",
      "**6.** Extract the third item in the list.\n",
      "##Your Code Here\n",
      "**7.** Make a `for` loop that considers each item in the `child_status` list and prints \"Person has child\" if the person has a child and \"Person does not have child\" if not\n",
      "##Your Code Here\n",
      "  ##Your Code Here\n",
      "       ##Your Code Here\n",
      "    ##Your Code Here\n",
      "        ##Your Code Here\n",
      "**8.** Make a list that contains each of the above Irish immigrants' genders and assign to a variable called `gender`\n",
      "##Your Code Here\n",
      "**9.** Add an item to the list called \"not known\"\n",
      "##Your Code Here\n",
      "**10.** Make a `for` loop that considers each item in the `gender` list and prints \"Person is male\" if the person is male, \"Person is female\" if the person is female, and \"Person's gender is not known\" if unknown\n",
      " ##Your Code Here\n",
      "     ##Your Code Here\n",
      "         ##Your Code Here\n",
      "     ##Your Code Here\n",
      "         ##Your Code Here\n",
      "     ##Your Code Here\n",
      "         ##Your Code Here\n",
      "## String Methods\n",
      "Here are some special things you can do with \"strings\" in Python. Strings are one of four basic Python data types. They're typically words, and they're always surrounded by quotation marks.\n",
      "\"this is a string\"\n",
      "'this is a string'\n",
      "this is not a string\n",
      "| **String Method** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `string.lower()`         | makes the string lowercase                                                                                |\n",
      "| `string.upper()`         | makes the string uppercase  \n",
      "| `string.title()`         | makes the string titlecase \n",
      "| `string.strip()`         | removes lead and trailing white spaces     |\n",
      "| `string.replace('old string', 'new string')`      | replaces `old string` with `new string`          |\n",
      "| `s.split('delim')`          | returns a list of substrings separated by the given delimiter |\n",
      "| `s.join(list)`         | opposite of split(), joins the elements in the given list together using the string                                                                        |\n",
      "| `string.startswith('some string')`       | tests whether string begins with `some string` |                                                       |\n",
      "| `string.endswith('some string')`       |  tests whether string ends with `some string`   |\n",
      "| `string.isspace()`       |  tests whether string is a space |\n",
      "\n",
      "                                                            \n",
      "## Practice with Strings!\n",
      "We're going to practice with Franz Kafka's 1915 novella, *The Metamorphosis.*\n",
      "sample_text = open(\"../texts/literature/Kafka-The-Metamorphosis.txt\", encoding=\"utf-8\").read()\n",
      "print(sample_text)\n",
      "## Extract Parts of Strings\n",
      "### Index\n",
      "By using square brackets `[]`, you can \"index\"—or grab—part of a string based on its character number. The very first words of the novella are \"One morning.\" Here's what happens if we grab the first character.\n",
      "(Remember that the zero-th place in a Python index is actually the very first place.)\n",
      "sample_text[0]\n",
      "sample_text[1]\n",
      "### Slice\n",
      "By using a colon `:`, we can slice a string up to a certain character. Let's index our Kafka sample text up to the 121st character.\n",
      "sample_text[:121]\n",
      "Great! That's the first sentence. Let's assign it to the variable `first_line`.\n",
      "first_line = sample_text[:121]\n",
      "first_line\n",
      "## Replace Words\n",
      "first_line.replace(\"morning\", \"evening\")\n",
      "Your turn! Replace the word \"vermin\" with a word of your choosing:\n",
      "first_line.replace(\"vermin\", #your code here )\n",
      "Your turn! Replace the words \"Gregor Samsa\" with another name:\n",
      "# your code here\n",
      "## Transform Strings to Lowercase/Uppercase\n",
      "(\"I am really very quiet\").lower()\n",
      "(\"I am really very quiet\").upper()\n",
      "Your turn! Transform the first line of Kafka's *The Metamorphosis* to lower case:\n",
      "# your code here\n",
      "Your turn! Transform the first line of Kafka's *The Metamorphosis* to upper case:\n",
      "# your code here\n",
      "## Split Strings By a Delimiter\n",
      "With the `.split()` method, you can split up a strings into a a list of parts. By default, it splits on spaces, but you can put in a different delimiter and split on something else. \n",
      "first_line.split()\n",
      "Your turn! Split on the words \"dreams\" and see what happens.\n",
      "# your code here\n",
      "## Join Strings By a Delimiter\n",
      "You can also put something back together again with the `join()` method!\n",
      "kafka_split_words = first_line.split()\n",
      "kafka_split_words\n",
      "\"SPACE\".join(kafka_split_words)\n",
      "\" \".join(kafka_split_words)\n",
      "Your turn! Join `kafka_split_words` with a delimiter of your choosing.\n",
      "# your code here\n",
      "## Installation\n",
      "## Installing Anaconda\n",
      "We're going to install Python by installing something called Anaconda. Anaconda is a Python distribution that includes many other convenient packages, such as Jupyter Notebooks and JupyterLab. It also helps to keep your Python environment clean and well-managed.\n",
      "<img src=\"https://docs.anaconda.com/_images/win-install-options.png\" width=100% , border=2>\n",
      "\n",
      "<img src=\"../images/installation/navigator-app.png\" width=100% , border=2>\n",
      "<img src=\"https://docs.anaconda.com/_images/nav-defaults.png\" widht=100%, border=2>\n",
      "Then launch Jupyter Lab.\n",
      "\n",
      "If Anaconda is added to your PATH, you should be able to launch Jupyter Lab from the command line like so:\n",
      "%jupyter lab\n",
      "## Installing Atom\n",
      "Though we're mostly going to be programming in Jupyter notebooks, we're also going to install a text editor called Atom, where you can also write Python code.\n",
      "<img src=\"../images/Python-Atom.png\" width=100%, border=2>\n",
      "* Go to [Atom's home page](https://atom.io/), click download (it should know which OS you're using), and follow the instructions.\n",
      "\n",
      "* If you want to be able to launch Atom from the command line, as well, you can click \"Atom\" in the menu bar and then click \"Install Shell Commands.\"\n",
      "<img src=\"../images/installation/atom-shell-commands.png\" width=100%, border=2>\n",
      "## What We're Not Covering\n",
      "The Python lessons in this course are intended to cover the most practical and most basic elements of Python that you will need to know to collect and analyze cultural data. There are a range of subjects that we are not covering in this course, which you may want to learn about, including but not limited to:\n",
      "* [Full Explanation of Object Oriented Programming](https://realpython.com/python3-object-oriented-programming/)\n",
      "* [Python Classes](https://docs.python.org/3/tutorial/classes.html)\n",
      "* [Handling Python Exception Errors](https://docs.python.org/3/tutorial/errors.html#handling-exceptions)\n",
      "* [While Loops](https://docs.python.org/3/tutorial/errors.html#handling-exceptions)\n",
      "* [Sets](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset)\n",
      "## Data Types\n",
      "[Download necessary Jupyter notebooks and text files here](https://melaniewalsh.org/Intro-CA-Notebooks-V2.zip)\n",
      "* 4 essential kinds of Python data with different powers and capabilities, like starter pack Pokémon    \n",
      "    - Strings (Words)\n",
      "    - Integers (Whole Numbers)\n",
      "    - Floats (Decimal Numbers)\n",
      "    - Booleans (True/False)\n",
      "<img src=\"https://hips.hearstapps.com/digitalspyuk.cdnds.net/16/08/1456483171-pokemon2.jpg?resize=768:*\", border=2>\n",
      "**HEADS UP!**\n",
      "🚨 To run any of the code on this page, you need to run this cell first!!🚨\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "most_frequent_meaningful_words\n",
      "You might be wondering why we put quotation marks around `\"../texts/Beyonce-Lemonade.txt\"` and not around `40`,  or why the file path shows up in red and 40 shows up in green.\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "number_of_desired_words = 40\n",
      "That's because these are two different \"types\" of Python data. The file path is what's called a \"string,\" or words, and 40 is an \"integer,\" or a whole number.\n",
      "\n",
      "In Python, there are four basic data types:\n",
      "\n",
      "- Strings (Words)\n",
      "- Integers (Whole Numbers)\n",
      "- Floats (Decimal Numbers)\n",
      "- Booleans (True/False)\n",
      "\n",
      "Each data type has different properties and different capabilities. You can check a data type if you use the function `type()`. As you may have noticed, functions use parentheses, and they do something to the thing inside the parentheses, what we call an \"argument.\"\n",
      "\n",
      "type(filepath_of_text)\n",
      "type(number_of_desired_words)\n",
      "Let's look at what happens if we change the data types of `filepath_of_text` and `number_of_desired_words`\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = ../texts/music/Beyonce-Lemonade.txt\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = ../texts/music/Beyonce-Lemonade.txt\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = \"40\"\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Strings (Words)\n",
      "- Enclosed by either single or double quotation marks (doesn't matter which but you have to be consistent)\n",
      "- Ability to combine strings with `+`\n",
      "- Ability to manipulate in special ways (make lowercase or uppercase, replace parts, grab slices, etc.)\n",
      "full_text\n",
      "**Heads up!**\n",
      "The `\\n` above means \"new line.\" \n",
      "type(full_text)\n",
      "## Extract Parts of Strings\n",
      "### Index\n",
      "By using square brackets `[]`, you can \"index\"—or grab—part of a string based on its character number. The first line of Beyonce's album Lemonade is:\n",
      "> Six inch heels, she walked in the club like nobody's business.\n",
      "\n",
      "If we index the very first character of the album, which character do you think we'll get?\n",
      "full_text[1]\n",
      "full_text[0]\n",
      "This is one weird thing about Python that you're just going to have to commit to memory. The zero-th place in a Python index is actually the very first place. Its number system starts with zero.\n",
      "### Slice\n",
      "By using a colon `:`, we can index a string up to a certain character.\n",
      "full_text[:61]\n",
      "lemonade_first_line = full_text[:61]\n",
      "## Add Strings Together\n",
      "lemonade_first_line + \", and then she took off her shoes because they were pretty tall\"\n",
      "## Lots of Handy String Methods!\n",
      "For more, see [String Methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html)\n",
      "lemonade_first_line.upper()\n",
      "A method is like a function, except that it follows the thing it's going to act upon, and it doesn't necessarily need an \"argument\" in the parentheses.\n",
      "## Put Variables Inside Strings\n",
      "You can also insert variables into a string with something called **f-strings**! They're amazing. An f-string must begin with an `f` outside the quotation marks, and then the variable must be be placed within curly brackets `{}`, like so:\n",
      "print(f\"Beyonce stepped on stage and sang: \\n\\n'{lemonade_first_line}'\")\n",
      "### Your Turn\n",
      "Remix! Make a variable called `new_first_line` and assign it the value of `lemonade_first_line` plus `+` your new remixed ending `new_first_line`. Then print it.\n",
      "new_first_line = #Your Code Here\n",
      "print(f\"Beyonce stepped on stage and sang: \\n\\n'{#Your Code Here}'\")\n",
      "## Integers & Floats (Numbers)\n",
      "- You can do math with them!\n",
      "number_of_desired_words = 40\n",
      "type(number_of_desired_words)\n",
      "number_of_desired_words + 57\n",
      "new_number = 100005\n",
      "number_of_desired_words * new_number\n",
      "number_of_desired_words= 40.5\n",
      "type(number_of_desired_words)\n",
      "type(40.555555555555)\n",
      "## Multiplication\n",
      "4 * 2\n",
      "## Exponents\n",
      "4 ** 2\n",
      "## Order of Operations\n",
      "4 + 2 * 2\n",
      "(4 + 2) * 2\n",
      "## Booleans (True/False)\n",
      "Booleans are like little judgments. They report on whether things in your Python universe are `True` or `False`.\n",
      "beyonce = \"Grammy award-winner\"\n",
      "beyonce == \"Grammy award-winner\"\n",
      "beyonce == \"Oscar award-winner\"\n",
      "beyonce == \"Grammy award winner\"\n",
      "type(beyonce == \"Oscar award-winner\")\n",
      "## Type Conversion\n",
      "number_of_desired_words = 40\n",
      "number_of_desired_words\n",
      "type(number_of_desired_words)\n",
      "str(number_of_desired_words)\n",
      "converted_num_desired_words = str(number_of_desired_words)\n",
      "type(converted_num_desired_words)\n",
      "int(lemonade_first_line)\n",
      "## In-Class Exercise\n",
      "name = 'Prof. Walsh' #string\n",
      "age = 1000 #integer\n",
      "place = 'Chicago' #string \n",
      "favorite_food = 'tacos' #string\n",
      "dog_years_age = age * 7.5 #float\n",
      "student = False #boolean\n",
      "print(f'✨This is...{name}!✨')\n",
      "\n",
      "print(f'{name} likes {favorite_food} and once lived in {place}. {name} is {age} years old, which is {dog_years_age} in dog years. The statement \"{name} is a student\" is {student}.')\n",
      "## Your turn!\n",
      "\n",
      "name = #Your code here\n",
      "age = #Your code here\n",
      "home_town = #Your code here\n",
      "favorite_food = #Your code here\n",
      "dog_years_age =#Your code here * 7.5\n",
      "student = False #boolean\n",
      "print(f'✨This is...{name}!✨')\n",
      "\n",
      "print(f'{name} likes {favorite_food} and once lived in {place}. {name} is {age} years old, which is {dog_years_age} in dog years. The statement \"{name} is a student\" is {student}.')\n",
      "Now add a new variable called `favorite_movie` and update the f-string to include a new sentence about the person's favorite movie.\n",
      "name = \n",
      "age = \n",
      "home_town = \n",
      "favorite_food = \n",
      "dog_years_age =\n",
      "#favorite_movie = \n",
      "print(f'✨This is...{name}!✨')\n",
      "\n",
      "print(f'{name} likes {favorite_food} and once lived in {place}. {name} is {age} years old, which is {dog_years_age} in dog years. The statement \"{name} is a student\" is {student}. # YOUR NEW SENTENCE HERE')\n",
      "## Programming in Python\n",
      "In this series of lessons, we will learn the basics of a programming language called Python, which we will continue to use throughout the semester.\n",
      "## Why Learn a Programming Language At All?\n",
      "* Save time and energy by automating tasks\n",
      "* Collect and analyze cultural data from a different perspective, often more comprehensively and at a larger scale\n",
      "* Gain more flexibility, customizability, and autonomy over your digital research projects\n",
      "* Better understand how data and programming languages shape contemporary society and culture \n",
      "## Why Python Specifically?\n",
      "* Readable (resembles English)\n",
      "* Relatively smooth learning curve\n",
      "* Broadly used and popular\n",
      "* Good for data science\n",
      "\n",
      "\n",
      "## Learning Goals with Python\n",
      "* Learn how to use Python as a tool and apply it to cultural questions\n",
      "* Learn how to read and think critically about Python code\n",
      "* Learn how to tinker with existing Python code for our own purposes\n",
      "* Learn how to better collaborate with computer scientists and software developers\n",
      "We are not, however, trying to:\n",
      "   ❌   Learn every technical nook and cranny of Python<br>\n",
      "   ❌   Learn how to become a software developer by the end of the course\n",
      "## Python Version 3\n",
      " The latest version of Python, which you should be using, is Python 3. [Python 2 has officially been retired](https://www.python.org/doc/sunset-python-2/). You should not use Python 2.\n",
      " \n",
      " It's possible that you may come across old Python 2 code on the internet, which won't work with Python 3. A telltale difference between Python 3 and Python 2 code is the syntax for the `print()` function. \n",
      "### Python 3\n",
      "In Python 3, the `print()` function requires parentheses...\n",
      "print(\"Good riddance, Python 2!\")\n",
      "### Python 2\n",
      "while Python 2 code does not...\n",
      "print \"Good riddance, Python 2!\"\"\n",
      "## Files and Character Encoding\n",
      "## Open a Text File\n",
      "The most basic way to open a file is to use Python's built-in `open()` function and to insert your desired file path surrounded by quotation marks. That creates a file object. Then we tack on the `.read()` method to transform the file object into one big string. But, as we will see below, it's good practice to add an extra step here in order to accommodate character encoding issues.\n",
      "open('sample-character-encoding.txt').read()\n",
      "## Character Encoding\n",
      "> Written text is a sequence of graphemes – characters. Every character you type – whether letters like ‘a’ and ‘b’, punctuation marks like ‘?’, or even emoji like 💪👬👍 – has an ID number that computers use to store it. In order to communicate, computers need to agree on a common roster of how to assign these graphemes to numbers and vice versa. These rosters are known as character encodings.\n",
      "\n",
      "> -Aditya Mukerjee, \"[I Can Text You A Pile of Poo, But I Can’t Write My Name](https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name)\"\n",
      "Python uses the UTF-8 character encoding, or Unicode, by default. Unicode is the most popular character encoding on the internet and even includes emojis.\n",
      "\n",
      "However, as Mukerjee points out in his essay, Unicode still does not include characters that are essential to the Bengali alphabet as well as to many other non-English languages.\n",
      "sample_text_default = open('sample-character-encoding.txt').read()\n",
      "print(sample_text_default)\n",
      "## Computers Don't Know What Text Is\n",
      "That's why they need character encoding, which is a system that maps character to numbers. You can check the Unicode \"code point,\" or place in the Unicode universe, for any character with the function `ord()`\n",
      "help(ord)\n",
      "ord?\n",
      "ord(\"a\")\n",
      "ord(\"💩\")\n",
      "ord(\"ত\")\n",
      "ord(\"!\")\n",
      "## Adding (UTF-8) Encoding for Text Files\n",
      "Though Python reads UTF-8 by default, it's good practice to explicitly declare UTF-8 encoding, as below.\n",
      "sample_text_default = open('sample-character-encoding.txt', encoding='utf-8').read()\n",
      "print(sample_text_default)\n",
      "Look what happens if we read in the exact same text with a different encoding.\n",
      "sample_text_iso = open('sample-character-encoding.txt', encoding='iso-8859-1').read()\n",
      "print(sample_text_iso)\n",
      "sample_text_ascii = open('sample-character-encoding.txt', encoding='ascii').read()\n",
      "print(sample_text_ascii)\n",
      "## Debugging Tip\n",
      "> If you open a document and it looks like this, there's one and only one reason for it: Your text editor, browser, word processor or whatever else that's trying to read the document is assuming the wrong encoding. That's all. The document is not broken (well, unless it is, see below), there's no magic you need to perform, you simply need to select the right encoding to display the document.\n",
      "\n",
      "> -[What Every Programmer Absolutely, Positively Needs to Know About Encodings and Character Sets to Work With Text](http://kunststube.net/encoding/)\n",
      "sample_text_iso = open('sample-character-encoding.txt', encoding='iso-8859-1').read()\n",
      "print(sample_text_iso)\n",
      "## Common Python Errors\n",
      "Below are a few common error messages that you will likely encounter as you first learn Python and — to be perfectly honest — long afterward. No matter how much you know, you will always encounter errors!\n",
      "\n",
      "To learn more about these and other Python errors, see [Python's official documentation](https://docs.python.org/3/library/exceptions.html#bltin-exceptions).\n",
      "## SyntaxError\n",
      "A `SyntaxError` means that something has gone wrong with your Python syntax, aka the arrangement of words and punctuation in your code. Often, as below, this error will result from forgetting a closing quotation mark in a string or from forgetting a colon in a `for` loop. \n",
      "print(\"Hope this goes off without a hitch!)\n",
      "for item in items\n",
      "    print(item)\n",
      "The error message will often include a caret or arrow that points to the problematic part of the code:\n",
      "<img src=\"../images/Errors/SyntaxError.png\", border=2>\n",
      "## FileNotFound Error\n",
      "A `FileNotFoundError` means that whatever file name you've typed in cannot be located. Often, this error will result from simple typos in the file name or from not pointing to the correct directory which contains the file. Double check your spelling and where your desired file is located relative to your Python code.\n",
      "open('../Wrong-Directory/File-Name.txt').read()\n",
      "## TypeError\n",
      "A `TypeError` means that you're trying to perform a function or operation on something that is not the correct data type for that function or operation. For example, if you try to divide by a variable that is a string, rather than an integer or float (as in the example below), a `TypeError` will be thrown. Double check your data types with the `type()` function.\n",
      "favorite_artist = \"Beyoncé\"\n",
      "favorite_artist / 2\n",
      "type(favorite_artist)\n",
      "## NameError\n",
      "A `NameError` means that the variable name that you're using cannot be found. Often, this error results from forgetting to run the cell that defines your variable or from misspelling the name of your variable, as below. Check your spelling and make sure you've run all necessary cells.\n",
      "favorite_artist = \"Beyoncé\"\n",
      "favorite_arteest\n",
      "## AttributeError\n",
      "An `AttributeError` means that you're trying to access something from an object that that object doesn't possess or do something with an object that that object cannot do. For example, to transform the variable `favorite_artist` (which contains the string \"Beyoncé\") from title case to uppercase, we can run `favorite_arist.upper()` because `.upper()` is a built-in string method. But if we forget the name of that string method and instead type `.uppercase()`, which is not an existing string method, then an `AttributeError` will be raised.\n",
      "favorite_artist.upper()\n",
      "favorite_artist.uppercase()\n",
      "## More Lists & Loops, Plus Modules\n",
      "[Download relevant files here](https://melaniewalsh.org/More-Lists-Loops.zip)\n",
      "Last lesson, we learned how to make, manipulate, and iterate through lists, an important Python collection type. But we weren't actually working with a real CSV file, and we weren't doing a very comprehensive analysis of the data. In this lesson, we're going to work with a real CSV file and try to answer some analytical questions about the Bellevue Almshouse data, such as:\n",
      "\n",
      "- What is the most common \"disease\" and the least common \"disease\"?\n",
      "- What is the most common \"profession\" and the least common \"profession\"?\n",
      "- What is the gender breakdown of those admitted to the Bellevue Almshouse?\n",
      "\n",
      "We're going to answer these questions by practicing more with lists and loops while also introducing the csv module and the collections module.\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "## Reading in a CSV File\n",
      "The [csv module](https://docs.python.org/3/library/csv.html) allows you to read and write tabular data in CSV (comma separated values) format, one of the most common formats for spreadsheets. (Soon we're going to talk about the [Python library Pandas](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Pandas.html), an even more powerful and more convenient way of working with tabular data.)\n",
      "import csv\n",
      "To use the csv module, you have to first import it, as above. Then to read in a CSV file, as below, you need to `with open()` your desired CSV file `as` a csv object `:` then use the `csv.reader()` function and insert your csv object. The \"delimiter\" argument tells the computer how to read the CSV file. Sometimes you might have a CSV file that is separated by tabs (\\t) instead of commas (,) so it's typically good to specify.\n",
      "almshouse_filepath = '../data/bellevue_almshouse_modified.csv'\n",
      "\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "almshouse_data\n",
      "The `csv.reader()` function will create a \"reader object.\" To actually get at the data in there, we'll need to iterate through it in some way. Each row in the reader object is a list of strings, so if we iterate through every row in the dataset, we will get 9,000+ lists (!). It's helpful to remember what each row in the dataset represents and name your variables accordingly. For the Bellevue Almshouse dataset, each row represents a person.\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        print(person)\n",
      "**Pro tip!** If you have a really long output, you can [\"Enable Scrolling for Outputs\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics/images/enable-scrolling.png) by right-clicking and selecting that option.\n",
      "If we wanted to answer our first question — *What is the most common \"disease\" and the least common \"disease\"?* — how might we isolate only the names of the diseases so we can count them? Think back to how [we indexed a list](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html#Index) in the last lesson...\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        print(person[0])\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        print(person[4])\n",
      "### Build a List With a `For` Loop\n",
      "Great! We figured out how to isolate the diseases. But to count them, we want to get them in a data collection of their own, like a list. How would we put this data into a list? Let's make an empty list and then append each disease from each row into the list.\n",
      "❌ ❌ ❌ **Not Correct**\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        diseases = []\n",
      "        diseases.append(person[4])\n",
      "diseases\n",
      "Wait, that's not quite right. We only got a list with a single value. What's going on?\n",
      "The problem is that the list building is happening *inside* the `for` loop. This means that, `for` every person/row, the list is being re-written over and over again. \"destitution\" is the very last disease in the dataset, so we're only getting the very last value. To keep building on a list, we need to put the empty list *outside* of the `for` loop and then keep adding to it.\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    diseases = []\n",
      "    for person in almshouse_data:   \n",
      "        diseases.append(person[4])\n",
      "diseases\n",
      "## Measure Length of List\n",
      "To measure the length of a list, use the `len()` function.\n",
      "len(diseases)\n",
      "## Count Items In a List or Collection\n",
      "The Counter tool from the collections module is extremely useful. It can help you count all kinds of things. To use it, you first need to `import` the Counter `from` collections.\n",
      "from collections import Counter\n",
      "To count something, you simply need to insert it inside the `Counter()` function, like so:\n",
      "Counter(diseases)\n",
      "This will give you what's called a dictionary, which includes every disease in the dataset and how many times it appears. To sort this Counter dictionary based on the most common items, you can use the `.most_common()` method.\n",
      "disease_tally = Counter(diseases)\n",
      "disease_tally.most_common()\n",
      "You can also select a certain number of the most common items by placing a number inside the `.most_common()` method.\n",
      "disease_tally.most_common(10)\n",
      "You can also select a certain number of the *least* common items by extracting a slice from the end of list, like so:\n",
      "disease_tally.most_common()[-10:]\n",
      "disease_tally.most_common()[-3:]\n",
      "## Your Turn!\n",
      "By using the same methods, find the 10 most common professions and the 10 least common professions in the Bellevue Almshouse dataset.\n",
      "Build a list called `professions` by using a `for` loop and the `.append()` method.\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    #Your Code Here\n",
      "    for person in almshouse_data:   \n",
      "        #Your Code Here\n",
      "professions\n",
      "Count the list `professions` with the Counter tool then display the top 10 most common values.\n",
      "from collections import Counter\n",
      "\n",
      "professions_tally = #Your Code Here\n",
      "#Your Code Here\n",
      "Display the 10 least common values.\n",
      "#Your Code Here\n",
      "Now find out how many men vs women are included in the Bellevue Almshouse data. Build a list called `gender` with a `for` loop and the `.append()` method\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    #Your Code Here\n",
      "    for person in almshouse_data:   \n",
      "        #Your Code Here\n",
      "gender\n",
      "Count the values in the gender column wiht the Counter tool and then display the results.\n",
      "from collections import Counter\n",
      "\n",
      "gender_tally = #Your Code Here\n",
      "gender_tally\n",
      "## List Comprehensions\n",
      "There's a slightly easier and more compact way to build a list with a `for` loop called a \"list comprehension.\" Instead of creating an empty list, you can build the `for` loop inside of a list.\n",
      "<img src='../images/lists/list-comprehensions' width=100%, border=2>\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    diseases = [person[4] for person in almshouse_data]\n",
      "diseases\n",
      "Remember our Python script for counting words in a text file? Though you probably didn't recognize it at the time, this code contains a list comprehension. Can you spot it?\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "This is the list comprehension:\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "which is exactly the same as\n",
      "meaningful_words = []\n",
      "for word in all_the_words:\n",
      "    if word not in nltk_stop_words:\n",
      "        meaningful_words.append(word)\n",
      "\n",
      "empty_string = []\n",
      "    for item in collection:\n",
      "        if item in items_we_want:\n",
      "            empty_string.append(item)\n",
      "empty_string = [item for item in collection if item in items_we_want]\n",
      "## Functions\n",
      "[Download this \"Functions\" notebook and other relevant files here](https://melaniewalsh.org/Functions-More-Pandas.zip)\n",
      "<img src=\"https://www.tintoyarcade.com/image/cache/data/product/Images_5100_5199/TTA5100-Penguin-Windup-01-1000x1000.jpg\" width=100%, border=2>\n",
      "A \"function\" is way of bundling up code to perform specific tasks. It's kind of like making a little Python wind-up toy that runs on command.\n",
      "\n",
      "Functions are useful because they can help make our code more organized, and because they can save us from repetition. If you have to do some task over and over again, you don't want to write out the same code over and over again from scratch. It's easier to just make a function that contains that code and simply \"call\" it whenever you need it.\n",
      "\n",
      "We've encountered functions many times already. For example, `print()`, `len()`, and `type()` are all built-in Python functions. They contain bundled up code that performs specific tasks whenever we call them.\n",
      "## Define a Function\n",
      "To make your own function, you use the keyword `def`—short for \"define\"—followed by your desired name for the function, parentheses `()` and a colon `:`.\n",
      "\n",
      "Then, on the following lines, you indent one tab over and write some code that you want your function to perform. In our case, we're using multiple `print()` statements to print some lyrics from Beyonce's \"Formation.\"\n",
      "\n",
      "Finally, you complete the function with a `return` statement. Sometimes you will want to `return` a specific value but here we're not returning anything.\n",
      "def sing_beyonce_lyrics():\n",
      "    print(\"Okay, okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "    print(\"Okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "    print(\"Prove to me you got some coordination, 'cause I slay\")\n",
      "    print(\"Slay trick, or you get eliminated\")\n",
      "    return \n",
      "Behold the importance of the indent:\n",
      "def sing_beyonce_lyrics():\n",
      "print(\"Okay, okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "print(\"Okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "print(\"Prove to me you got some coordination, 'cause I slay\")\n",
      "print(\"Slay trick, or you get eliminated\")\n",
      "    return \n",
      "## Call a Function\n",
      "To use or \"call\" a function, you simply type the name of the function with parentheses.\n",
      "sing_beyonce_lyrics()\n",
      "def sing_happy_birthday():\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday dear human life form\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    return \n",
      "sing_happy_birthday()\n",
      "## Add Parameters/Arguments\n",
      "You can add \"parameters\" to your functions—or values that are required by your function—by putting parameter names inside the parentheses. For example, if we want to personalize our birthday song function to include a specific person's name, we can add the parameter `personalized_name` inside the parentheses, which will require a personalized name to be passed to the function. The thing you pass to the function is called an \"argument.\" \n",
      "\n",
      "- parameter = `personalized_name` (thing that requires a value for the function) \n",
      "- argument = \"Beyonce\" (actual value passed to function)\n",
      "\n",
      "Since parameters and arguments are so interrelated, they're sometimes confused for each other. You can read [Python's official distinction here](https://docs.python.org/3.3/faq/programming.html#faq-argument-vs-parameter).\n",
      "def sing_personalized_happy_birthday(personalized_name):\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(f\"Happy Birthday dear {personalized_name}\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    return \n",
      "^^^ Later on in the code, we'll use whatever name gets passed to the function inside an f-string: `f\"Happy Birthday dear {personalized_name}\"`\n",
      "Once you set a parameter that requires an argument, you have to pass something inside the function for the function to run. So if we run `sing_personalized_happy_birthday()` as we did with `sing_happy_birthday()`, it won't work.\n",
      "sing_personalized_happy_birthday()\n",
      "This error is telling us that we have to pass in a value or \"argument.\"\n",
      "sing_personalized_happy_birthday(\"Beyonce\")\n",
      "sing_personalized_happy_birthday(\"Carly Rae Jepsen\")\n",
      "sing_personalized_happy_birthday(#Insert Your Name Here)\n",
      "## Keyword Arguments\n",
      "There's another way that you can require arguments in a function, which is with \"keyword arguments.\" Before we were using \"positional arguments,\" where the function automatically knew that \"Beyonce\" was the `personalized_name` argument simply because \"Beyonce\" was in the right position. (There was only one argument required, so, duh.)\n",
      "\n",
      "But you can also explicitly define your arguments with keyword arguments that use an `=` sign, which can become more useful if you have multiple parameters. This can also be a way of setting default values in your functions.\n",
      "def sing_keyword_happy_birthday(to_name='Beyonce', from_name='Info 1350'):\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(f\"Happy Birthday dear {to_name}\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(f\"\\nSincerely, \\n{from_name}\")\n",
      "    return \n",
      "For example, if we don't pass in any arguments into this function, it will use the default arguments.\n",
      "sing_keyword_happy_birthday()\n",
      "But if we set the keyword arguments to different values—even if we switch the order or position of the arguments!—the function will know which arguments they're supposed to be.\n",
      "sing_keyword_happy_birthday(from_name=\"Big Bird\", to_name=\"Cookie Monster\")\n",
      "These keyword arguments help explain some of the errors we've been getting when we use Pandas, such as with the `.sort_values()` method and the `by=` parameter.\n",
      "sing_keyword_happy_birthday(from_name=\"Big Bird\", to_name=\"Cookie Monster\", by=\"Column_name\")\n",
      "## Return Values\n",
      "In all of the examples above, we weren't returning any specific value, just using `print()` statements. But sometimes you want a specific value out of your function. For example, if we want to make a function that transforms a bit of text into very loud-sounding text, then we'll want to `return` that loud-sounding text.\n",
      "def make_text_shouty(text):\n",
      "    shouty_text = text.upper()\n",
      "    return shouty_text\n",
      "make_text_shouty(\"I like tacos\")\n",
      "def make_text_shoutier(text):\n",
      "    shouty_text = text.upper()\n",
      "    shoutier_text = shouty_text + '!!!'\n",
      "    return shoutier_text\n",
      "make_text_shoutier(\"I like tacos\")\n",
      "def calculate_dog_years_age(age):\n",
      "    dog_years_age = age * 7\n",
      "    return dog_years_age\n",
      "calculate_dog_years_age(52)\n",
      "## Your Turn\n",
      "Make a function called `make_text_whispery` that transforms text to lower case\n",
      "#Your Code Here\n",
      "    whispery_text = #Your Code HEre\n",
      "    return #Your Code Here\n",
      "Now insert the string \"I AM WHISPERING\" into `make_text_whispery`\n",
      "#Your Code Here\n",
      "## Variables\n",
      "[Download relevant files for today's class here](https://melaniewalsh.org/Intro-CA-Notebooks-V2.zip)\n",
      "* Storage containers for data values, like little data gift boxes\n",
      "![](https://cdn.pixabay.com/photo/2016/09/14/20/48/birthday-1670415_960_720.png)\n",
      "**HEADS UP!**\n",
      "🚨 To run any of the code on this page, you need to run this cell first!!🚨\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "## Example Word Count Python Code\n",
      "\"\"\"\n",
      "Example Python code for\n",
      "calculating word frequency\n",
      "in a text file\n",
      "\"\"\"\n",
      "\n",
      "#Import Libraries and Modules\n",
      "\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# Define Functions\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "# Define Filepaths and Assign Variables\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "# Read in File\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "# Manipulate and Analyze File\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "# Output Results\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Assigning Variables\n",
      "Variables are one of the fundamental building blocks of Python. A variable is like a tiny container where you store values and data — filenames, words, numbers, collections of words and numbers, etc. You can name variables almost anything you want (more on that below). The variable name will point to a value that you \"assign\" it. You might think about variable assignment like putting a value \"into\" the variable, as if the variable is a little box 🎁\n",
      "\n",
      "You assign variables with an equals `=` sign, which is slightly confusing. In Python, a single equals sign `=` is the \"assignment operator,\" and a double equals sign `==` is the \"real\" equals sign, e.g. `2 * 2 == 4`.\n",
      "Let's look at some of the variables that we used when we counted the most frequent words in Charlotte Perkins Gilman's \"The Yellow Wallpaper.\"\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "We made the variables:\n",
      "- `filepath_of_text` and assigned it `=` the location of our \"The Yellow Wallpaper\" text file (\"../texts/The-Yellow-Wallpaper.txt\")\n",
      "- `nltk_stop_words` and assigned it `=` the stopwords from the `nltk` library\n",
      "- `number_of_desired_words` and assigned it `=` `40` because we wanted the 40 most frequently occuring words\n",
      "- `full_text` and assigned it `=` the contents of \"The Yellow Wallpaper\" text file\n",
      "## `Print()` Vs Jupyter Display\n",
      "We can check to see what's \"inside\" these variables by running a cell with the variable's name. This is one of the handiest features of a Jupyter notebook. Outside the Jupyter environment, you would need to run `print(filepath_of_text)` to display the variable.\n",
      "filepath_of_text\n",
      "nltk_stop_words\n",
      "number_of_desired_words\n",
      "full_text\n",
      "Your turn! Pick another variable from the script above and see what's inside it below.\n",
      "#your_chosen_variable\n",
      "You can run the `print` function inside the Jupyter environment, too, which is sometimes useful because:\n",
      "\n",
      "- Jupyter will only display the last variable in a cell, but `print()` can display multiple variables\n",
      "- Jupyter will display text with `\\n` characters (which means \"new line\") but `print()` will display the text appropriately formatted with new lines\n",
      "\n",
      "filepath_of_text\n",
      "nltk_stop_words\n",
      "number_of_desired_words\n",
      "full_text\n",
      "Only the last variable in the cell above, `full_text`, is displayed with `\\n` characters. But if you `print()` each variable...\n",
      "print(filepath_of_text)\n",
      "print(nltk_stop_words)\n",
      "print(number_of_desired_words)\n",
      "print(full_text)\n",
      "...then each of the variables are displayed, plus the \"The Yellow Wallpaper\" is properly formatted with new lines.\n",
      " \n",
      "## Variable Names\n",
      "Though we named our variables `filepath_of_text`, `nltk_stop_words`,`number_of_desired_words`, and `full_text`, we could have named them almost anything else.\n",
      "\n",
      "Variable names can be as long or as short as you want, and they can include:\n",
      "- upper and lower-case letters (A-Z)\n",
      "- digits (0-9)\n",
      "- underscores (_)\n",
      "\n",
      "Variable names *cannot* include:\n",
      "- ❌ other punctuation (-.!?@)\n",
      "- ❌ spaces ( )\n",
      "- ❌ a reserved Python word\n",
      "Instead of `filepath_of_text`, we could have simply named the variable `filepath`.\n",
      "filepath = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "filepath = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "filepath\n",
      "Or we could have gone even simpler and named the filepath `f`.\n",
      "f = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "f\n",
      "### Striving for Good Variable Names\n",
      "As you start to code, you will almost certainly be tempted to use extremely short variables names like `f`.\n",
      "\n",
      "Your fingers will get tired, your coffee will wear off, you will see other people using variables like `f`, and you'll promise yourself that you'll definitely remember what `f` means. But you probably won't.\n",
      "\n",
      "Thus, you must resist the temptation of bad variable names. Clear and precisely-named variables will:\n",
      "\n",
      "1. Make your code more readable (both to yourself and others)\n",
      "2. Reinforce your understanding of Python and what's happening in the code\n",
      "3. Clarify and strengthen your thinking\n",
      "\n",
      "### Example Python Code ❌ **With Bad Variable Names** ❌\n",
      "For the sake of illustration, here's our same word count Python code with poorly named variables. The code works exactly the same as our original code and outputs the 40 most frequently occurring words in \"The Yellow Wallpaper\" — but it's *so much harder to read*!\n",
      "\n",
      "Imagine if you stumbled across this code for the first time and were trying to figure out how it works. Or imagine that you wrote this code two summers ago and were returning to it to do some updates. You'd have to spend a lot more time deciphering and decoding.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def sp(t):\n",
      "    lt = t.lower()\n",
      "    sw = re.split(\"\\W+\", lt)\n",
      "    return sw\n",
      "\n",
      "f = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "st = stopwords.words(\"english\")\n",
      "\n",
      "with open(f, encoding=\"utf-8\") as fo:\n",
      "    ft = fo.read()\n",
      "\n",
      "words = sp(ft)\n",
      "words = [w for w in words if w not in st]\n",
      "words = Counter(words)\n",
      "words = words.most_common(40)\n",
      "\n",
      "print(words)\n",
      "### Example Python Code ✨ **With Good Variable Names** ✨\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "### Off-Limit Names\n",
      "The only variable names that are off-limits are names that are reserved by, or built into, the Python programming language itself, such as `print`, `True`, and `list`. It's not something to worry too much about. You'll know very quickly if a name is reserved by Python because it will show up in green and often give you an error message.\n",
      "True = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "filepath-of-text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "## Re-Assigning Variables\n",
      "Variable assignment does not set a variable in stone. You can later re-assign the same variable a different value.\n",
      "For instance, I could re-assign `filepath_of_text` to the filepath for the lyrics of Beyonce's album *Lemonade* instead of Perkins-Gilman's \"The Yellow Wallpaper.\"\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "filepath_of_text\n",
      "If I change this one variable in our example code, then we get the most frequent words for *Lemonade*.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "### Your Turn\n",
      "Ok now it's your turn to insert a new file path and calculate a new word frequency! Take a look inside our `/texts` directory and see which texts you can choose from.\n",
      "(Do you remember how to look inside a directory and see what's there? Go back to [the command line lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Command-Line/The-Command-Line.htm) if you need a refresher. Hint: To look at the contents of a directory *inside* a directory, you can use the `-R` flag, short for \"recursive.\")\n",
      "!ls # your code here\n",
      "!ls -R #your code here\n",
      "Pick a file from the list above and assign `filepath_of_text` to its corresponding filepath below:\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = #Insert a New Text File Here\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "Now let's change the `number_of_desired_words` variable! Rename the variable and then chooose a value other than 40 and see what happens.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = #Insert a New Text File Here\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "#your_new_variable_name = #number\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(#your_new_variable_name)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "Bonus: how might you put the stopwords back in?\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = #Insert a New Text File Here\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "#your_new_variable_name = #number\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(#your_new_variable_name)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Dictionaries\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data.head(20)\n",
      "## Dictionary\n",
      "When we used lists with the Bellevue Almshouse data, it was a lot more convenient than individually assigning individual variables. We could put multiples names, ages, or professions into a single list.\n",
      "**Indivudal Variables**\n",
      "person1_name = 'Mary Gallagher'\n",
      "person2_name = 'John Sanin (?)'\n",
      "person1_age = 18\n",
      "person2_age = 19\n",
      "vs\n",
      "**Lists**\n",
      "names = ['Mary Gallagher', 'John Sanin(?)', 'Anthony Clark', 'Margaret Farrell']\n",
      "ages = [28, 19, 60, 30]\n",
      "professions = ['married', 'laborer', 'laborer', 'widow']\n",
      "But it would be nice if we could somehow group Mary Gallagher's name, age, and profession into a single Python data collection, since it all corresponds to a single person. Luckily we can with something called a \"dictionary\"!\n",
      "person_1 = {\"name\": \"Mary Gallagher\",\n",
      "             \"age\": 28,\n",
      "             \"profession\": \"married\"}\n",
      "type(person_1)\n",
      "A dictionary is made up of \"key\"-\"value\" pairs, which are separated by a colon `:` and separated from other key-value pairs by a comma `,`. A dictionary is always enclosed by curly brackets `{}`. \n",
      "You can check all the keys in a dictionary by using the `.keys()` method or all the values in a dictionary by using the `.values()` method.\n",
      "person_1.keys()\n",
      "person_1.values()\n",
      "## Access Items\n",
      "You can access a value in a dictionary by using square brackets `[]` and its key name (kind of like how we indexed a string or a list).\n",
      "person_1[\"name\"]\n",
      "person_1[\"age\"]\n",
      "person_1[\"profession\"]\n",
      "## Change Item\n",
      "You can change a value in a dictionary by re-assigning a new value to a dictionary key.\n",
      "person_1[\"age\"] = 100\n",
      "person_1\n",
      "person_1['profession'] = 'spinster'\n",
      "person_1\n",
      "## Nested Dictionary\n",
      "You can also nest a dictionary inside another dictionary.\n",
      "bellevue_people = {\n",
      "                \"person_1\":\n",
      "                  {\"name\": \"Mary Gallagher\",\n",
      "                   \"age\": 28,\n",
      "                   \"profession\": \"married\"},\n",
      "                \"person_2\":\n",
      "                  {\"name\": \"John Sanin(?)\",\n",
      "                   \"age\": 19,\n",
      "                   \"profession\": \"laborer\"}\n",
      "                }\n",
      "bellevue_people['person_1']\n",
      "bellevue_people['person_1']['name']\n",
      "bellevue_people['person_2']\n",
      "bellevue_people['person_2']['age']\n",
      "## Iterate Through Dictionary\n",
      "for person in bellevue_people.keys():\n",
      "    print(person)\n",
      "for person in bellevue_people.values():\n",
      "    print(person)\n",
      "for person in bellevue_people.values():\n",
      "    if person['age'] > 20:\n",
      "        name = person['name']\n",
      "        age = person['age']\n",
      "        print(f'{name} is more than 20 years old. She is {age}.')\n",
      "for person in bellevue_people.items():\n",
      "    print(person)\n",
      "## How to Use Jupyter\n",
      "## What is Jupyter?\n",
      "Jupyter is an interactive environment for writing code, which means that, instead of writing static code in a text editor, we can test out bits and pieces as we go.\n",
      "\n",
      "For example, one of the most basic Python functions is `print()`, which will display or \"print\" values to the screen. Here I can display my feelings about Jupyter as well as a quotation from a great Carly Rae Jepsen song:\n",
      "my_feelings_about_jupyter = \"I really,\\n really,\\n really,\\n really,\\n really,\\n really\\n like you\"\n",
      "print(my_feelings_about_jupyter)\n",
      "In Jupyter, I can also beautifully display and easily explore data, such as this CSV data from [Cornell's Movie Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html):\n",
      "import pandas as pd\n",
      "movie_data = pd.read_csv('../data/cornell-movie-corpus/movie_titles_metadata.csv', parse_dates=True)\n",
      "movie_data.head()\n",
      "movie_data.hist()\n",
      "## Jupyter \"notebook\"\n",
      "Jupyter \"notebooks\" are documents that can combine live code, explanatory text, images, and very pretty displays of data. This clutch combination makes them great for exploring data and really great for teaching and learning. *This* is a Jupyter notebook!\n",
      "## Launch JupyterLab\n",
      "> \"JupyterLab is the next generation of the Jupyter Notebook\"\n",
      "\n",
      "In this class, we're going to be using JupyterLab, which is \"the next generation of the Jupyter Notebook.\" JupyterLab runs classic Jupyter notebooks, but it also has a bigger and better user interface — as well as other improved features. \n",
      "You can launch JupyterLab from Anaconda Navigator:\n",
      "<img src=\"../images/installation/navigator-app.png\" width=100% , border=2>\n",
      "<img src=\"https://docs.anaconda.com/_images/nav-defaults.png\" widht=100%, border=2>\n",
      "Or, if Anaconda is added to your PATH, you can launch Jupyter Lab from the command line:\n",
      "%jupyter lab\n",
      "JupyterLab will open in a web browser, which is a little confusing. It's not actually connected to the internet. It's just running on a local server on your computer.\n",
      "\n",
      "IMPORTANT: If JupyterLab isn't running / the local server isn't running, then you can't open Jupyter notebook files (.ipynb)!\n",
      "## Make a New Jupyter Notebook\n",
      "To make a new notebook, select the Python 3 icon under \"Notebook.\"\n",
      "<img src=\"../images/make-new-Jupyter.png\" width=100%, border=2>\n",
      "## Create and Run a Cell\n",
      "Jupyter notebooks consist of \"cells,\" which can either contain code or [Markdown](https://www.markdownguide.org/getting-started/) text. Markdown is a way of writing plain text with formatting. \n",
      "For example, to make text *italics* or **bold**, *you put asterisks around it.*\n",
      "For example, to make text *italics* or **bold**, *you put **asterisks** around it.*\n",
      "* You can create a new cell by clicking the plus `+` sign in the toolbar or by pressing `Option` + `Return` (Mac) / `Alt` `Return` (Windows)\n",
      "* You can change the cell from \"Code\" to \"Markdown\" by clicking the drop down in the toolbar.\n",
      "* You can run the cell by cliking the play button ▶️ on the toolbar above or by typing `Shift` + `Return`.\n",
      "## Save Your Notebook\n",
      "If you want to save your notebook, press `Command ⌘` + `S` (Mac) / `Windows Key` + `S` (Windows).\n",
      "## Conditionals and Comparisons\n",
      "[Download this file and Bellevue Almshouse data (not necessary but optional to explore)](https://melaniewalsh.org/Conditionals-Comparisons.zip)\n",
      "| **Comparison Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x == y `         | `True` if x is equal to y                                                                                |\n",
      "| `x != y `         | `True` if x is not equal to y                                               |\n",
      "| `x > y`       |  `True` if x is greater than y                                                        |\n",
      "| `x < y`       |   `True` if x is less than y  \n",
      "| `x >= y`       |   `True` if x is greater than or equal to y |\n",
      "| `x <= y`      | `True` if x is less than or equal to y`                                                                             |\n",
      "                                                                      |\n",
      "                                                            \n",
      "| **Logical Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x and y`         | `True` if x and y are both True                                                                             |\n",
      "| `x or y`         | `True` if either x or y is True                                              |\n",
      "| `not x`       |  `True` if is x is not True                                                       |\n",
      "                                                            \n",
      "Last lesson, we talked about the four Python data types, which included Booleans or True/False statements. We discussed how if we make a variable called `beyonce` and assign it the value \"Grammy award-winner\"...\n",
      "beyonce = \"Grammy award-winner\"\n",
      "...and then we evaluate that statement with the equals operator `==`...\n",
      "beyonce == \"Grammy award-winner\"\n",
      "type(beyonce == \"Grammy award-winner\")\n",
      "...the operator will return a Boolean, which will tell us whether the statement is True or False.\n",
      "\n",
      "Booleans may seem simple, but they're extremely powerful and function as the engine of a lot of code. They allow us to instruct the computer to do things based on different \"conditions.\"\n",
      "## If Statement\n",
      "An `if` statement is an instruction to do something *if* a particular condition is met.\n",
      "\n",
      "A common conditional will consist of two lines. On the first line, you will type the English word `if` followed by an evaluation `beyonce == \"Grammy award-winner\"` and then a colon `:`. On the second line, you will indent (press `Tab`), then write an instruction to be completed if the condition is met, such as `print(\"Congratulations, Beyonce!\")`\n",
      "beyonce = \"Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "Python is picky about how you format `if` statements. Look what happens if we forget to tab over on the second line or if we forget the colon:\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "print(\"Congratulations, Beyonce!\")\n",
      "if beyonce == \"Grammy award-winner\"\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "## Else Statement\n",
      "You can add even more complexity in a conditional by adding an `else` statement. This will instruct the program to do something in case the condition is not met. You put it after an `if` statement and format it the same way.\n",
      "beyonce = \"not a Grammy award-winner this year\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Elif Statement\n",
      "Sometimes you want even more nuance to respond to slightly different conditions. For example, if Beyonce was nominated for a Grammy but didn't win, then we might want to express a slightly different sentiment than if she won or was not nominated at all. You can add in this nuance with an `elif` state or else if statement, which will evaluate the first `if` statement and then *if* that statement is not True, it will evaluate the `elif` statement.\n",
      "beyonce = \"Grammy award-nominee\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "elif beyonce == \"Grammy award-nominee\":\n",
      "    print(\"Ok well at least they nominated you, Beyonce.\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Group Exercise / HW Assignment 3 (Part I)\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's [Bellevue Almshouse data](https://www.nyuirish.net/almshouse/the-almshouse-records/).\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "We're going to make a series of variables and assign them values based on the Bellevue Almshouse dataset. Make sure you run these cells.\n",
      "#### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "#### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "#### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "#### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Write an `if` statement that reports whether `person1_age` is less than 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "**2.** Write an `if` statement that reports whether `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "**3.** Write an `if` statement that reports whether `person1_age` is less than 30 years old *and* `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old and married.')\n",
      "**4.** Complicate your `if` statement from Question 1 by adding an `else` statement that prints \"Person is older than 30 years old\". Then evaluate whether `person3_age` is less than 30 years old. \n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**5.** Now evaluate whether `person4_age` is less than 30 years using the same code as Question 4.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "Hmmm, with the code as written, it's telling us that Margaret Farrell, who is 30 years old, is *more* than 30 years old. Add an `elif` statement that reports whether the person is exactly 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is exactly 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**6.** Write an `if` statement that will report whether `person1_child_status` includes children.\n",
      "#Your code here\n",
      "    print('Person has children.')\n",
      "**7.** Write one `if` statement that will accurately report whether `person1_child_status` includes children and, separately, if `person2_child_status` includes children. (Hint: think about how you might use the `!=` operator.)\n",
      "if person1_child_status #Your Code Here\n",
      "    print('Person has children.')\n",
      "if person2_child_status #Same Code Here\n",
      "    print('Person has children.')\n",
      "**8.** Write a conditional that will report whether `person1_profession` is \"married,\" \"laborer,\" \"widow,\" or \"unknown profession.\" Test your code by reassigning the variable as indicated below.\n",
      "person1_profession = 'married'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'laborer'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'widow'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'student'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "**9.** Some of the Irish immigrants' names have question marks after them. Let's clean up some of the data and remove the question marks. You can use the Python keyword `in` to test whether a string appears within another string. Print `person2_name` with the question mark and parentheses removed. (Hint: think about f-strings and string methods!)\n",
      "if \"(?)\" in person2_name:\n",
      "    #Your code here\n",
      "**10.** In a few sentences, write about your experiencing using and manipulating the Bellevue Almshouse data after reading Shrout's essay. How, if at all, did using this data influence your understanding of how Python works?\n",
      "**Your thoughts here**\n",
      "## Jupyter Keyboard Shortcuts\n",
      "## Command Mode\n",
      "\n",
      "\n",
      "| Mac        | Jupyter Function                                                                                             | Windows       | \n",
      "|:---------------------------:|:-----------------------------------------------------------------------------------------------------------:|:---------------------------:|\n",
      "| `Shift` + `Return`  | Run cell  (**Both modes**)                                            | `Shift` + `Enter`     |\n",
      "| `Option` + `Return`                      | Create new cell below (**Both modes**)                                          | `Alt` + `Enter` \n",
      "| `B`                      | Create new cell below (**Command mode**)                                         | `B` \n",
      "| `A`                      | Create new cell above (**Command mode**)                                       | `A` \n",
      "| `D` + `D`                      | Delete cell (**Command mode**)                                         | `D` + `D`                    |\n",
      "| `Z`                      | Undo cell action (**Command mode**)                                         | `Z`  \n",
      "| `Shift` + `M`                     | Merge cells (**Command mode**)                     | `Shift` + `m`                      | \n",
      "| `Control` + `Shift` + `-`                   | Split cell into two cells (**Edit mode**)   |         `Control` + `Shift` + `-`   |                          \n",
      "| `Tab`                      | Autocomplete file/variable/function name (**Edit mode**)     | `Tab`                        \n",
      "\n",
      "## Tab Autocomplete\n",
      "One of the most useful things about Jupyter notebooks is that you can hit `Tab` to autocomplete a file path or a function name. Typing out the correct file path isn't always easy, and tab autocomlete helps enormously! \n",
      "from IPython.display import IFrame\n",
      "IFrame(\"../videos/Tab-Autocomplete.mp4\", width='100%', height='300px')\n",
      "## More Resources\n",
      "* [Jupyter Python Notebook Keyboard Shortcuts and Text Snippets for Beginners](http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html)\n",
      "## The Life and Anatomy of a Python Script\n",
      "<img src=\"https://cdn.pixabay.com/photo/2017/01/31/23/21/animal-2028134_960_720.png\" alt=\"The command line\" width=\"100%\", border=2>\n",
      "\n",
      "```{admonition} Heads up!\n",
      ":class: headsup\n",
      "To run any of the code on this page, you need to run the cell below\n",
      "```\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "most_frequent_meaningful_words\n",
      "Calculating word frequency is a very basic form of computational text analysis. Often it is not terribly interesting on its own, especially with a single short text. However, calculating word frequency *is* important. It's at the center of most text analysis approaches, even far more complicated ones.\n",
      "```{admonition} A Note About Python Philosophy\n",
      ":class: pythonreview\n",
      "This is just *one* way to count words in a text file, not the one *right* way. There is no *right* way to count words in a text file or to do anything else in Python.\n",
      "\n",
      "Rather than asking \"Is this code *right*?\", you want to ask:\n",
      "- \"Is this code efficient?\"\n",
      "- \"Is this code readable?\"\n",
      "- \"Does this code help me accomplish my goal?\"\n",
      "\n",
      "Sometimes you'll prioritize one of these concerns over another. For example, maybe your code isn't as efficient as humanly possible, but if it gets the job done, and you understand it, then you might not care about maximum efficiency.\n",
      "```\n",
      "## The Anatomy of a Python Script\n",
      "#### Import Libraries/Packages/Modules\n",
      "import re\n",
      "from collections import Counter\n",
      "Ready for some great Python news? You don't have to code everything by yourself from scratch! Many other people have written Python code that you can `import` into your own code, which will save you time and often do a lot of complex, powerful work behind-the-scenes. \n",
      "\n",
      "We call the code written and packaged up by other people a \"library,\" \"package,\" or \"module.\" We'll talk more about them [in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html##Reading-in-a-CSV-File). For now simply know that you `import` libraries/packages/modules at the very top of a Python script for later use.\n",
      "\n",
      "- [`Counter`](https://stackabuse.com/introduction-to-pythons-collections-module/) will help me count words\n",
      "- `re`, short for regular expressions, is basically a fancy find-and-replace that will help me split \"The Yellow Wallpaper\" into individual words and get rid of trailing punctuation\n",
      "#### Define Functions\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    words = re.split(\"\\W+\", any_chunk_of_text.lower())\n",
      "    return words \n",
      "After `import`ing modules and libraries, you typically `def`ine your \"functions.\" Functions are a nifty way to bundle up code so that you can use them again later. Functions also keep your code neat and tidy.\n",
      "\n",
      "Here we're making a function called `split_into_words`, which takes in any chunk of text, transforms that text to lower-case, and splits the text into a list of clean words without punctuation or spaces. We're not actually using the function yet. We're just setting it up. We'll talk more about functions in two weeks.\n",
      "#### Define Filepaths and [Assign Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html)\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "number_of_desired_words = 40\n",
      "Here we establish some values and data that we're going to call later. We'll need the filepath of the short story in order to read it, so we make a variable called `filepath_of_text` and assign it to the relative path of the text file \"../texts/literature/The-Yellow-Wallpaper.txt\". We also make a variable called `stopwords` and plug in a list of English language stopwords. Lastly we make a variable called `number_of_desired_words`, which will eventually tell the script how many words to display, and we assign it the value `40`. If we changed this value to `50`, then the script would display 50 words instead.\n",
      "#### [Read in File](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Character-Encoding.html)\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "The line above opens Charlotte Perkins Gilman's \"The Yellow Wallpaper,\" reads in the novel, and then assigns it to the variable `full_text`.\n",
      "#### Manipulate and Analyze File\n",
      "all_the_words = split_into_words(full_text)\n",
      "To count the words in \"The Yellow Wallpaper,\" we need to break the full text into individual words. Above  we call the function `split_into_words`, which we created earlier, and use it to split the `full_text` of the story into individual words. Then we put the words inside a variable called `all_the_words`.\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "At this point, we don't really care about tiny words — such as \"the,\" \"and,\" \"or,\" — so we're going to remove them from our list. The line of code above makes a new list of words that includes every word in `all_the_words` if it does *not* appear in `nltk_stop_words` (aka it nixes the stopwords). We'll learn more about [lists and for loops in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html).\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "Now we're ready to count! We plug `meaningful_words` into our `Counter` ([discussed in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html##Count-Items-In-a-List-or-Collection)), which gives us a tally of how many times each word in the story appears. Then we put the tally into a new variable called `meaningful_words_tally`. This kind of tally is called a \"dictionary,\" which we will discuss two weeks from now.\n",
      "#### Output Results\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "Lastly, we pull out the top 40 most frequently occurring words from our complete tally. We make one final variable and grab our top `number_of_desired_words`, which we previously established as \"40.\"\n",
      "print(most_frequent_meaningful_words)\n",
      "Then we display the results with `print()`, a built-in Python function for displaying data. \n",
      "#### Comments\n",
      "The cell below shows the script again with explanations. These are called comments. Lines that begin with a hash symbol `##` are excluded from the running code, so you can use them to write notes or instructions to yourself or others. If you want to write a multi-line comment, you can put it in between three quotations marks `\"\"\" \"\"\"`.\n",
      "\"\"\"\n",
      "Example Python code for\n",
      "calculating word frequency\n",
      "in a text file\n",
      "\"\"\"\n",
      "\n",
      "##Import Libraries and Modules\n",
      "\n",
      "\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "## Define Functions\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "## Define Filepaths and Assign Variables\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "## Read in File\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "## Manipulate and Analyze File\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "## Output Results\n",
      "\n",
      "most_frequent_meaningful_words\n",
      "## The Life of a Python Script\n",
      "#### Jupyter Notebook / JupyterLab\n",
      "The primary way that we're going to write and run Python in this class is through JupyterLab and Jupyter notebooks. As we've already covered, Jupyter notebooks are documents that can combine live code, explanatory text, and nice displays of data, which makes them great for teaching and learning. But it's also a fully functional way to run Python. By running a cell of Python code in a Jupyter notebook, you can read files from your computer and write files to your computer, you can make and save a bar chart, you can gather data from YouTube and Spotify, you can programmatically tweet from a Twitter bot account, and more!\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "with open(\"most-frequent-words-Yellow-Wallpaper.txt\", \"w\") as file_object:\n",
      "    file_object.write(str(most_frequent_meaningful_words))\n",
      "By adding two lines of code at the bottom of our script, I can output the most frequent words from \"The Yellow Wallpaper\" into a text file.\n",
      "#### Text Editor —> Command Line\n",
      "<img src=\"../images/Python-plain-text.png\" width=100%, border=2>\n",
      "\n",
      "!python word_frequency_Yellow_Wallpaper.py\n",
      "<img src=\"../images/Python-Atom.png\" width=100%, border=2>\n",
      "Though it's possible to write Python from TextEdit, it's not very common, because it's a pain. It's much more common to write Python code in a text editor like Atom, as shown above. You can see that there's all sorts of formatting and functionality that makes the code writing faster and easier.\n",
      "You can also write Python scripts such that they can work with different files or any file you want it to. With a few small alterations, our word frequency script can crunch numbers for Grimms Fairy Tales...\n",
      "!python word_frequency.py ../texts/literature/Grimms-Fairy-Tales.txt\n",
      "or Louisa May Alcott's *Little Women*...\n",
      "!python word_frequency.py ../texts/literature/Little-Women.txt\n",
      "or any other text your heart desires!\n",
      "## Make Random Student Groups\n",
      "import random\n",
      "students = ['LeBron James',\n",
      "'Giannis Antetokounmpo',                \n",
      "'Kevin Durant',\n",
      "'Steph Curry',\n",
      "'Kyrie Irving',\n",
      "'Joel Embiid', \n",
      "'Kawhi Leonard', \n",
      "'Paul George', \n",
      "'James Harden', \n",
      "'Kemba Walker', \n",
      "'Khris Middleton', \n",
      "'Anthony Davis', \n",
      "'Nikola Jokić', \n",
      "'Klay Thompson', \n",
      "'Ben Simmons', \n",
      "'Damian Lillard', \n",
      "'Blake Griffin', \n",
      "'Russell Westbrook', \n",
      "'D\\'Angelo Russell', \n",
      "'LaMarcus Aldridge', \n",
      "'Nikola Vučević', \n",
      "'Karl-Anthony Towns', \n",
      "'Kyle Lowry', \n",
      "'Bradley Beal', \n",
      "'Dwyane Wade', \n",
      "'Dirk Nowitzki']\n",
      "def make_random_groups (students, number_of_groups):\n",
      "    \n",
      "    random.shuffle(students)\n",
      "    \n",
      "    all_groups = []\n",
      "    \n",
      "    for index in range(number_of_groups):\n",
      "        group = students[index::number_of_groups]\n",
      "        all_groups.append(group)\n",
      "    \n",
      "    for index, group in enumerate(all_groups):\n",
      "        formatted_group = ' / '.join(group)\n",
      "        \n",
      "        print(f\"\"\"✨Group {index+1}✨: {formatted_group}\n",
      "        \"\"\")\n",
      "    return\n",
      "make_random_groups(students, 10)\n",
      "## Make Random Student Groups Explained\n",
      "def make_random_groups (students, number_of_groups):\n",
      "    \n",
      "    #Shuffle the order of the students\n",
      "    random.shuffle(students)\n",
      "    \n",
      "    #Make an empty list where we will put all the student groups\n",
      "    all_groups = []\n",
      "    \n",
      "    for index in range(number_of_groups):\n",
      "        \n",
      "        \"\"\"\n",
      "        Extract a group of students from the list.\n",
      "        Start by extracting the student at spot # index in the list\n",
      "        then jump by number_of groups to extract the next student, and so on.\n",
      "        \n",
      "        Let's say we have 15 students and want 5 groups.\n",
      "        \n",
      "        For each number in 5 (1,2,3,4,5), we make a group by\n",
      "        taking the 1st person in the list, jumping by five to take the 6th person in the list,\n",
      "        then jumping by five to take the 11th person in the list.\n",
      "        \n",
      "        group = students[1::5]\n",
      "        \n",
      "        That's Group 1.\n",
      "        \n",
      "        Then we move on to take the 2nd, 7th, and 12th person.\n",
      "        \n",
      "        group = students[2::5]\n",
      "        \n",
      "        That's Group 2.\n",
      "        \n",
      "        Continue until Group 5!\n",
      "        \"\"\"\n",
      "        \n",
      "        group = students[index::number_of_groups]\n",
      "        \n",
      "        #Add this group to our all_groups master list\n",
      "        all_groups.append(group)\n",
      "    \n",
      "    for index, group in enumerate(all_groups):\n",
      "        \n",
      "        #Nicely format the groups by joining the student names together with a slash\n",
      "        formatted_group = ' / '.join(group)\n",
      "        \n",
      "        #Print out the final student groups!\n",
      "        print(f\"\"\"\n",
      "                ✨Group {index+1}✨:\n",
      "                        \n",
      "                {formatted_group}\n",
      "        \"\"\")\n",
      "    return\n",
      "## Files and Character Encoding\n",
      "## Open a Text File\n",
      "The most basic way to open a file is to use Python's built-in `open()` function and to insert your desired file path surrounded by quotation marks. That creates a file object. Then we tack on the `.read()` method to transform the file object into one big string. But, as we will see below, it's good practice to add an extra step here in order to accommodate character encoding issues.\n",
      "open('sample-character-encoding.txt').read()\n",
      "## Character Encoding\n",
      "> Written text is a sequence of graphemes – characters. Every character you type – whether letters like ‘a’ and ‘b’, punctuation marks like ‘?’, or even emoji like 💪👬👍 – has an ID number that computers use to store it. In order to communicate, computers need to agree on a common roster of how to assign these graphemes to numbers and vice versa. These rosters are known as character encodings.\n",
      "\n",
      "> -Aditya Mukerjee, \"[I Can Text You A Pile of Poo, But I Can’t Write My Name](https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name)\"\n",
      "Python uses the UTF-8 character encoding, or Unicode, by default. Unicode is the most popular character encoding on the internet and even includes emojis.\n",
      "\n",
      "However, as Mukerjee points out in his essay, Unicode still does not include characters that are essential to the Bengali alphabet as well as to many other non-English languages.\n",
      "sample_text_default = open('sample-character-encoding.txt').read()\n",
      "print(sample_text_default)\n",
      "## Computers Don't Know What Text Is\n",
      "That's why they need character encoding, which is a system that maps character to numbers. You can check the Unicode \"code point,\" or place in the Unicode universe, for any character with the function `ord()`\n",
      "help(ord)\n",
      "ord?\n",
      "ord(\"a\")\n",
      "ord(\"💩\")\n",
      "ord(\"ত\")\n",
      "ord(\"!\")\n",
      "## Adding (UTF-8) Encoding for Text Files\n",
      "Though Python reads UTF-8 by default, it's good practice to explicitly declare UTF-8 encoding, as below.\n",
      "sample_text_default = open('sample-character-encoding.txt', encoding='utf-8').read()\n",
      "print(sample_text_default)\n",
      "Look what happens if we read in the exact same text with a different encoding.\n",
      "sample_text_iso = open('sample-character-encoding.txt', encoding='iso-8859-1').read()\n",
      "print(sample_text_iso)\n",
      "sample_text_ascii = open('sample-character-encoding.txt', encoding='ascii').read()\n",
      "print(sample_text_ascii)\n",
      "## Debugging Tip\n",
      "> If you open a document and it looks like this, there's one and only one reason for it: Your text editor, browser, word processor or whatever else that's trying to read the document is assuming the wrong encoding. That's all. The document is not broken (well, unless it is, see below), there's no magic you need to perform, you simply need to select the right encoding to display the document.\n",
      "\n",
      "> -[What Every Programmer Absolutely, Positively Needs to Know About Encodings and Character Sets to Work With Text](http://kunststube.net/encoding/)\n",
      "sample_text_iso = open('sample-character-encoding.txt', encoding='iso-8859-1').read()\n",
      "print(sample_text_iso)\n",
      "## Functions\n",
      "[Download this \"Functions\" notebook and other relevant files here](https://melaniewalsh.org/Functions-More-Pandas.zip)\n",
      "<img src=\"https://www.tintoyarcade.com/image/cache/data/product/Images_5100_5199/TTA5100-Penguin-Windup-01-1000x1000.jpg\" width=100%, border=2>\n",
      "A \"function\" is way of bundling up code to perform specific tasks. It's kind of like making a little Python wind-up toy that runs on command.\n",
      "\n",
      "Functions are useful because they can help make our code more organized, and because they can save us from repetition. If you have to do some task over and over again, you don't want to write out the same code over and over again from scratch. It's easier to just make a function that contains that code and simply \"call\" it whenever you need it.\n",
      "\n",
      "We've encountered functions many times already. For example, `print()`, `len()`, and `type()` are all built-in Python functions. They contain bundled up code that performs specific tasks whenever we call them.\n",
      "## Define a Function\n",
      "To make your own function, you use the keyword `def`—short for \"define\"—followed by your desired name for the function, parentheses `()` and a colon `:`.\n",
      "\n",
      "Then, on the following lines, you indent one tab over and write some code that you want your function to perform. In our case, we're using multiple `print()` statements to print some lyrics from Beyonce's \"Formation.\"\n",
      "\n",
      "Finally, you complete the function with a `return` statement. Sometimes you will want to `return` a specific value but here we're not returning anything.\n",
      "def sing_beyonce_lyrics():\n",
      "    print(\"Okay, okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "    print(\"Okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "    print(\"Prove to me you got some coordination, 'cause I slay\")\n",
      "    print(\"Slay trick, or you get eliminated\")\n",
      "    return \n",
      "Behold the importance of the indent:\n",
      "def sing_beyonce_lyrics():\n",
      "print(\"Okay, okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "print(\"Okay, ladies, now let's get in formation, 'cause I slay\")\n",
      "print(\"Prove to me you got some coordination, 'cause I slay\")\n",
      "print(\"Slay trick, or you get eliminated\")\n",
      "    return \n",
      "## Call a Function\n",
      "To use or \"call\" a function, you simply type the name of the function with parentheses.\n",
      "sing_beyonce_lyrics()\n",
      "def sing_happy_birthday():\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday dear human life form\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    return \n",
      "sing_happy_birthday()\n",
      "## Add Parameters/Arguments\n",
      "You can add \"parameters\" to your functions—or values that are required by your function—by putting parameter names inside the parentheses. For example, if we want to personalize our birthday song function to include a specific person's name, we can add the parameter `personalized_name` inside the parentheses, which will require a personalized name to be passed to the function. The thing you pass to the function is called an \"argument.\" \n",
      "\n",
      "- parameter = `personalized_name` (thing that requires a value for the function) \n",
      "- argument = \"Beyonce\" (actual value passed to function)\n",
      "\n",
      "Since parameters and arguments are so interrelated, they're sometimes confused for each other. You can read [Python's official distinction here](https://docs.python.org/3.3/faq/programming.html#faq-argument-vs-parameter).\n",
      "def sing_personalized_happy_birthday(personalized_name):\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(f\"Happy Birthday dear {personalized_name}\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    return \n",
      "^^^ Later on in the code, we'll use whatever name gets passed to the function inside an f-string: `f\"Happy Birthday dear {personalized_name}\"`\n",
      "Once you set a parameter that requires an argument, you have to pass something inside the function for the function to run. So if we run `sing_personalized_happy_birthday()` as we did with `sing_happy_birthday()`, it won't work.\n",
      "sing_personalized_happy_birthday()\n",
      "This error is telling us that we have to pass in a value or \"argument.\"\n",
      "sing_personalized_happy_birthday(\"Beyonce\")\n",
      "sing_personalized_happy_birthday(\"Carly Rae Jepsen\")\n",
      "sing_personalized_happy_birthday(#Insert Your Name Here)\n",
      "## Keyword Arguments\n",
      "There's another way that you can require arguments in a function, which is with \"keyword arguments.\" Before we were using \"positional arguments,\" where the function automatically knew that \"Beyonce\" was the `personalized_name` argument simply because \"Beyonce\" was in the right position. (There was only one argument required, so, duh.)\n",
      "\n",
      "But you can also explicitly define your arguments with keyword arguments that use an `=` sign, which can become more useful if you have multiple parameters. This can also be a way of setting default values in your functions.\n",
      "def sing_keyword_happy_birthday(to_name='Beyonce', from_name='Info 1350'):\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(f\"Happy Birthday dear {to_name}\")\n",
      "    print(\"Happy Birthday to you\")\n",
      "    print(f\"\\nSincerely, \\n{from_name}\")\n",
      "    return \n",
      "For example, if we don't pass in any arguments into this function, it will use the default arguments.\n",
      "sing_keyword_happy_birthday()\n",
      "But if we set the keyword arguments to different values—even if we switch the order or position of the arguments!—the function will know which arguments they're supposed to be.\n",
      "sing_keyword_happy_birthday(from_name=\"Big Bird\", to_name=\"Cookie Monster\")\n",
      "These keyword arguments help explain some of the errors we've been getting when we use Pandas, such as with the `.sort_values()` method and the `by=` parameter.\n",
      "sing_keyword_happy_birthday(from_name=\"Big Bird\", to_name=\"Cookie Monster\", by=\"Column_name\")\n",
      "## Return Values\n",
      "In all of the examples above, we weren't returning any specific value, just using `print()` statements. But sometimes you want a specific value out of your function. For example, if we want to make a function that transforms a bit of text into very loud-sounding text, then we'll want to `return` that loud-sounding text.\n",
      "def make_text_shouty(text):\n",
      "    shouty_text = text.upper()\n",
      "    return shouty_text\n",
      "make_text_shouty(\"I like tacos\")\n",
      "def make_text_shoutier(text):\n",
      "    shouty_text = text.upper()\n",
      "    shoutier_text = shouty_text + '!!!'\n",
      "    return shoutier_text\n",
      "make_text_shoutier(\"I like tacos\")\n",
      "def calculate_dog_years_age(age):\n",
      "    dog_years_age = age * 7\n",
      "    return dog_years_age\n",
      "calculate_dog_years_age(52)\n",
      "## Your Turn\n",
      "Make a function called `make_text_whispery` that transforms text to lower case\n",
      "#Your Code Here\n",
      "    whispery_text = #Your Code HEre\n",
      "    return #Your Code Here\n",
      "Now insert the string \"I AM WHISPERING\" into `make_text_whispery`\n",
      "#Your Code Here\n",
      "## The Life and Anatomy of a Python Script\n",
      "<img src=\"https://cdn.pixabay.com/photo/2017/01/31/23/21/animal-2028134_960_720.png\" alt=\"The command line\" width=\"100%\", border=2>\n",
      "\n",
      "```{admonition} Heads up!\n",
      ":class: headsup\n",
      "To run any of the code on this page, you need to run the cell below\n",
      "```\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "most_frequent_meaningful_words\n",
      "Calculating word frequency is a very basic form of computational text analysis. Often it is not terribly interesting on its own, especially with a single short text. However, calculating word frequency *is* important. It's at the center of most text analysis approaches, even far more complicated ones.\n",
      "```{admonition} A Note About Python Philosophy\n",
      ":class: pythonreview\n",
      "This is just *one* way to count words in a text file, not the one *right* way. There is no *right* way to count words in a text file or to do anything else in Python.\n",
      "\n",
      "Rather than asking \"Is this code *right*?\", you want to ask:\n",
      "- \"Is this code efficient?\"\n",
      "- \"Is this code readable?\"\n",
      "- \"Does this code help me accomplish my goal?\"\n",
      "\n",
      "Sometimes you'll prioritize one of these concerns over another. For example, maybe your code isn't as efficient as humanly possible, but if it gets the job done, and you understand it, then you might not care about maximum efficiency.\n",
      "```\n",
      "## The Anatomy of a Python Script\n",
      "#### Import Libraries/Packages/Modules\n",
      "import re\n",
      "from collections import Counter\n",
      "Ready for some great Python news? You don't have to code everything by yourself from scratch! Many other people have written Python code that you can `import` into your own code, which will save you time and often do a lot of complex, powerful work behind-the-scenes. \n",
      "\n",
      "We call the code written and packaged up by other people a \"library,\" \"package,\" or \"module.\" We'll talk more about them [in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html##Reading-in-a-CSV-File). For now simply know that you `import` libraries/packages/modules at the very top of a Python script for later use.\n",
      "\n",
      "- [`Counter`](https://stackabuse.com/introduction-to-pythons-collections-module/) will help me count words\n",
      "- `re`, short for regular expressions, is basically a fancy find-and-replace that will help me split \"The Yellow Wallpaper\" into individual words and get rid of trailing punctuation\n",
      "#### Define Functions\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    words = re.split(\"\\W+\", any_chunk_of_text.lower())\n",
      "    return words \n",
      "After `import`ing modules and libraries, you typically `def`ine your \"functions.\" Functions are a nifty way to bundle up code so that you can use them again later. Functions also keep your code neat and tidy.\n",
      "\n",
      "Here we're making a function called `split_into_words`, which takes in any chunk of text, transforms that text to lower-case, and splits the text into a list of clean words without punctuation or spaces. We're not actually using the function yet. We're just setting it up. We'll talk more about functions in two weeks.\n",
      "#### Define Filepaths and [Assign Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html)\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "number_of_desired_words = 40\n",
      "Here we establish some values and data that we're going to call later. We'll need the filepath of the short story in order to read it, so we make a variable called `filepath_of_text` and assign it to the relative path of the text file \"../texts/literature/The-Yellow-Wallpaper.txt\". We also make a variable called `stopwords` and plug in a list of English language stopwords. Lastly we make a variable called `number_of_desired_words`, which will eventually tell the script how many words to display, and we assign it the value `40`. If we changed this value to `50`, then the script would display 50 words instead.\n",
      "#### [Read in File](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Character-Encoding.html)\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "The line above opens Charlotte Perkins Gilman's \"The Yellow Wallpaper,\" reads in the novel, and then assigns it to the variable `full_text`.\n",
      "#### Manipulate and Analyze File\n",
      "all_the_words = split_into_words(full_text)\n",
      "To count the words in \"The Yellow Wallpaper,\" we need to break the full text into individual words. Above  we call the function `split_into_words`, which we created earlier, and use it to split the `full_text` of the story into individual words. Then we put the words inside a variable called `all_the_words`.\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "At this point, we don't really care about tiny words — such as \"the,\" \"and,\" \"or,\" — so we're going to remove them from our list. The line of code above makes a new list of words that includes every word in `all_the_words` if it does *not* appear in `nltk_stop_words` (aka it nixes the stopwords). We'll learn more about [lists and for loops in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html).\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "Now we're ready to count! We plug `meaningful_words` into our `Counter` ([discussed in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html##Count-Items-In-a-List-or-Collection)), which gives us a tally of how many times each word in the story appears. Then we put the tally into a new variable called `meaningful_words_tally`. This kind of tally is called a \"dictionary,\" which we will discuss two weeks from now.\n",
      "#### Output Results\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "Lastly, we pull out the top 40 most frequently occurring words from our complete tally. We make one final variable and grab our top `number_of_desired_words`, which we previously established as \"40.\"\n",
      "print(most_frequent_meaningful_words)\n",
      "Then we display the results with `print()`, a built-in Python function for displaying data. \n",
      "#### Comments\n",
      "The cell below shows the script again with explanations. These are called comments. Lines that begin with a hash symbol `##` are excluded from the running code, so you can use them to write notes or instructions to yourself or others. If you want to write a multi-line comment, you can put it in between three quotations marks `\"\"\" \"\"\"`.\n",
      "\"\"\"\n",
      "Example Python code for\n",
      "calculating word frequency\n",
      "in a text file\n",
      "\"\"\"\n",
      "\n",
      "##Import Libraries and Modules\n",
      "\n",
      "\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "## Define Functions\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "## Define Filepaths and Assign Variables\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "## Read in File\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "## Manipulate and Analyze File\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "## Output Results\n",
      "\n",
      "most_frequent_meaningful_words\n",
      "## The Life of a Python Script\n",
      "#### Jupyter Notebook / JupyterLab\n",
      "The primary way that we're going to write and run Python in this class is through JupyterLab and Jupyter notebooks. As we've already covered, Jupyter notebooks are documents that can combine live code, explanatory text, and nice displays of data, which makes them great for teaching and learning. But it's also a fully functional way to run Python. By running a cell of Python code in a Jupyter notebook, you can read files from your computer and write files to your computer, you can make and save a bar chart, you can gather data from YouTube and Spotify, you can programmatically tweet from a Twitter bot account, and more!\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "with open(\"most-frequent-words-Yellow-Wallpaper.txt\", \"w\") as file_object:\n",
      "    file_object.write(str(most_frequent_meaningful_words))\n",
      "By adding two lines of code at the bottom of our script, I can output the most frequent words from \"The Yellow Wallpaper\" into a text file.\n",
      "#### Text Editor —> Command Line\n",
      "<img src=\"../images/Python-plain-text.png\" width=100%, border=2>\n",
      "\n",
      "!python word_frequency_Yellow_Wallpaper.py\n",
      "<img src=\"../images/Python-Atom.png\" width=100%, border=2>\n",
      "Though it's possible to write Python from TextEdit, it's not very common, because it's a pain. It's much more common to write Python code in a text editor like Atom, as shown above. You can see that there's all sorts of formatting and functionality that makes the code writing faster and easier.\n",
      "You can also write Python scripts such that they can work with different files or any file you want it to. With a few small alterations, our word frequency script can crunch numbers for Grimms Fairy Tales...\n",
      "!python word_frequency.py ../texts/literature/Grimms-Fairy-Tales.txt\n",
      "or Louisa May Alcott's *Little Women*...\n",
      "!python word_frequency.py ../texts/literature/Little-Women.txt\n",
      "or any other text your heart desires!\n",
      "## Jupyter Keyboard Shortcuts\n",
      "## Command Mode\n",
      "\n",
      "\n",
      "| Mac        | Jupyter Function                                                                                             | Windows       | \n",
      "|:---------------------------:|:-----------------------------------------------------------------------------------------------------------:|:---------------------------:|\n",
      "| `Shift` + `Return`  | Run cell  (**Both modes**)                                            | `Shift` + `Enter`     |\n",
      "| `Option` + `Return`                      | Create new cell below (**Both modes**)                                          | `Alt` + `Enter` \n",
      "| `B`                      | Create new cell below (**Command mode**)                                         | `B` \n",
      "| `A`                      | Create new cell above (**Command mode**)                                       | `A` \n",
      "| `D` + `D`                      | Delete cell (**Command mode**)                                         | `D` + `D`                    |\n",
      "| `Z`                      | Undo cell action (**Command mode**)                                         | `Z`  \n",
      "| `Shift` + `M`                     | Merge cells (**Command mode**)                     | `Shift` + `m`                      | \n",
      "| `Control` + `Shift` + `-`                   | Split cell into two cells (**Edit mode**)   |         `Control` + `Shift` + `-`   |                          \n",
      "| `Tab`                      | Autocomplete file/variable/function name (**Edit mode**)     | `Tab`                        \n",
      "\n",
      "## Tab Autocomplete\n",
      "One of the most useful things about Jupyter notebooks is that you can hit `Tab` to autocomplete a file path or a function name. Typing out the correct file path isn't always easy, and tab autocomlete helps enormously! \n",
      "from IPython.display import IFrame\n",
      "IFrame(\"../videos/Tab-Autocomplete.mp4\", width='100%', height='300px')\n",
      "## More Resources\n",
      "* [Jupyter Python Notebook Keyboard Shortcuts and Text Snippets for Beginners](http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html)\n",
      "## Genius Song Lyric API\n",
      "from secrets import *\n",
      "import lyricsgenius\n",
      "genius = lyricsgenius.Genius(client_access_token)\n",
      "genius.remove_section_headers = True\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    resp = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_str = resp.text\n",
      "    document = BeautifulSoup(html_str, \"html.parser\")\n",
      "    title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [title.text for title in title_tags]\n",
      "    clean_songs = []\n",
      "    for song in song_titles:\n",
      "    #re.split(r'\\W', song)\n",
      "    #song.rstrip()\n",
      "    #song.replace(\" \", \"\")\n",
      "        clean_song = re.search('(?<=\\\\n).*?(?=\\\\n)', song).group(0)\n",
      "        clean_song = clean_song.strip()\n",
      "        clean_song = unicodedata.normalize(\"NFKD\", clean_song)\n",
      "        clean_songs.append(clean_song)\n",
      "    return clean_songs\n",
      "get_all_songs_from_album('Beyonce', 'Lemonade')\n",
      "album_title = 'Lemonade/'\n",
      "for song in clean_songs:\n",
      "    song_lyrics = genius.search_song(song, 'Beyonce')\n",
      "    song_title = song.replace(\" \", \"-\")\n",
      "    #print(song.lyrics)\n",
      "    song_lyrics.save_lyrics(extension='txt', filename=f'{song_title}', sanitize=False)\n",
      "## Lists & For Loops\n",
      "For this lesson, we're again going to draw on Anelise Shrout's [Bellevue Almshouse data](https://docs.google.com/spreadsheets/d/1uf8uaqicknrn0a6STWrVfVMScQQMtzYf5I_QyhB9r7I/edit##gid=2057113261).\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "## Lists\n",
      "Last lesson, when we worked with conditionals, we made a bunch of individual variables about each individual Irish immigrant. Often it's much more advantageous, however, to create a *collection* of values and hold them in a single place. One of the most common Python data collections is called a \"list.\" For example, rather than all these separate variables...\n",
      "**Person 1**\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "**Person 2**\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "**Person 3**\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "**Person 4**\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "...we can create a *list* of all the Irish immigrants' names and hold them in a single place.\n",
      "names = ['Mary Gallagher', 'John Sanin(?)', 'Anthony Clark', 'Margaret Farrell']\n",
      "type(names)\n",
      "ages = [28, 19, 60, 30]\n",
      "type(ages)\n",
      "A list is always enclosed by square brackets `[]` and accepts items in a row separated by commas `,`. A list can contain any combination of Python data types.\n",
      "### Index\n",
      "You can index a list just like you would index a string. For example, if we wanted to pull out the first item in our `names` list, we could put square brackets and our desired index number immediately after the list. Just like with strings, the Python index begins with 0.\n",
      "names[0]\n",
      "names[3]\n",
      "### Slice\n",
      "You can also slice lists just like you would a string.\n",
      "names[:2]\n",
      "### List Methods\n",
      "Lists also have a number of special methods that can be used with them, such as a method for adding items to a list, which is an extremely common one.\n",
      "| **List Method** | **Explanation**                                                                                   |\n",
      "|-------------|---------------------------------------------------------------------------------------------------|\n",
      "| `list.append(another_item)`          | adds new item to end of list                                                                                |\n",
      "| `list.extend(another_list)`        | adds items from another_list to list                                                |\n",
      "| `list.remove(item)`        | removes first instance of item                                                       |\n",
      "| `list.reverse()`       | reverses order of list                                                                                 |                                                     |\n",
      "###### Add Items To List\n",
      "names.append(\"Lawrence Feeney\")\n",
      "names\n",
      "###### Extend List With Another List\n",
      "names.extend(ages)\n",
      "names\n",
      "## For Loops\n",
      "One of the best ways to work with a list is with something called \"`for` loops.\" This is a way of considering each item in the list, also known as \"iterating\" through the list.\n",
      "names = ['Mary Gallagher', 'John Sanin(?)', 'Anthony Clark', 'Margaret Farrell']\n",
      "for name in names:\n",
      "    print(name)\n",
      "A `for` loop contains the English word `for` followed by a variable name for each item in the list that you want to consider (it could be anything!) followed by the English word in` followed by a colon `:`\n",
      "for name in names:\n",
      "    print(f\"Person's name is {name}\")\n",
      "for x in names:\n",
      "    print(f\"Person's name is {x}\")\n",
      "ages = [28, 19, 60, 30]\n",
      "for age in ages:\n",
      "    if age > 30:\n",
      "        print(\"Person is less than 30 years old\")\n",
      "    else:\n",
      "        print(\"Person is more than 30 years old\")\n",
      "## Group Exercise / HW Assignment 3 (Part II)\n",
      "###### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "###### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "###### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "###### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Make a list that contains each of the above Irish immigrants' professions and assign to a variable called `professions`\n",
      "##Your Code Here\n",
      "**2.** Extract the second item in the list `professions`. Hint: remember how the Python index works!\n",
      "##Your Code Here\n",
      "**3.** Add the item \"spinster\" to your `professions` list, then print the list.\n",
      "##Your Code Here\n",
      "##Your Code Here\n",
      "**4.** Make a `for` loop that considers each item in the `professions` list and prints \"Person's profession is ___\"\n",
      "##Your Code Here\n",
      "    ##Your Code Here\n",
      "**5.** Make a list that contains each of the above Irish immigrants' child statuses and assign to a variable called `child_status`. You can make Margaret Farrell's child status an empty string `''`.\n",
      "##Your Code Here\n",
      "**6.** Extract the third item in the list.\n",
      "##Your Code Here\n",
      "**7.** Make a `for` loop that considers each item in the `child_status` list and prints \"Person has child\" if the person has a child and \"Person does not have child\" if not\n",
      "##Your Code Here\n",
      "  ##Your Code Here\n",
      "       ##Your Code Here\n",
      "    ##Your Code Here\n",
      "        ##Your Code Here\n",
      "**8.** Make a list that contains each of the above Irish immigrants' genders and assign to a variable called `gender`\n",
      "##Your Code Here\n",
      "**9.** Add an item to the list called \"not known\"\n",
      "##Your Code Here\n",
      "**10.** Make a `for` loop that considers each item in the `gender` list and prints \"Person is male\" if the person is male, \"Person is female\" if the person is female, and \"Person's gender is not known\" if unknown\n",
      " ##Your Code Here\n",
      "     ##Your Code Here\n",
      "         ##Your Code Here\n",
      "     ##Your Code Here\n",
      "         ##Your Code Here\n",
      "     ##Your Code Here\n",
      "         ##Your Code Here\n",
      "## The Life and Anatomy of a Python Script\n",
      "A lot of Python tutorials begin with the nitty-gritty, but we're going to start from the abstract, from a zoomed-out perspective. We're going to take a look at the life and anatomy of a Python script.\n",
      "\n",
      "## Import libraries and modules\n",
      "import pandas as pd\n",
      "import csv\n",
      "import spacy\n",
      "from collections import Counter\n",
      "import re\n",
      "from spacy.lang.en.stop_words import STOP_WORDS\n",
      "## Define functions\n",
      "def split_into_words(full_text):\n",
      "    words = re.split(r\"\\W+\", full_text.lower())\n",
      "    return words \n",
      "## Read in files\n",
      "pride_prejudice_novel = \"Pride-and-Prejudice-Novel.txt\"\n",
      "pride_prejudice_screenplay = \"cornell-movie-corpus/Pride-and-Prejudice-Screenplay.txt\"\n",
      "## Where the Action Happens\n",
      "Specify the UTF-8 encoding. This is default on Mac/Linux, but not all Windows.\n",
      "with open(pride_prejudice_screenplay, encoding=\"utf-8\") as file:\n",
      "    \n",
      "        ## This block reads a file line by line.\n",
      "    for line in file:\n",
      "        line = line.rstrip()\n",
      "with open(pride_prejudice_screenplay, encoding=\"utf-8\") as file:\n",
      "    full_text = file.read()\n",
      "with open(pride_prejudice_novel, encoding=\"utf-8\") as file:\n",
      "    full_text_novel = file.read()\n",
      "full_text_novel\n",
      "full_text\n",
      "all_words = split_into_words(full_text)\n",
      "all_words\n",
      "all_words_novel = split_into_words(full_text_novel)\n",
      "all_words_novel\n",
      "all_words_frequency\n",
      "all_words_frequency = Counter(all_words)\n",
      "all_words_frequency\n",
      "\n",
      "words = [word for word in all_words_novel if word not in STOP_WORDS]\n",
      "full_text_lowercase = full_text.lower()\n",
      "full_text_lowercase\n",
      "all_words = full_text_lowercase.split()\n",
      "all_words\n",
      "all_words_frequency = Counter(all_words_novel)\n",
      "words_frequency = Counter(words)\n",
      "all_words_frequency.most_common(30)\n",
      "words_frequency.most_common(30)\n",
      "counter = Counter()\n",
      "all_words_frequency = Counter(all_words)\n",
      "all_words_frequency\n",
      "with open(pride_prejudice_screenplay, encoding=\"utf-8\") as file:\n",
      "    contents_lines = file.readlines()\n",
      "type(contents)\n",
      "type(contents_lines)\n",
      "contents.split?\n",
      "?contents.split\n",
      "contents.lower().split()\n",
      "contents_lines.split()\n",
      "with open(\"cornell-movie-corpus/movie_lines.txt\", encoding=\"latin-1\") as file:\n",
      "    lines = []\n",
      "    ## This block reads a file line by line.\n",
      "    for line in file:\n",
      "        lines.append(line.rstrip())\n",
      "\n",
      "lines\n",
      "movie_metadata = pd.read_csv('cornell-movie-corpus/movie_titles_metadata.csv', encoding='utf-8')\n",
      "movie_metadata.head()\n",
      "movie_metadata = movie_metadata.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
      "movie_metadata.info()\n",
      "movie_metadata['year'] = pd.to_datetime(movie_metadata['year'], yearfirst = True, errors = 'coerce')\n",
      "movie_metadata['year2'] = movie_metadata['year'].dt.year\n",
      "movie_metadata['year2'] = movie_metadata['year'].astype(int)\n",
      "movie_metadata.year.hist()\n",
      "\n",
      "## How to Use Jupyter\n",
      "## What is Jupyter?\n",
      "Jupyter is an interactive environment for writing code, which means that, instead of writing static code in a text editor, we can test out bits and pieces as we go.\n",
      "\n",
      "For example, one of the most basic Python functions is `print()`, which will display or \"print\" values to the screen. Here I can display my feelings about Jupyter as well as a quotation from a great Carly Rae Jepsen song:\n",
      "my_feelings_about_jupyter = \"I really,\\n really,\\n really,\\n really,\\n really,\\n really\\n like you\"\n",
      "print(my_feelings_about_jupyter)\n",
      "In Jupyter, I can also beautifully display and easily explore data, such as this CSV data from [Cornell's Movie Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html):\n",
      "import pandas as pd\n",
      "movie_data = pd.read_csv('../data/cornell-movie-corpus/movie_titles_metadata.csv', parse_dates=True)\n",
      "movie_data.head()\n",
      "movie_data.hist()\n",
      "## Jupyter \"notebook\"\n",
      "Jupyter \"notebooks\" are documents that can combine live code, explanatory text, images, and very pretty displays of data. This clutch combination makes them great for exploring data and really great for teaching and learning. *This* is a Jupyter notebook!\n",
      "## Launch JupyterLab\n",
      "> \"JupyterLab is the next generation of the Jupyter Notebook\"\n",
      "\n",
      "In this class, we're going to be using JupyterLab, which is \"the next generation of the Jupyter Notebook.\" JupyterLab runs classic Jupyter notebooks, but it also has a bigger and better user interface — as well as other improved features. \n",
      "You can launch JupyterLab from Anaconda Navigator:\n",
      "<img src=\"../images/installation/navigator-app.png\" width=100% , border=2>\n",
      "<img src=\"https://docs.anaconda.com/_images/nav-defaults.png\" widht=100%, border=2>\n",
      "Or, if Anaconda is added to your PATH, you can launch Jupyter Lab from the command line:\n",
      "%jupyter lab\n",
      "JupyterLab will open in a web browser, which is a little confusing. It's not actually connected to the internet. It's just running on a local server on your computer.\n",
      "\n",
      "IMPORTANT: If JupyterLab isn't running / the local server isn't running, then you can't open Jupyter notebook files (.ipynb)!\n",
      "## Make a New Jupyter Notebook\n",
      "To make a new notebook, select the Python 3 icon under \"Notebook.\"\n",
      "<img src=\"../images/make-new-Jupyter.png\" width=100%, border=2>\n",
      "## Create and Run a Cell\n",
      "Jupyter notebooks consist of \"cells,\" which can either contain code or [Markdown](https://www.markdownguide.org/getting-started/) text. Markdown is a way of writing plain text with formatting. \n",
      "For example, to make text *italics* or **bold**, *you put asterisks around it.*\n",
      "For example, to make text *italics* or **bold**, *you put **asterisks** around it.*\n",
      "* You can create a new cell by clicking the plus `+` sign in the toolbar or by pressing `Option` + `Return` (Mac) / `Alt` `Return` (Windows)\n",
      "* You can change the cell from \"Code\" to \"Markdown\" by clicking the drop down in the toolbar.\n",
      "* You can run the cell by cliking the play button ▶️ on the toolbar above or by typing `Shift` + `Return`.\n",
      "## Save Your Notebook\n",
      "If you want to save your notebook, press `Command ⌘` + `S` (Mac) / `Windows Key` + `S` (Windows).\n",
      "## More Lists & Loops, Plus Modules\n",
      "[Download relevant files here](https://melaniewalsh.org/More-Lists-Loops.zip)\n",
      "Last lesson, we learned how to make, manipulate, and iterate through lists, an important Python collection type. But we weren't actually working with a real CSV file, and we weren't doing a very comprehensive analysis of the data. In this lesson, we're going to work with a real CSV file and try to answer some analytical questions about the Bellevue Almshouse data, such as:\n",
      "\n",
      "- What is the most common \"disease\" and the least common \"disease\"?\n",
      "- What is the most common \"profession\" and the least common \"profession\"?\n",
      "- What is the gender breakdown of those admitted to the Bellevue Almshouse?\n",
      "\n",
      "We're going to answer these questions by practicing more with lists and loops while also introducing the csv module and the collections module.\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "## Reading in a CSV File\n",
      "The [csv module](https://docs.python.org/3/library/csv.html) allows you to read and write tabular data in CSV (comma separated values) format, one of the most common formats for spreadsheets. (Soon we're going to talk about the [Python library Pandas](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Pandas.html), an even more powerful and more convenient way of working with tabular data.)\n",
      "import csv\n",
      "To use the csv module, you have to first import it, as above. Then to read in a CSV file, as below, you need to `with open()` your desired CSV file `as` a csv object `:` then use the `csv.reader()` function and insert your csv object. The \"delimiter\" argument tells the computer how to read the CSV file. Sometimes you might have a CSV file that is separated by tabs (\\t) instead of commas (,) so it's typically good to specify.\n",
      "almshouse_filepath = '../data/bellevue_almshouse_modified.csv'\n",
      "\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "almshouse_data\n",
      "The `csv.reader()` function will create a \"reader object.\" To actually get at the data in there, we'll need to iterate through it in some way. Each row in the reader object is a list of strings, so if we iterate through every row in the dataset, we will get 9,000+ lists (!). It's helpful to remember what each row in the dataset represents and name your variables accordingly. For the Bellevue Almshouse dataset, each row represents a person.\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        print(person)\n",
      "**Pro tip!** If you have a really long output, you can [\"Enable Scrolling for Outputs\"](https://melaniewalsh.github.io/Intro-Cultural-Analytics/images/enable-scrolling.png) by right-clicking and selecting that option.\n",
      "If we wanted to answer our first question — *What is the most common \"disease\" and the least common \"disease\"?* — how might we isolate only the names of the diseases so we can count them? Think back to how [we indexed a list](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html#Index) in the last lesson...\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        print(person[0])\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        print(person[4])\n",
      "### Build a List With a `For` Loop\n",
      "Great! We figured out how to isolate the diseases. But to count them, we want to get them in a data collection of their own, like a list. How would we put this data into a list? Let's make an empty list and then append each disease from each row into the list.\n",
      "❌ ❌ ❌ **Not Correct**\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    for person in almshouse_data:\n",
      "        diseases = []\n",
      "        diseases.append(person[4])\n",
      "diseases\n",
      "Wait, that's not quite right. We only got a list with a single value. What's going on?\n",
      "The problem is that the list building is happening *inside* the `for` loop. This means that, `for` every person/row, the list is being re-written over and over again. \"destitution\" is the very last disease in the dataset, so we're only getting the very last value. To keep building on a list, we need to put the empty list *outside* of the `for` loop and then keep adding to it.\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    diseases = []\n",
      "    for person in almshouse_data:   \n",
      "        diseases.append(person[4])\n",
      "diseases\n",
      "## Measure Length of List\n",
      "To measure the length of a list, use the `len()` function.\n",
      "len(diseases)\n",
      "## Count Items In a List or Collection\n",
      "The Counter tool from the collections module is extremely useful. It can help you count all kinds of things. To use it, you first need to `import` the Counter `from` collections.\n",
      "from collections import Counter\n",
      "To count something, you simply need to insert it inside the `Counter()` function, like so:\n",
      "Counter(diseases)\n",
      "This will give you what's called a dictionary, which includes every disease in the dataset and how many times it appears. To sort this Counter dictionary based on the most common items, you can use the `.most_common()` method.\n",
      "disease_tally = Counter(diseases)\n",
      "disease_tally.most_common()\n",
      "You can also select a certain number of the most common items by placing a number inside the `.most_common()` method.\n",
      "disease_tally.most_common(10)\n",
      "You can also select a certain number of the *least* common items by extracting a slice from the end of list, like so:\n",
      "disease_tally.most_common()[-10:]\n",
      "disease_tally.most_common()[-3:]\n",
      "## Your Turn!\n",
      "By using the same methods, find the 10 most common professions and the 10 least common professions in the Bellevue Almshouse dataset.\n",
      "Build a list called `professions` by using a `for` loop and the `.append()` method.\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    #Your Code Here\n",
      "    for person in almshouse_data:   \n",
      "        #Your Code Here\n",
      "professions\n",
      "Count the list `professions` with the Counter tool then display the top 10 most common values.\n",
      "from collections import Counter\n",
      "\n",
      "professions_tally = #Your Code Here\n",
      "#Your Code Here\n",
      "Display the 10 least common values.\n",
      "#Your Code Here\n",
      "Now find out how many men vs women are included in the Bellevue Almshouse data. Build a list called `gender` with a `for` loop and the `.append()` method\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    #Your Code Here\n",
      "    for person in almshouse_data:   \n",
      "        #Your Code Here\n",
      "gender\n",
      "Count the values in the gender column wiht the Counter tool and then display the results.\n",
      "from collections import Counter\n",
      "\n",
      "gender_tally = #Your Code Here\n",
      "gender_tally\n",
      "## List Comprehensions\n",
      "There's a slightly easier and more compact way to build a list with a `for` loop called a \"list comprehension.\" Instead of creating an empty list, you can build the `for` loop inside of a list.\n",
      "<img src='../images/lists/list-comprehensions' width=100%, border=2>\n",
      "with open(almshouse_filepath) as csv_object:\n",
      "    almshouse_data = csv.reader(csv_object, delimiter=',')\n",
      "    \n",
      "    diseases = [person[4] for person in almshouse_data]\n",
      "diseases\n",
      "Remember our Python script for counting words in a text file? Though you probably didn't recognize it at the time, this code contains a list comprehension. Can you spot it?\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "This is the list comprehension:\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "which is exactly the same as\n",
      "meaningful_words = []\n",
      "for word in all_the_words:\n",
      "    if word not in nltk_stop_words:\n",
      "        meaningful_words.append(word)\n",
      "\n",
      "empty_string = []\n",
      "    for item in collection:\n",
      "        if item in items_we_want:\n",
      "            empty_string.append(item)\n",
      "empty_string = [item for item in collection if item in items_we_want]\n",
      "## Installation\n",
      "## Installing Anaconda\n",
      "We're going to install Python by installing something called Anaconda. Anaconda is a Python distribution that includes many other convenient packages, such as Jupyter Notebooks and JupyterLab. It also helps to keep your Python environment clean and well-managed.\n",
      "<img src=\"https://docs.anaconda.com/_images/win-install-options.png\" width=100% , border=2>\n",
      "\n",
      "<img src=\"../images/installation/navigator-app.png\" width=100% , border=2>\n",
      "<img src=\"https://docs.anaconda.com/_images/nav-defaults.png\" widht=100%, border=2>\n",
      "Then launch Jupyter Lab.\n",
      "\n",
      "If Anaconda is added to your PATH, you should be able to launch Jupyter Lab from the command line like so:\n",
      "%jupyter lab\n",
      "## Installing Atom\n",
      "Though we're mostly going to be programming in Jupyter notebooks, we're also going to install a text editor called Atom, where you can also write Python code.\n",
      "<img src=\"../images/Python-Atom.png\" width=100%, border=2>\n",
      "* Go to [Atom's home page](https://atom.io/), click download (it should know which OS you're using), and follow the instructions.\n",
      "\n",
      "* If you want to be able to launch Atom from the command line, as well, you can click \"Atom\" in the menu bar and then click \"Install Shell Commands.\"\n",
      "<img src=\"../images/installation/atom-shell-commands.png\" width=100%, border=2>\n",
      "## Conditionals and Comparisons\n",
      "[Download this file and Bellevue Almshouse data (not necessary but optional to explore)](https://melaniewalsh.org/Conditionals-Comparisons.zip)\n",
      "| **Comparison Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x == y `         | `True` if x is equal to y                                                                                |\n",
      "| `x != y `         | `True` if x is not equal to y                                               |\n",
      "| `x > y`       |  `True` if x is greater than y                                                        |\n",
      "| `x < y`       |   `True` if x is less than y  \n",
      "| `x >= y`       |   `True` if x is greater than or equal to y |\n",
      "| `x <= y`      | `True` if x is less than or equal to y`                                                                             |\n",
      "                                                                      |\n",
      "                                                            \n",
      "| **Logical Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x and y`         | `True` if x and y are both True                                                                             |\n",
      "| `x or y`         | `True` if either x or y is True                                              |\n",
      "| `not x`       |  `True` if is x is not True                                                       |\n",
      "                                                            \n",
      "Last lesson, we talked about the four Python data types, which included Booleans or True/False statements. We discussed how if we make a variable called `beyonce` and assign it the value \"Grammy award-winner\"...\n",
      "beyonce = \"Grammy award-winner\"\n",
      "...and then we evaluate that statement with the equals operator `==`...\n",
      "beyonce == \"Grammy award-winner\"\n",
      "type(beyonce == \"Grammy award-winner\")\n",
      "...the operator will return a Boolean, which will tell us whether the statement is True or False.\n",
      "\n",
      "Booleans may seem simple, but they're extremely powerful and function as the engine of a lot of code. They allow us to instruct the computer to do things based on different \"conditions.\"\n",
      "## If Statement\n",
      "An `if` statement is an instruction to do something *if* a particular condition is met.\n",
      "\n",
      "A common conditional will consist of two lines. On the first line, you will type the English word `if` followed by an evaluation `beyonce == \"Grammy award-winner\"` and then a colon `:`. On the second line, you will indent (press `Tab`), then write an instruction to be completed if the condition is met, such as `print(\"Congratulations, Beyonce!\")`\n",
      "beyonce = \"Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "Python is picky about how you format `if` statements. Look what happens if we forget to tab over on the second line or if we forget the colon:\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "print(\"Congratulations, Beyonce!\")\n",
      "if beyonce == \"Grammy award-winner\"\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "## Else Statement\n",
      "You can add even more complexity in a conditional by adding an `else` statement. This will instruct the program to do something in case the condition is not met. You put it after an `if` statement and format it the same way.\n",
      "beyonce = \"not a Grammy award-winner this year\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Elif Statement\n",
      "Sometimes you want even more nuance to respond to slightly different conditions. For example, if Beyonce was nominated for a Grammy but didn't win, then we might want to express a slightly different sentiment than if she won or was not nominated at all. You can add in this nuance with an `elif` state or else if statement, which will evaluate the first `if` statement and then *if* that statement is not True, it will evaluate the `elif` statement.\n",
      "beyonce = \"Grammy award-nominee\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "elif beyonce == \"Grammy award-nominee\":\n",
      "    print(\"Ok well at least they nominated you, Beyonce.\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Group Exercise / HW Assignment 3 (Part I)\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's [Bellevue Almshouse data](https://www.nyuirish.net/almshouse/the-almshouse-records/).\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "We're going to make a series of variables and assign them values based on the Bellevue Almshouse dataset. Make sure you run these cells.\n",
      "#### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "#### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "#### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "#### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Write an `if` statement that reports whether `person1_age` is less than 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "**2.** Write an `if` statement that reports whether `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "**3.** Write an `if` statement that reports whether `person1_age` is less than 30 years old *and* `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old and married.')\n",
      "**4.** Complicate your `if` statement from Question 1 by adding an `else` statement that prints \"Person is older than 30 years old\". Then evaluate whether `person3_age` is less than 30 years old. \n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**5.** Now evaluate whether `person4_age` is less than 30 years using the same code as Question 4.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "Hmmm, with the code as written, it's telling us that Margaret Farrell, who is 30 years old, is *more* than 30 years old. Add an `elif` statement that reports whether the person is exactly 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is exactly 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**6.** Write an `if` statement that will report whether `person1_child_status` includes children.\n",
      "#Your code here\n",
      "    print('Person has children.')\n",
      "**7.** Write one `if` statement that will accurately report whether `person1_child_status` includes children and, separately, if `person2_child_status` includes children. (Hint: think about how you might use the `!=` operator.)\n",
      "if person1_child_status #Your Code Here\n",
      "    print('Person has children.')\n",
      "if person2_child_status #Same Code Here\n",
      "    print('Person has children.')\n",
      "**8.** Write a conditional that will report whether `person1_profession` is \"married,\" \"laborer,\" \"widow,\" or \"unknown profession.\" Test your code by reassigning the variable as indicated below.\n",
      "person1_profession = 'married'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'laborer'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'widow'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'student'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "**9.** Some of the Irish immigrants' names have question marks after them. Let's clean up some of the data and remove the question marks. You can use the Python keyword `in` to test whether a string appears within another string. Print `person2_name` with the question mark and parentheses removed. (Hint: think about f-strings and string methods!)\n",
      "if \"(?)\" in person2_name:\n",
      "    #Your code here\n",
      "**10.** In a few sentences, write about your experiencing using and manipulating the Bellevue Almshouse data after reading Shrout's essay. How, if at all, did using this data influence your understanding of how Python works?\n",
      "**Your thoughts here**\n",
      "## What We're Not Covering\n",
      "The Python lessons in this course are intended to cover the most practical and most basic elements of Python that you will need to know to collect and analyze cultural data. There are a range of subjects that we are not covering in this course, which you may want to learn about, including but not limited to:\n",
      "## [Full Explanation of Object Oriented Programming](https://realpython.com/python3-object-oriented-programming/)\n",
      "## [Python Classes](https://docs.python.org/3/tutorial/classes.html)\n",
      "## [Handling Python Exception Errors](https://docs.python.org/3/tutorial/errors.html#handling-exceptions)\n",
      "## [While Loops](https://docs.python.org/3/tutorial/errors.html#handling-exceptions)\n",
      "## [Sets](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset)\n",
      "## Make Random Student Groups\n",
      "import random\n",
      "students = ['LeBron James',\n",
      "'Giannis Antetokounmpo',                \n",
      "'Kevin Durant',\n",
      "'Steph Curry',\n",
      "'Kyrie Irving',\n",
      "'Joel Embiid', \n",
      "'Kawhi Leonard', \n",
      "'Paul George', \n",
      "'James Harden', \n",
      "'Kemba Walker', \n",
      "'Khris Middleton', \n",
      "'Anthony Davis', \n",
      "'Nikola Jokić', \n",
      "'Klay Thompson', \n",
      "'Ben Simmons', \n",
      "'Damian Lillard', \n",
      "'Blake Griffin', \n",
      "'Russell Westbrook', \n",
      "'D\\'Angelo Russell', \n",
      "'LaMarcus Aldridge', \n",
      "'Nikola Vučević', \n",
      "'Karl-Anthony Towns', \n",
      "'Kyle Lowry', \n",
      "'Bradley Beal', \n",
      "'Dwyane Wade', \n",
      "'Dirk Nowitzki']\n",
      "def make_random_groups (students, number_of_groups):\n",
      "    \n",
      "    random.shuffle(students)\n",
      "    \n",
      "    all_groups = []\n",
      "    \n",
      "    for index in range(number_of_groups):\n",
      "        group = students[index::number_of_groups]\n",
      "        all_groups.append(group)\n",
      "    \n",
      "    for index, group in enumerate(all_groups):\n",
      "        formatted_group = ' / '.join(group)\n",
      "        \n",
      "        print(f\"\"\"✨Group {index+1}✨: {formatted_group}\n",
      "        \"\"\")\n",
      "    return\n",
      "make_random_groups(students, 10)\n",
      "## Make Random Student Groups Explained\n",
      "def make_random_groups (students, number_of_groups):\n",
      "    \n",
      "    #Shuffle the order of the students\n",
      "    random.shuffle(students)\n",
      "    \n",
      "    #Make an empty list where we will put all the student groups\n",
      "    all_groups = []\n",
      "    \n",
      "    for index in range(number_of_groups):\n",
      "        \n",
      "        \"\"\"\n",
      "        Extract a group of students from the list.\n",
      "        Start by extracting the student at spot # index in the list\n",
      "        then jump by number_of groups to extract the next student, and so on.\n",
      "        \n",
      "        Let's say we have 15 students and want 5 groups.\n",
      "        \n",
      "        For each number in 5 (1,2,3,4,5), we make a group by\n",
      "        taking the 1st person in the list, jumping by five to take the 6th person in the list,\n",
      "        then jumping by five to take the 11th person in the list.\n",
      "        \n",
      "        group = students[1::5]\n",
      "        \n",
      "        That's Group 1.\n",
      "        \n",
      "        Then we move on to take the 2nd, 7th, and 12th person.\n",
      "        \n",
      "        group = students[2::5]\n",
      "        \n",
      "        That's Group 2.\n",
      "        \n",
      "        Continue until Group 5!\n",
      "        \"\"\"\n",
      "        \n",
      "        group = students[index::number_of_groups]\n",
      "        \n",
      "        #Add this group to our all_groups master list\n",
      "        all_groups.append(group)\n",
      "    \n",
      "    for index, group in enumerate(all_groups):\n",
      "        \n",
      "        #Nicely format the groups by joining the student names together with a slash\n",
      "        formatted_group = ' / '.join(group)\n",
      "        \n",
      "        #Print out the final student groups!\n",
      "        print(f\"\"\"\n",
      "                ✨Group {index+1}✨:\n",
      "                        \n",
      "                {formatted_group}\n",
      "        \"\"\")\n",
      "    return\n",
      "## Common Python Errors\n",
      "Below are a few common error messages that you will likely encounter as you first learn Python and — to be perfectly honest — long afterward. No matter how much you know, you will always encounter errors!\n",
      "\n",
      "To learn more about these and other Python errors, see [Python's official documentation](https://docs.python.org/3/library/exceptions.html#bltin-exceptions).\n",
      "## SyntaxError\n",
      "A `SyntaxError` means that something has gone wrong with your Python syntax, aka the arrangement of words and punctuation in your code. Often, as below, this error will result from forgetting a closing quotation mark in a string or from forgetting a colon in a `for` loop. \n",
      "print(\"Hope this goes off without a hitch!)\n",
      "for item in items\n",
      "    print(item)\n",
      "The error message will often include a caret or arrow that points to the problematic part of the code:\n",
      "<img src=\"../images/Errors/SyntaxError.png\", border=2>\n",
      "## FileNotFound Error\n",
      "A `FileNotFoundError` means that whatever file name you've typed in cannot be located. Often, this error will result from simple typos in the file name or from not pointing to the correct directory which contains the file. Double check your spelling and where your desired file is located relative to your Python code.\n",
      "open('../Wrong-Directory/File-Name.txt').read()\n",
      "## TypeError\n",
      "A `TypeError` means that you're trying to perform a function or operation on something that is not the correct data type for that function or operation. For example, if you try to divide by a variable that is a string, rather than an integer or float (as in the example below), a `TypeError` will be thrown. Double check your data types with the `type()` function.\n",
      "favorite_artist = \"Beyoncé\"\n",
      "favorite_artist / 2\n",
      "type(favorite_artist)\n",
      "## NameError\n",
      "A `NameError` means that the variable name that you're using cannot be found. Often, this error results from forgetting to run the cell that defines your variable or from misspelling the name of your variable, as below. Check your spelling and make sure you've run all necessary cells.\n",
      "favorite_artist = \"Beyoncé\"\n",
      "favorite_arteest\n",
      "## AttributeError\n",
      "An `AttributeError` means that you're trying to access something from an object that that object doesn't possess or do something with an object that that object cannot do. For example, to transform the variable `favorite_artist` (which contains the string \"Beyoncé\") from title case to uppercase, we can run `favorite_arist.upper()` because `.upper()` is a built-in string method. But if we forget the name of that string method and instead type `.uppercase()`, which is not an existing string method, then an `AttributeError` will be raised.\n",
      "favorite_artist.upper()\n",
      "favorite_artist.uppercase()\n",
      "## Variables\n",
      "[Download relevant files for today's class here](https://melaniewalsh.org/Intro-CA-Notebooks-V2.zip)\n",
      "* Storage containers for data values, like little data gift boxes\n",
      "![](https://cdn.pixabay.com/photo/2016/09/14/20/48/birthday-1670415_960_720.png)\n",
      "**HEADS UP!**\n",
      "🚨 To run any of the code on this page, you need to run this cell first!!🚨\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "## Example Word Count Python Code\n",
      "\"\"\"\n",
      "Example Python code for\n",
      "calculating word frequency\n",
      "in a text file\n",
      "\"\"\"\n",
      "\n",
      "#Import Libraries and Modules\n",
      "\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "# Define Functions\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "# Define Filepaths and Assign Variables\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "# Read in File\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "# Manipulate and Analyze File\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "# Output Results\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Assigning Variables\n",
      "Variables are one of the fundamental building blocks of Python. A variable is like a tiny container where you store values and data — filenames, words, numbers, collections of words and numbers, etc. You can name variables almost anything you want (more on that below). The variable name will point to a value that you \"assign\" it. You might think about variable assignment like putting a value \"into\" the variable, as if the variable is a little box 🎁\n",
      "\n",
      "You assign variables with an equals `=` sign, which is slightly confusing. In Python, a single equals sign `=` is the \"assignment operator,\" and a double equals sign `==` is the \"real\" equals sign, e.g. `2 * 2 == 4`.\n",
      "Let's look at some of the variables that we used when we counted the most frequent words in Charlotte Perkins Gilman's \"The Yellow Wallpaper.\"\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "We made the variables:\n",
      "- `filepath_of_text` and assigned it `=` the location of our \"The Yellow Wallpaper\" text file (\"../texts/The-Yellow-Wallpaper.txt\")\n",
      "- `nltk_stop_words` and assigned it `=` the stopwords from the `nltk` library\n",
      "- `number_of_desired_words` and assigned it `=` `40` because we wanted the 40 most frequently occuring words\n",
      "- `full_text` and assigned it `=` the contents of \"The Yellow Wallpaper\" text file\n",
      "## `Print()` Vs Jupyter Display\n",
      "We can check to see what's \"inside\" these variables by running a cell with the variable's name. This is one of the handiest features of a Jupyter notebook. Outside the Jupyter environment, you would need to run `print(filepath_of_text)` to display the variable.\n",
      "filepath_of_text\n",
      "nltk_stop_words\n",
      "number_of_desired_words\n",
      "full_text\n",
      "Your turn! Pick another variable from the script above and see what's inside it below.\n",
      "#your_chosen_variable\n",
      "You can run the `print` function inside the Jupyter environment, too, which is sometimes useful because:\n",
      "\n",
      "- Jupyter will only display the last variable in a cell, but `print()` can display multiple variables\n",
      "- Jupyter will display text with `\\n` characters (which means \"new line\") but `print()` will display the text appropriately formatted with new lines\n",
      "\n",
      "filepath_of_text\n",
      "nltk_stop_words\n",
      "number_of_desired_words\n",
      "full_text\n",
      "Only the last variable in the cell above, `full_text`, is displayed with `\\n` characters. But if you `print()` each variable...\n",
      "print(filepath_of_text)\n",
      "print(nltk_stop_words)\n",
      "print(number_of_desired_words)\n",
      "print(full_text)\n",
      "...then each of the variables are displayed, plus the \"The Yellow Wallpaper\" is properly formatted with new lines.\n",
      " \n",
      "## Variable Names\n",
      "Though we named our variables `filepath_of_text`, `nltk_stop_words`,`number_of_desired_words`, and `full_text`, we could have named them almost anything else.\n",
      "\n",
      "Variable names can be as long or as short as you want, and they can include:\n",
      "- upper and lower-case letters (A-Z)\n",
      "- digits (0-9)\n",
      "- underscores (_)\n",
      "\n",
      "Variable names *cannot* include:\n",
      "- ❌ other punctuation (-.!?@)\n",
      "- ❌ spaces ( )\n",
      "- ❌ a reserved Python word\n",
      "Instead of `filepath_of_text`, we could have simply named the variable `filepath`.\n",
      "filepath = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "filepath = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "filepath\n",
      "Or we could have gone even simpler and named the filepath `f`.\n",
      "f = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "f\n",
      "### Striving for Good Variable Names\n",
      "As you start to code, you will almost certainly be tempted to use extremely short variables names like `f`.\n",
      "\n",
      "Your fingers will get tired, your coffee will wear off, you will see other people using variables like `f`, and you'll promise yourself that you'll definitely remember what `f` means. But you probably won't.\n",
      "\n",
      "Thus, you must resist the temptation of bad variable names. Clear and precisely-named variables will:\n",
      "\n",
      "1. Make your code more readable (both to yourself and others)\n",
      "2. Reinforce your understanding of Python and what's happening in the code\n",
      "3. Clarify and strengthen your thinking\n",
      "\n",
      "### Example Python Code ❌ **With Bad Variable Names** ❌\n",
      "For the sake of illustration, here's our same word count Python code with poorly named variables. The code works exactly the same as our original code and outputs the 40 most frequently occurring words in \"The Yellow Wallpaper\" — but it's *so much harder to read*!\n",
      "\n",
      "Imagine if you stumbled across this code for the first time and were trying to figure out how it works. Or imagine that you wrote this code two summers ago and were returning to it to do some updates. You'd have to spend a lot more time deciphering and decoding.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def sp(t):\n",
      "    lt = t.lower()\n",
      "    sw = re.split(\"\\W+\", lt)\n",
      "    return sw\n",
      "\n",
      "f = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "st = stopwords.words(\"english\")\n",
      "\n",
      "with open(f, encoding=\"utf-8\") as fo:\n",
      "    ft = fo.read()\n",
      "\n",
      "words = sp(ft)\n",
      "words = [w for w in words if w not in st]\n",
      "words = Counter(words)\n",
      "words = words.most_common(40)\n",
      "\n",
      "print(words)\n",
      "### Example Python Code ✨ **With Good Variable Names** ✨\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "### Off-Limit Names\n",
      "The only variable names that are off-limits are names that are reserved by, or built into, the Python programming language itself, such as `print`, `True`, and `list`. It's not something to worry too much about. You'll know very quickly if a name is reserved by Python because it will show up in green and often give you an error message.\n",
      "True = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "filepath-of-text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
      "## Re-Assigning Variables\n",
      "Variable assignment does not set a variable in stone. You can later re-assign the same variable a different value.\n",
      "For instance, I could re-assign `filepath_of_text` to the filepath for the lyrics of Beyonce's album *Lemonade* instead of Perkins-Gilman's \"The Yellow Wallpaper.\"\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "filepath_of_text\n",
      "If I change this one variable in our example code, then we get the most frequent words for *Lemonade*.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "### Your Turn\n",
      "Ok now it's your turn to insert a new file path and calculate a new word frequency! Take a look inside our `/texts` directory and see which texts you can choose from.\n",
      "(Do you remember how to look inside a directory and see what's there? Go back to [the command line lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Command-Line/The-Command-Line.htm) if you need a refresher. Hint: To look at the contents of a directory *inside* a directory, you can use the `-R` flag, short for \"recursive.\")\n",
      "!ls # your code here\n",
      "!ls -R #your code here\n",
      "Pick a file from the list above and assign `filepath_of_text` to its corresponding filepath below:\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = #Insert a New Text File Here\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "Now let's change the `number_of_desired_words` variable! Rename the variable and then chooose a value other than 40 and see what happens.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = #Insert a New Text File Here\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "#your_new_variable_name = #number\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(#your_new_variable_name)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "Bonus: how might you put the stopwords back in?\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = #Insert a New Text File Here\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "#your_new_variable_name = #number\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(#your_new_variable_name)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Programming in Python\n",
      "In this series of lessons, we will learn the basics of a programming language called Python, which we will continue to use throughout the semester.\n",
      "## Why Learn a Programming Language At All?\n",
      "* Save time and energy by automating tasks\n",
      "* Collect and analyze cultural data from a different perspective, often more comprehensively and at a larger scale\n",
      "* Gain more flexibility, customizability, and autonomy over your digital research projects\n",
      "* Better understand how data and programming languages shape contemporary society and culture \n",
      "## Why Python Specifically?\n",
      "* Readable (resembles English)\n",
      "* Relatively smooth learning curve\n",
      "* Broadly used and popular\n",
      "* Good for data science\n",
      "\n",
      "\n",
      "## Learning Goals with Python\n",
      "* Learn how to use Python as a tool and apply it to cultural questions\n",
      "* Learn how to read and think critically about Python code\n",
      "* Learn how to tinker with existing Python code for our own purposes\n",
      "* Learn how to better collaborate with computer scientists and software developers\n",
      "We are not, however, trying to:\n",
      "   ❌   Learn every technical nook and cranny of Python<br>\n",
      "   ❌   Learn how to become a software developer by the end of the course\n",
      "## Python Version 3\n",
      " The latest version of Python, which you should be using, is Python 3. [Python 2 has officially been retired](https://www.python.org/doc/sunset-python-2/). You should not use Python 2.\n",
      " \n",
      " It's possible that you may come across old Python 2 code on the internet, which won't work with Python 3. A telltale difference between Python 3 and Python 2 code is the syntax for the `print()` function. \n",
      "### Python 3\n",
      "In Python 3, the `print()` function requires parentheses...\n",
      "print(\"Good riddance, Python 2!\")\n",
      "### Python 2\n",
      "while Python 2 code does not...\n",
      "print \"Good riddance, Python 2!\"\"\n",
      "## Pandas\n",
      "[Download relevant files here](https://melaniewalsh.org/Pandas.zip)\n",
      "In this lesson, we're going to learn about [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html), a powerful Python library for working with tabular data like CSV files. Things that were kind of clunky to accomplish with the csv module are much easier to accomplish with Pandas.\n",
      "\n",
      "Here are some of the things you can do with Pandas:\n",
      "\n",
      "* Easily read and write CSV files\n",
      "* Sort and filter data\n",
      "* Analyze data\n",
      "* Combine datasets\n",
      "* Create data visualizations\n",
      "## Our Dataset\n",
      "<img src=\"../images/Slave-Voyages.png\" width=100%, border=2>\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson,\n",
      "<a href=\"https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death\">“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>\n",
      "The dataset that we're going to be working with in this lesson is taken from [The Trans-Atlantic Slave Trade Database](https://www.slavevoyages.org/voyage/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "\n",
      "We're working with this data for a number of reasons. First, the *Slave Voyages* project is a major data-driven contribution to the history of slavery and to the field of the digital humanities. Second, working with this data makes clear that, as Johnson writes, computation and data alone cannot capture \"the violent quandary\" of slavery or bring about justice. As we learn how to manipulate and analyze this dataset with Pandas, we should be reminded, at every step, of the challenges that are involved in using this data responsibly and intentionally.\n",
      "## Import Pandas\n",
      "To use the Pandas library, we first need to `import` it, as below. This `import` statement not only imports the library but also gives it a handy little nickname, `as` \"pd\" instead of \"pandas.\" You'll run across this alias convention a lot. It's a way to save time and not have to write the entire words \"pandas\" over and over again.\n",
      "import pandas as pd\n",
      "## Read in CSV File\n",
      "To read in a CSV file, you use the function `pd.read_csv()` and insert the name of your desired file path. You can also add the \"delimiter\" argument to specify the delimiter for your file.\n",
      "slave_voyages_df = pd.read_csv('../data/Slave-Voyages-Trans-Atlantic-North-America.csv', delimiter=\",\")\n",
      "**Pro tip!** If you use the `help()` function, you can see the documentation for almost any bit of code. If we run it on `pd.read_csv()`, we can see all the possible parameters that can be used with `pd.read_csv()`.\n",
      "help(pd.read_csv)\n",
      "When you read in a CSV file with Pandas, you transform it into a Pandas object called a dataframe.\n",
      "type(slave_voyages_df)\n",
      "## Display the Data\n",
      "In a Jupyter notebook, you can display a dataframe simply by running a cell with the name of the dataframe. The `NaN` value is the Pandas value for any missing data (`NaN` values have special properties that we'll get to later).\n",
      "slave_voyages_df\n",
      "You should always eyeball your dataset either by looking at the first few rows or by looking at a few random rows. To look at the first few rows, you can use a method called `.head()` (very similar to the `head` command on the command line).\n",
      "slave_voyages_df.head(10)\n",
      "To look at a random sample of rows, you can use the `.sample()` method. You might run this cell a few times to make sure everything looks ok.\n",
      "slave_voyages_df.sample(10)\n",
      "## Examine the Data\n",
      "You can check the dataframe's \"shape\" or how many rows vs columns it contains with `.shape` (number of rows, number of columns)\n",
      "slave_voyages_df.shape\n",
      "You can (and always should) check the data types that each column contains with `.dtypes`\n",
      "\n",
      "\n",
      "| **Pandas dtypes** |                                                                                    |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `object`         | string                                                                               |\n",
      "| `float64`         | float                                               |\n",
      "| `int64`       | integer                                                        |\n",
      "| `datetime64`       |  date time              \n",
      "slave_voyages_df.dtypes\n",
      "It's important to always check the data types in your dataframe. For example, sometimes numeric values, which you want to do mathematical calculations with, will accidentally be interpreted as a string object. To perform calculations on this data, you would need to first convert that column from a string to an integer.\n",
      "\n",
      "The data types in our Trans-Atlantic Slave Trade dataset look ok, except the column \"year_of_arrival,\" which is currently interpreted as an integer value when ideally it should be a datetime value.\n",
      "You can also check the column names of your dataframe with `.columns`\n",
      "slave_voyages_df.columns\n",
      "## Rename and Drop Columns\n",
      "Let's say we wanted to rename the \"flag\" column as \"national_affiliation.\" One way to approach this goal would be to make a new column identical to the \"flag\" column but with a different name.\n",
      "\n",
      "Making a new column with Pandas is very similar to making a new Python variable. You simply type the name of the dataframe with square brackets `[]` and include the name of your new desired column that doesn't exist yet. Then you assign it the value of the already existing flag column.\n",
      "slave_voyages_df['national_affiliation'] = slave_voyages_df['flag']\n",
      "If you scroll all the way to the end of the dataframe, you can see that we added a new column called \"national_affiliation\" that's exactly the same as the flag column.\n",
      "slave_voyages_df.head()\n",
      "But what if we don't want multiple columns with the same information? What if we just want to rename the \"flag\" column?\n",
      "First, let's get rid of the \"national_affiliation\" column so we can try again. To remove columns, you can use the `.drop()` method and the `columns=` parameter, followed by the column you'd like to drop.\n",
      "slave_voyages_df = slave_voyages_df.drop(columns='national_affiliation')\n",
      "slave_voyages_df.columns\n",
      "A better way to rename columns is with the `.rename()` method and the `columns=` parameter that includes the original name of the column followed by a colon and the new desired name of the column.\n",
      "slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "Renaming the \"flag\" column as we did above will only momentarily change that column's name. If we display our dataframe, we'll see that the column name has *not* changed permamently.\n",
      "slave_voyages_df.head(5)\n",
      "To save changes in your dataframe, you need to keep re-assigning your dataframe, as below.\n",
      "slave_voyages_df = slave_voyages_df.rename(columns={'flag': 'national_affiliation'})\n",
      "slave_voyages_df.head(5)\n",
      "## Select Columns\n",
      "To isolate a column with the csv module was kind of a pain. We had to loop through each row and index a certain value in each row's list. But with Pandas it's very simple to select a column. You can do so by typing the name of the dataframe accompanied by square brackets `[]` with the name of the column in quotation marks.\n",
      "slave_voyages_df['vessel_name']\n",
      "type(slave_voyages_df['vessel_name'])\n",
      "Each column in a dataframe is technically a Pandas object called a \"Series.\" We won't worry too much about the \"Series\" object right now, except to note that a Series object does not display as nicely as a DataFrame. To select a column as a dataframe, you can use two square brackets `[[]]`\n",
      "slave_voyages_df[['vessel_name']]\n",
      "type(slave_voyages_df[['vessel_name']])\n",
      "With two square brackets, you can also select and isolate multiple columns at the same time.\n",
      "slave_voyages_df[['vessel_name', 'national_affiliation', 'year_of_arrival']]\n",
      "## Filter Data\n",
      "You can also filter data based on different criteria. For example, let's say we wanted to look at the slave voyages whose national affiliation was American. You can evaluate each row in a column by isolating that column and then using the comparison operators that we discussed a few lessons ago, such as equals `==`.\n",
      "slave_voyages_df['national_affiliation'] == \"U.S.A.\"\n",
      "The output above is an evaluation of whether each row in the \"flag\" column equals \"U.S.A.\" or not. This function becomes much more helpful when we use this comparison as a selection criteria in its own right.\n",
      "\n",
      "Just as we can select a column by placing its name inside square brackets, we can select filtered data by placing a filter inside square brackets `[]`. Essentially, the line below is saying give me all the rows in the dataframe that match `slave_voyages_df['flag'] == \"U.S.A.\"`\n",
      "slave_voyages_df[slave_voyages_df['national_affiliation'] == \"U.S.A.\"]\n",
      "You can do the same thing with integers and other comparison operators. For example, if we wanted to filter for only the slave voyages that arrived after 1830, we could do so with `slave_voyages_df['year_of_arrival'] > 1830`\n",
      "slave_voyages_df['year_of_arrival'] > 1830\n",
      "slave_voyages_df[slave_voyages_df['year_of_arrival'] > 1830]\n",
      "## Sort Columns\n",
      "You can sort a dataframe with the `.sort_values()` method, inside of which you include the parameter `by=` and indicate the name of the column you want to sort by (written in quotation marks).\n",
      "slave_voyages_df.sort_values(by='total_disembarked')\n",
      "Take a look at the way the dataframe above is sorted. Do you notice that something seems a bit off?\n",
      "\n",
      "By default, Pandas will sort in \"ascending\" order, from the smallest value to the largest value. If you want to sort the largest values first, you need to include another parameter `ascending=False`.\n",
      "slave_voyages_df.sort_values(by='total_disembarked', ascending=False)\n",
      "If you want to sort a Series object, you don't need to use the `by=` paramter.\n",
      "slave_voyages_df['total_disembarked'].sort_values(ascending=False)\n",
      "## Count Values in Columns\n",
      "To count the unique values in a column, you can use the `.value_counts()` method.\n",
      "slave_voyages_df['national_affiliation'].value_counts()\n",
      "slave_voyages_df['vessel_name'].value_counts()\n",
      "slave_voyages_df['place_of_slave_disembarkation'].value_counts()\n",
      "## Calculate Columns\n",
      "| Pandas calculations | Explanation                         |\n",
      "|----------|-------------------------------------|\n",
      "| .count()    | Number of observations    |\n",
      "| .sum()      | Sum of values                       |\n",
      "| .mean()     | Mean of values                      |\n",
      "| .median()   | Median of values         |\n",
      "| .min()      | Minimum                             |\n",
      "| .max()      | Maximum                             |\n",
      "| .mode()     | Mode                                |\n",
      "| .std()      | Unbiased standard deviation         |\n",
      "\n",
      "\n",
      "You can do different calculations on columns with built-in Pandas functions, such as `.sum()` and `.max()\n",
      "slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum()\n",
      "slave_voyages_df['total_embarked'].sum() - slave_voyages_df['total_disembarked'].sum()\n",
      "slave_voyages_df['total_disembarked'].mean()\n",
      "slave_voyages_df['total_disembarked'].max()\n",
      "## Groupby Columns\n",
      "One powerful Pandas function is called `.groupby()`, which allows you to aggregate data and perform calculations on it. For example, if we wanted to see how many enslaved people were transported by each nation, we could group by, or aggregate our data based on, \"national_affiliation\". The first step to using groupby is to type the name of your dataframe followed by `.groupby()` with the column you'd like to aggregate based on. \n",
      "slave_voyages_df.groupby('national_affiliation')\n",
      "This action will created a \"groupby\" object, which you won't be able to access unless you perform a calculation on it, such as `.sum()`\n",
      "slave_voyages_df.groupby('national_affiliation').sum()\n",
      "If we want to isolate only the \"total_disembarked\" column from this groupby calculation, then we can add in \"total_disembarked\" in square brackets `[]`\n",
      "slave_voyages_df.groupby('national_affiliation')['total_disembarked'].sum().sort_values(ascending=False)\n",
      "slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "Let's save this groupby calculation into a new variable called `national_totals`\n",
      "national_totals = slave_voyages_df.groupby('national_affiliation')[['total_disembarked']].sum().sort_values(by='total_disembarked', ascending=False)\n",
      "## Make Plots \n",
      "We're going to talk about making plots and data visualizations more extensively later. But for now we're going to introduce the fact that you can make simple plots simply by using the [`.plot()` method](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html) that's built into Pandas.\n",
      "national_totals.plot()\n",
      "national_totals.plot(kind='bar')\n",
      "national_totals.plot(kind='barh')\n",
      "## Time Series\n",
      "slave_voyages_df['year_of_arrival'].dtypes\n",
      "Transform an integer into a datetime\n",
      "slave_voyages_df['year_of_arrival'] = pd.to_datetime(slave_voyages_df['year_of_arrival'], format='%Y',errors='coerce')\n",
      "Number of voyages over time\n",
      "slave_voyages_df['year_of_arrival'].value_counts().plot()\n",
      "Total number of enslaved people disembarked over time\n",
      "slave_voyages_df.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "usa_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='U.S.A.']\n",
      "usa_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "gb_slave_voyages = slave_voyages_df[slave_voyages_df['national_affiliation']=='Great Britain']\n",
      "gb_slave_voyages.groupby('year_of_arrival')['total_disembarked'].sum().plot()\n",
      "## Dictionaries\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data.head(20)\n",
      "## Dictionary\n",
      "When we used lists with the Bellevue Almshouse data, it was a lot more convenient than individually assigning individual variables. We could put multiples names, ages, or professions into a single list.\n",
      "**Indivudal Variables**\n",
      "person1_name = 'Mary Gallagher'\n",
      "person2_name = 'John Sanin (?)'\n",
      "person1_age = 18\n",
      "person2_age = 19\n",
      "vs\n",
      "**Lists**\n",
      "names = ['Mary Gallagher', 'John Sanin(?)', 'Anthony Clark', 'Margaret Farrell']\n",
      "ages = [28, 19, 60, 30]\n",
      "professions = ['married', 'laborer', 'laborer', 'widow']\n",
      "But it would be nice if we could somehow group Mary Gallagher's name, age, and profession into a single Python data collection, since it all corresponds to a single person. Luckily we can with something called a \"dictionary\"!\n",
      "person_1 = {\"name\": \"Mary Gallagher\",\n",
      "             \"age\": 28,\n",
      "             \"profession\": \"married\"}\n",
      "type(person_1)\n",
      "A dictionary is made up of \"key\"-\"value\" pairs, which are separated by a colon `:` and separated from other key-value pairs by a comma `,`. A dictionary is always enclosed by curly brackets `{}`. \n",
      "You can check all the keys in a dictionary by using the `.keys()` method or all the values in a dictionary by using the `.values()` method.\n",
      "person_1.keys()\n",
      "person_1.values()\n",
      "## Access Items\n",
      "You can access a value in a dictionary by using square brackets `[]` and its key name (kind of like how we indexed a string or a list).\n",
      "person_1[\"name\"]\n",
      "person_1[\"age\"]\n",
      "person_1[\"profession\"]\n",
      "## Change Item\n",
      "You can change a value in a dictionary by re-assigning a new value to a dictionary key.\n",
      "person_1[\"age\"] = 100\n",
      "person_1\n",
      "person_1['profession'] = 'spinster'\n",
      "person_1\n",
      "## Nested Dictionary\n",
      "You can also nest a dictionary inside another dictionary.\n",
      "bellevue_people = {\n",
      "                \"person_1\":\n",
      "                  {\"name\": \"Mary Gallagher\",\n",
      "                   \"age\": 28,\n",
      "                   \"profession\": \"married\"},\n",
      "                \"person_2\":\n",
      "                  {\"name\": \"John Sanin(?)\",\n",
      "                   \"age\": 19,\n",
      "                   \"profession\": \"laborer\"}\n",
      "                }\n",
      "bellevue_people['person_1']\n",
      "bellevue_people['person_1']['name']\n",
      "bellevue_people['person_2']\n",
      "bellevue_people['person_2']['age']\n",
      "## Iterate Through Dictionary\n",
      "for person in bellevue_people.keys():\n",
      "    print(person)\n",
      "for person in bellevue_people.values():\n",
      "    print(person)\n",
      "for person in bellevue_people.values():\n",
      "    if person['age'] > 20:\n",
      "        name = person['name']\n",
      "        age = person['age']\n",
      "        print(f'{name} is more than 20 years old. She is {age}.')\n",
      "for person in bellevue_people.items():\n",
      "    print(person)\n",
      "## String Methods\n",
      "Here are some special things you can do with \"strings\" in Python. Strings are one of four basic Python data types. They're typically words, and they're always surrounded by quotation marks.\n",
      "\"this is a string\"\n",
      "'this is a string'\n",
      "this is not a string\n",
      "| **String Method** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `string.lower()`         | makes the string lowercase                                                                                |\n",
      "| `string.upper()`         | makes the string uppercase  \n",
      "| `string.title()`         | makes the string titlecase \n",
      "| `string.strip()`         | removes lead and trailing white spaces     |\n",
      "| `string.replace('old string', 'new string')`      | replaces `old string` with `new string`          |\n",
      "| `s.split('delim')`          | returns a list of substrings separated by the given delimiter |\n",
      "| `s.join(list)`         | opposite of split(), joins the elements in the given list together using the string                                                                        |\n",
      "| `string.startswith('some string')`       | tests whether string begins with `some string` |                                                       |\n",
      "| `string.endswith('some string')`       |  tests whether string ends with `some string`   |\n",
      "| `string.isspace()`       |  tests whether string is a space |\n",
      "\n",
      "                                                            \n",
      "## Practice with Strings!\n",
      "We're going to practice with Franz Kafka's 1915 novella, *The Metamorphosis.*\n",
      "sample_text = open(\"../texts/literature/Kafka-The-Metamorphosis.txt\", encoding=\"utf-8\").read()\n",
      "print(sample_text)\n",
      "## Extract Parts of Strings\n",
      "### Index\n",
      "By using square brackets `[]`, you can \"index\"—or grab—part of a string based on its character number. The very first words of the novella are \"One morning.\" Here's what happens if we grab the first character.\n",
      "(Remember that the zero-th place in a Python index is actually the very first place.)\n",
      "sample_text[0]\n",
      "sample_text[1]\n",
      "### Slice\n",
      "By using a colon `:`, we can slice a string up to a certain character. Let's index our Kafka sample text up to the 121st character.\n",
      "sample_text[:121]\n",
      "Great! That's the first sentence. Let's assign it to the variable `first_line`.\n",
      "first_line = sample_text[:121]\n",
      "first_line\n",
      "## Replace Words\n",
      "first_line.replace(\"morning\", \"evening\")\n",
      "Your turn! Replace the word \"vermin\" with a word of your choosing:\n",
      "first_line.replace(\"vermin\", #your code here )\n",
      "Your turn! Replace the words \"Gregor Samsa\" with another name:\n",
      "# your code here\n",
      "## Transform Strings to Lowercase/Uppercase\n",
      "(\"I am really very quiet\").lower()\n",
      "(\"I am really very quiet\").upper()\n",
      "Your turn! Transform the first line of Kafka's *The Metamorphosis* to lower case:\n",
      "# your code here\n",
      "Your turn! Transform the first line of Kafka's *The Metamorphosis* to upper case:\n",
      "# your code here\n",
      "## Split Strings By a Delimiter\n",
      "With the `.split()` method, you can split up a strings into a a list of parts. By default, it splits on spaces, but you can put in a different delimiter and split on something else. \n",
      "first_line.split()\n",
      "Your turn! Split on the words \"dreams\" and see what happens.\n",
      "# your code here\n",
      "## Join Strings By a Delimiter\n",
      "You can also put something back together again with the `join()` method!\n",
      "kafka_split_words = first_line.split()\n",
      "kafka_split_words\n",
      "\"SPACE\".join(kafka_split_words)\n",
      "\" \".join(kafka_split_words)\n",
      "Your turn! Join `kafka_split_words` with a delimiter of your choosing.\n",
      "# your code here\n",
      "## Data Types\n",
      "[Download necessary Jupyter notebooks and text files here](https://melaniewalsh.org/Intro-CA-Notebooks-V2.zip)\n",
      "* 4 essential kinds of Python data with different powers and capabilities, like starter pack Pokémon    \n",
      "    - Strings (Words)\n",
      "    - Integers (Whole Numbers)\n",
      "    - Floats (Decimal Numbers)\n",
      "    - Booleans (True/False)\n",
      "<img src=\"https://hips.hearstapps.com/digitalspyuk.cdnds.net/16/08/1456483171-pokemon2.jpg?resize=768:*\", border=2>\n",
      "**HEADS UP!**\n",
      "🚨 To run any of the code on this page, you need to run this cell first!!🚨\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "most_frequent_meaningful_words\n",
      "You might be wondering why we put quotation marks around `\"../texts/Beyonce-Lemonade.txt\"` and not around `40`,  or why the file path shows up in red and 40 shows up in green.\n",
      "filepath_of_text = \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "number_of_desired_words = 40\n",
      "That's because these are two different \"types\" of Python data. The file path is what's called a \"string,\" or words, and 40 is an \"integer,\" or a whole number.\n",
      "\n",
      "In Python, there are four basic data types:\n",
      "\n",
      "- Strings (Words)\n",
      "- Integers (Whole Numbers)\n",
      "- Floats (Decimal Numbers)\n",
      "- Booleans (True/False)\n",
      "\n",
      "Each data type has different properties and different capabilities. You can check a data type if you use the function `type()`. As you may have noticed, functions use parentheses, and they do something to the thing inside the parentheses, what we call an \"argument.\"\n",
      "\n",
      "type(filepath_of_text)\n",
      "type(number_of_desired_words)\n",
      "Let's look at what happens if we change the data types of `filepath_of_text` and `number_of_desired_words`\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = ../texts/music/Beyonce-Lemonade.txt\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = 40\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = ../texts/music/Beyonce-Lemonade.txt\n",
      "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
      "number_of_desired_words = \"40\"\n",
      "\n",
      "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "meaningful_words_tally = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Strings (Words)\n",
      "- Enclosed by either single or double quotation marks (doesn't matter which but you have to be consistent)\n",
      "- Ability to combine strings with `+`\n",
      "- Ability to manipulate in special ways (make lowercase or uppercase, replace parts, grab slices, etc.)\n",
      "full_text\n",
      "**Heads up!**\n",
      "The `\\n` above means \"new line.\" \n",
      "type(full_text)\n",
      "## Extract Parts of Strings\n",
      "### Index\n",
      "By using square brackets `[]`, you can \"index\"—or grab—part of a string based on its character number. The first line of Beyonce's album Lemonade is:\n",
      "> Six inch heels, she walked in the club like nobody's business.\n",
      "\n",
      "If we index the very first character of the album, which character do you think we'll get?\n",
      "full_text[1]\n",
      "full_text[0]\n",
      "This is one weird thing about Python that you're just going to have to commit to memory. The zero-th place in a Python index is actually the very first place. Its number system starts with zero.\n",
      "### Slice\n",
      "By using a colon `:`, we can index a string up to a certain character.\n",
      "full_text[:61]\n",
      "lemonade_first_line = full_text[:61]\n",
      "## Add Strings Together\n",
      "lemonade_first_line + \", and then she took off her shoes because they were pretty tall\"\n",
      "## Lots of Handy String Methods!\n",
      "For more, see [String Methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html)\n",
      "lemonade_first_line.upper()\n",
      "A method is like a function, except that it follows the thing it's going to act upon, and it doesn't necessarily need an \"argument\" in the parentheses.\n",
      "## Put Variables Inside Strings\n",
      "You can also insert variables into a string with something called **f-strings**! They're amazing. An f-string must begin with an `f` outside the quotation marks, and then the variable must be be placed within curly brackets `{}`, like so:\n",
      "print(f\"Beyonce stepped on stage and sang: \\n\\n'{lemonade_first_line}'\")\n",
      "### Your Turn\n",
      "Remix! Make a variable called `new_first_line` and assign it the value of `lemonade_first_line` plus `+` your new remixed ending `new_first_line`. Then print it.\n",
      "new_first_line = #Your Code Here\n",
      "print(f\"Beyonce stepped on stage and sang: \\n\\n'{#Your Code Here}'\")\n",
      "## Integers & Floats (Numbers)\n",
      "- You can do math with them!\n",
      "number_of_desired_words = 40\n",
      "type(number_of_desired_words)\n",
      "number_of_desired_words + 57\n",
      "new_number = 100005\n",
      "number_of_desired_words * new_number\n",
      "number_of_desired_words= 40.5\n",
      "type(number_of_desired_words)\n",
      "type(40.555555555555)\n",
      "## Multiplication\n",
      "4 * 2\n",
      "## Exponents\n",
      "4 ** 2\n",
      "## Order of Operations\n",
      "4 + 2 * 2\n",
      "(4 + 2) * 2\n",
      "## Booleans (True/False)\n",
      "Booleans are like little judgments. They report on whether things in your Python universe are `True` or `False`.\n",
      "beyonce = \"Grammy award-winner\"\n",
      "beyonce == \"Grammy award-winner\"\n",
      "beyonce == \"Oscar award-winner\"\n",
      "beyonce == \"Grammy award winner\"\n",
      "type(beyonce == \"Oscar award-winner\")\n",
      "## Type Conversion\n",
      "number_of_desired_words = 40\n",
      "number_of_desired_words\n",
      "type(number_of_desired_words)\n",
      "str(number_of_desired_words)\n",
      "converted_num_desired_words = str(number_of_desired_words)\n",
      "type(converted_num_desired_words)\n",
      "int(lemonade_first_line)\n",
      "## In-Class Exercise\n",
      "name = 'Prof. Walsh' #string\n",
      "age = 1000 #integer\n",
      "place = 'Chicago' #string \n",
      "favorite_food = 'tacos' #string\n",
      "dog_years_age = age * 7.5 #float\n",
      "student = False #boolean\n",
      "print(f'✨This is...{name}!✨')\n",
      "\n",
      "print(f'{name} likes {favorite_food} and once lived in {place}. {name} is {age} years old, which is {dog_years_age} in dog years. The statement \"{name} is a student\" is {student}.')\n",
      "## Your turn!\n",
      "\n",
      "name = #Your code here\n",
      "age = #Your code here\n",
      "home_town = #Your code here\n",
      "favorite_food = #Your code here\n",
      "dog_years_age =#Your code here * 7.5\n",
      "student = False #boolean\n",
      "print(f'✨This is...{name}!✨')\n",
      "\n",
      "print(f'{name} likes {favorite_food} and once lived in {place}. {name} is {age} years old, which is {dog_years_age} in dog years. The statement \"{name} is a student\" is {student}.')\n",
      "Now add a new variable called `favorite_movie` and update the f-string to include a new sentence about the person's favorite movie.\n",
      "name = \n",
      "age = \n",
      "home_town = \n",
      "favorite_food = \n",
      "dog_years_age =\n",
      "#favorite_movie = \n",
      "print(f'✨This is...{name}!✨')\n",
      "\n",
      "print(f'{name} likes {favorite_food} and once lived in {place}. {name} is {age} years old, which is {dog_years_age} in dog years. The statement \"{name} is a student\" is {student}. # YOUR NEW SENTENCE HERE')\n",
      "## Conditionals and Comparisons — Answer Key\n",
      "## Group Exercise / HW Assignment 3 (Part I)\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's [Bellevue Almshouse data](https://www.nyuirish.net/almshouse/the-almshouse-records/). Below I'm displaying the dataset with the Python library \"Pandas\" just so we can get a peek at it. We won't actually be working directly with this CSV file at the moment, and we'll talk about Pandas more in a few lessons.\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data.head(20)\n",
      "We're going to make a series of variables and assign them values based on the Bellevue Almshouse dataset. Make sure you run these cells.\n",
      "### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "1. Write an `if` statement that reports whether `person1_age` is less than 30 years old.\n",
      "if person1_age < 30:\n",
      "    print('Person is less than 30 years old.')\n",
      "2. Write an `if` statement that reports whether `person1_profession` is \"married.\"\n",
      "if person1_profession == \"married\":\n",
      "    print('Person is married.')\n",
      "3. Write an `if` statement that reports whether `person1_age` is less than 30 years old *and* `person1_profession` is \"married.\"\n",
      "if person1_age < 30 and person1_profession == \"married\":\n",
      "    print('Person is less than 30 years old and married.')\n",
      "4. Complicate your `if` statement from Question 1 by adding an `else` statement that prints \"Person is older than 30 years old\". Then evaluate whether `person3_age` is less than 30 years old. \n",
      "if person3_age < 30:\n",
      "    print('Person is less than 30 years old.')\n",
      "else:\n",
      "    print('Person is more than 30 years old.')\n",
      "5. Now evaluate whether `person4_age` is less than 30 years using the same code as Question 4.\n",
      "if person4_age < 30:\n",
      "    print('Person is less than 30 years old')\n",
      "else:\n",
      "    print('Person is more than 30 years old.')\n",
      "Hmmm, with the code as written, it's telling us that Margaret Farrell, who is 30 years old, is *more* than 30 years old. Add an `elif` statement that reports whether the person is exactly 30 years old.\n",
      "if person4_age < 30:\n",
      "    print('Person is less than 30 years old.')\n",
      "elif person4_age == 30:\n",
      "    print('Person is exactly 30 years old.')\n",
      "else:\n",
      "    print('Person is more than 30 years old.')\n",
      "6. Write an `if` statement that will report whether `person1_child_status` includes children.\n",
      "if person1_child_status != '':\n",
      "    print('Person has children.')\n",
      "7. Write one `if` statement that will accurately report whether `person1_child_status` includes children and, separately, if `person2_child_status` includes children. (Hint: think about how you might use the `!=` operator.)\n",
      "if person1_child_status != '':\n",
      "    print('Person has children.')\n",
      "if person2_child_status != '':\n",
      "    print('Person has children.')\n",
      "8. Write a conditional that will report whether `person1_profession` is \"married,\" \"laborer,\" \"widow,\" or \"unknown profession.\" Test your code by reassigning the variable as indicated below.\n",
      "person1_profession = 'married'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'laborer'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'widow'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'student'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "9. Some of the Irish immigrants' names have question marks after them. Let's clean up some of the data and remove the question marks. You can use the Python keyword `in` to test whether a string appears within another string. Print `person2_name` with the question mark and parentheses removed. (Hint: think about f-strings and string methods!)\n",
      "if \"(?)\" in person2_name:\n",
      "    print(f\"{person2_name}\".replace(\"(?)\", \"\"))\n",
      "10. In a few sentences, write about your experiencing using and manipulating the Bellevue Almshouse data after reading Shrout's essay. How, if at all, did using this data influence your understanding of how Python works?\n",
      "#Your thoughts here\n",
      "## Comparison and Conditionals\n",
      "Last lesson, we talked about the four Python data types, including Booleans or True/False statements. We discussed how if we make a variable called `beyonce` and assign it the value \"Grammy award-winner\"...\n",
      "beyonce = \"Grammy award-winner\"\n",
      "...and then we evaluate that statement with the equals opeartor `==`...\n",
      "beyonce == \"Grammy award-winner\"\n",
      "...the operator will return a Boolean. That is to say, it will tell us whether the statement is True or False.\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "beyonce = \"not a Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "| **Comparison Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x == y `         | True if x is equal to y                                                                                |\n",
      "| `x != y `         | True if x is not equal to y                                               |\n",
      "| `x > y  `       |  True if is greater than y                                                        |\n",
      "| `string.endswith('some string')`       |  tests whether string ends with `some string`   \n",
      "| `string.isspace()`       |  tests whether string is a space |\n",
      "| `string.replace('old string', 'new string')`      | replaces `old string` with `new string`                                                                             |\n",
      "| `s.split('delim')`          | returns a list of substrings separated by the given delimiter |\n",
      "| `s.join(list)`         | opposite of split(), joins the elements in the given list together using the string                                                                        |\n",
      "                                                            \n",
      "              # Produce \n",
      "              # ... x is not equal to y\n",
      "x > y                # ... x is greater than y\n",
      "x < y                # ... x is less than y\n",
      "x >= y               # ... x is greater than or equal to y\n",
      "x <= y               # ... x is less than or equal to y\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data\n",
      "print(quiz_greeting_prompt)\n",
      "favorite_author = input(quiz_greeting_prompt)\n",
      "\n",
      "if favorite_author == 'Stephen King':\n",
      "    print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "    \n",
      "elif favorite_author == 'Virginia Woolf':\n",
      "    print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "    \n",
      "elif favorite_author == 'James Baldwin':\n",
      "    print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "    \n",
      "elif favorite_author == 'Stephenie Meyer':\n",
      "    print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "else:\n",
      "    print(f\"You said {favorite_author}. I don't know her\")\n",
      "\n",
      "def author_quiz():\n",
      "    \n",
      "    favorite_author = input('Insert name of immigrant')\n",
      "\n",
      "    if favorite_author == 'Mary Gallagher':\n",
      "        print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "\n",
      "    elif favorite_author == 'Virginia Woolf':\n",
      "        print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "\n",
      "    elif favorite_author == 'James Baldwin':\n",
      "        print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "\n",
      "    elif favorite_author == 'Stephenie Meyer':\n",
      "        print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "    else:\n",
      "        print(\"I don't know her\")\n",
      "def make_lowercase(some_chunk_of_text):\n",
      "    #Your line here\n",
      "    return lowercase_chunk\n",
      "def make_lowercase(some_chunk_of_text):\n",
      "    #Your line here\n",
      "    return lowercase_chunk\n",
      "author_quiz()\n",
      "favorite_author = input('Insert name of immigrant')\n",
      "\n",
      "if favorite_author == 'Mary Gallagher':\n",
      "    print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "    \n",
      "elif favorite_author == 'Virginia Woolf':\n",
      "    print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "    \n",
      "elif favorite_author == 'James Baldwin':\n",
      "    print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "    \n",
      "elif favorite_author == 'Stephenie Meyer':\n",
      "    print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "else:\n",
      "    print(\"I don't know her\")\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_gender = 'm'\n",
      "\n",
      "person1 = 'Mary Gallagher'\n",
      "if person1_gender == 'f':\n",
      "    print('According to the Bellevue gender is female')\n",
      "else:\n",
      "    print('Male')\n",
      "person1_last_name = '   Sanin (?)   '\n",
      "person1_last_name.strip()\n",
      "person1_last_name.replace(\"(?)\", \"\")\n",
      "person_1_children == \n",
      "if \n",
      "if person1_gender == 'f':\n",
      "    print()\n",
      "\n",
      "\"Greetings! This is a Buzzfeed-style quiz that will tell you where you'd like to live based on your favorite author.\n",
      "\n",
      "Please enter your favorite author from the following: James Joyce, Virginia Woolf, Ernest Hemingway, J.K. Rowling, Stephen King, Stephenie Meyer.\"\n",
      "%run Buzzfeed_Author_Quiz.py\n",
      "favorite_author\n",
      "if favorite_author == 'Stephen King':\n",
      "    print(\"A creepy hotel\")\n",
      "Can we guess your favorite [blank] absed on your age?\n",
      "Use a greater than sign, use a less than or equal to sign, \n",
      "favorite_author = input(\"What's your age?\")\n",
      "\n",
      "if int(favorite_author) >  16:\n",
      "    print(\"You'd probably like to live in: a creepy hotel\")\n",
      "    \n",
      "if favorite_author == 'Virginia Woolf':\n",
      "    print(\"You'd probably like to live in: a room of your own in London\")\n",
      "    \n",
      "if favorite_author == 'James Joyce':\n",
      "    print(\"You'd probably like to live in: a pub in Dublin\")\n",
      "    \n",
      "if favorite_author == 'Stephenie Meyer':\n",
      "    print(\"You'd probably like to live in: a rainy town in Washington with mysterious new neighbors\")\n",
      "\n",
      "else:\n",
      "    print(\"I don't know her\")\n",
      "## Comparisons and Conditionals\n",
      "| **Comparison Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x == y `         | `True` if x is equal to y                                                                                |\n",
      "| `x != y `         | `True` if x is not equal to y                                               |\n",
      "| `x > y`       |  `True` if is greater than y                                                        |\n",
      "| `x < y`       |   `True` if x is less than y  \n",
      "| `x >= y`       |   `True` if x is greater than or equal to y |\n",
      "| `x <= y`      | `True` if x is less than or equal to y`                                                                             |\n",
      "                                                                      |\n",
      "                                                            \n",
      "| **Logical Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x and y`         | `True` if x and y are both True                                                                             |\n",
      "| `x or y`         | `True` if either x or y is True                                              |\n",
      "| `not x`       |  `True` if is x is not True                                                       |\n",
      "                                                            \n",
      "Last lesson, we talked about the four Python data types, which included Booleans or True/False statements. We discussed how if we make a variable called `beyonce` and assign it the value \"Grammy award-winner\"...\n",
      "beyonce = \"Grammy award-winner \"\n",
      "...and then we evaluate that statement with the equals operator `==`...\n",
      "beyonce == \"Grammy award-winner\"\n",
      "type(beyonce == \"Grammy award-winner\")\n",
      "...the operator will return a Boolean, which will tell us whether the statement is True or False.\n",
      "\n",
      "Booleans may seem simple, but they're extremely powerful and function as the engine of a lot of code. They allow us to program the computer to do things based on different \"conditions.\"\n",
      "## If Statement\n",
      "An `if` statement is an instruction to do something *if* a particular condition is True. Both parts of this equation together are called a conditional.\n",
      "\n",
      "A common and simple conditional will consist of two lines. On the first line, you will type the English word `if` followed by an evaluation `beyonce == \"Grammy award-winner\"` and then a colon `:`. On the second line, you will indent (or hit `tab`), then write an instruction to be completed if the condition is met, such as `print(\"Congratulations, Beyonce!\")`\n",
      "beyonce = \"Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "Python is picky about how you format `if` statements. Look what happens if I forget to tab over on the second line or if I forget the colon:\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "print(\"Congratulations, Beyonce!\")\n",
      "if beyonce == \"Grammy award-winner\"\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "## Else Statement\n",
      "You can add even more complexity in a conditional by adding an `else` statement. This will instruct the computer program to do something in case the condition is not met. You format it the same way you format an `if` statement, directly following the `if` statement.\n",
      "beyonce = \"not a Grammy award-winner this year\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Elif Statement\n",
      "Sometimes you want even more nuance to respond to slightly different conditions. For example, if Beyonce was nominated for a Grammy but didn't win, then we might want to express a slightly different sentiment than if she was nominated at all. You can add in this nuance with an `elif` state or else if statement, which will evaluate the first `if` statement first and then *if* that statement is not True, it will evaluate the `elif` statement.\n",
      "beyonce = \"Grammy award-nominee\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "elif beyonce == \"Grammy award-nominee\":\n",
      "    print(\"Ok well at least they nominated you, Beyonce.\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Group Exercise / HW Assignment 3\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's Bellevue Almshouse data. Below I'm displaying the dataset with the Python library \"Pandas\" just so we can get a peek at it. We won't actually be working directly with this CSV file at the moment, and we'll talk about Pandas more in a few lessons.\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data.head(20)\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "1. Write an `if` statement that reports whether person1 is younger than 30 years old.\n",
      "#Your Code Here\n",
      "    print('Person is older than 30')\n",
      "if person1_age < 30:\n",
      "    print('Person is younger than 30 years old')\n",
      "else:\n",
      "    print('Person is younger than 30')\n",
      "if person1_age > 30:\n",
      "    print('Person is older than 30')\n",
      "else:\n",
      "    print('Person is younger than 30')\n",
      "person1 = 'Mary Gallagher'\n",
      "if person1_gender == 'f':\n",
      "    print('According to the Bellevue gender is female')\n",
      "else:\n",
      "    print('Male')\n",
      "if \"Child\" in person1_child_status:\n",
      "    print('Person has children')\n",
      "else:\n",
      "    print('Person does not have children')\n",
      "person1_last_name = '   Sanin (?)   '\n",
      "person1_last_name.strip()\n",
      "person1_last_name.replace(\"(?)\", \"\")\n",
      "person_1_children == \n",
      "\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "beyonce = \"not a Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Nifty Code for Class\n",
      "* [Make Random Student Groups](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Randomize-Students.html)\n",
      "## Genius Song Lyric API\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "import unicodedata\n",
      "from secrets import *\n",
      "import lyricsgenius\n",
      "client_access_token = \"rY1dvIUUg_dUqISKKmPItoRHRMFmPm5nSGSFwlbMXaJAwp90HXg5t8XMxFV1g92t\"\n",
      "genius = lyricsgenius.Genius(client_access_token)\n",
      "artist = genius.search_artist(\"Andy Shauf\", max_songs=3, sort=\"title\")\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    resp = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_str = resp.text\n",
      "    document = BeautifulSoup(html_str, \"html.parser\")\n",
      "    title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [title.text for title in title_tags]\n",
      "    clean_songs = []\n",
      "    for song in song_titles:\n",
      "    #re.split(r'\\W', song)\n",
      "    #song.rstrip()\n",
      "        #clean_song = song.replace(\" \", \"\")\n",
      "        #clean_song = re.search('(?<=\\\\n).*?(?=\\\\n)', clean_song).group(0)\n",
      "        clean_song = re.match('^[^\\(]+', song).group(0)\n",
      "        clean_song = re.sub('Lyrics', \"\", clean_song).strip()\n",
      "        clean_song = clean_song.strip()\n",
      "        clean_song = unicodedata.normalize(\"NFKD\", clean_song)\n",
      "        clean_songs.append(clean_song)\n",
      "    return clean_songs\n",
      "def download_album_lyrics(artist, album_name): \n",
      "    clean_songs = get_all_songs_from_album(artist, album_name)\n",
      "    genius = lyricsgenius.Genius(client_access_token)\n",
      "    genius.remove_section_headers = True\n",
      "    hyphen_artist = artist.replace(\" \", \"-\")\n",
      "    \n",
      "    album_title = album_name.replace(\" \", \"-\")\n",
      "    album_lyrics = []\n",
      "    for clean_song in clean_songs:\n",
      "        song_title = clean_song\n",
      "        song_title = song_title.replace(\" \", \"-\")\n",
      "        \n",
      "        song_object = genius.search_song(clean_song, artist)\n",
      "\n",
      "        filename = f'{hyphen_artist}-{album_title}/{song_title}.txt'\n",
      "        filename = re.sub('(-){2,}', '', filename)\n",
      "\n",
      "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
      "        \n",
      "        if song_object != None:\n",
      "            lyrics = song_object.lyrics\n",
      "            album_lyrics.append(lyrics)\n",
      "        \n",
      "            with open(filename, 'w') as file_object:\n",
      "                file_object.write(lyrics)\n",
      "        else:\n",
      "            print('No lyrics')\n",
      "    with open(f'{hyphen_artist}-{album_title}/{artist}-{album_title}-Album-Lyrics.txt', 'w') as album_file_object:\n",
      "        for song in album_lyrics:\n",
      "            album_file_object.write(song)\n",
      "        #album_lyrics.append(song_lyrics)\n",
      "        #song_lyrics.save_lyrics(extension='txt', filename=f'{album_title}/{song_title}', sanitize=True)\n",
      "    print(f'✨Download of {artist}\\'s album {album_title} complete.✨')\n",
      "download_album_lyrics(\"Lin-Manuel Miranda\", \"In the Heights Original Broadway Cast Recording\")\n",
      "download_album_lyrics(\"The Beatles\", \"Sgt Peppers Lonely Hearts Club Band\")\n",
      "download_album_lyrics(\"Mac Miller\", \"Circles\")\n",
      "download_album_lyrics(\"Jorja Smith\", \"Lost & Found\")\n",
      "download_album_lyrics(\"Mitski\", \"Puberty 2\")\n",
      "download_album_lyrics(\"Harry Styles\", \"Fine Line\")\n",
      "download_album_lyrics(\"\", \"Fine Line\")\n",
      "download_album_lyrics(\"Original Broadway Cast of Hamilton\", \"Hamilton Original Broadway Cast Recording\")\n",
      "download_album_lyrics(\"Chance The Rapper\", \"Coloring Book\")\n",
      "download_album_lyrics(\"Taylor Swift\", \"Red\")\n",
      "download_album_lyrics(\"Adele\", \"21\")\n",
      "download_album_lyrics(\"Drake\", \"Take Care\")\n",
      "download_album_lyrics(\"Lizzo\", \"Cuz I Love You\")\n",
      "download_album_lyrics(\"Carly Rae Jepsen\", \"Emotion\")\n",
      "\n",
      "## Python Fundamentals\n",
      "## Functions\n",
      "\n",
      "## Conditionals and Comparisons — Answer Key\n",
      "## Group Exercise / HW Assignment 3 (Part I)\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's [Bellevue Almshouse data](https://www.nyuirish.net/almshouse/the-almshouse-records/). Below I'm displaying the dataset with the Python library \"Pandas\" just so we can get a peek at it. We won't actually be working directly with this CSV file at the moment, and we'll talk about Pandas more in a few lessons.\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data.head(20)\n",
      "We're going to make a series of variables and assign them values based on the Bellevue Almshouse dataset. Make sure you run these cells.\n",
      "### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "1. Write an `if` statement that reports whether `person1_age` is less than 30 years old.\n",
      "if person1_age < 30:\n",
      "    print('Person is less than 30 years old.')\n",
      "2. Write an `if` statement that reports whether `person1_profession` is \"married.\"\n",
      "if person1_profession == \"married\":\n",
      "    print('Person is married.')\n",
      "3. Write an `if` statement that reports whether `person1_age` is less than 30 years old *and* `person1_profession` is \"married.\"\n",
      "if person1_age < 30 and person1_profession == \"married\":\n",
      "    print('Person is less than 30 years old and married.')\n",
      "4. Complicate your `if` statement from Question 1 by adding an `else` statement that prints \"Person is older than 30 years old\". Then evaluate whether `person3_age` is less than 30 years old. \n",
      "if person3_age < 30:\n",
      "    print('Person is less than 30 years old.')\n",
      "else:\n",
      "    print('Person is more than 30 years old.')\n",
      "5. Now evaluate whether `person4_age` is less than 30 years using the same code as Question 4.\n",
      "if person4_age < 30:\n",
      "    print('Person is less than 30 years old')\n",
      "else:\n",
      "    print('Person is more than 30 years old.')\n",
      "Hmmm, with the code as written, it's telling us that Margaret Farrell, who is 30 years old, is *more* than 30 years old. Add an `elif` statement that reports whether the person is exactly 30 years old.\n",
      "if person4_age < 30:\n",
      "    print('Person is less than 30 years old.')\n",
      "elif person4_age == 30:\n",
      "    print('Person is exactly 30 years old.')\n",
      "else:\n",
      "    print('Person is more than 30 years old.')\n",
      "6. Write an `if` statement that will report whether `person1_child_status` includes children.\n",
      "if person1_child_status != '':\n",
      "    print('Person has children.')\n",
      "7. Write one `if` statement that will accurately report whether `person1_child_status` includes children and, separately, if `person2_child_status` includes children. (Hint: think about how you might use the `!=` operator.)\n",
      "if person1_child_status != '':\n",
      "    print('Person has children.')\n",
      "if person2_child_status != '':\n",
      "    print('Person has children.')\n",
      "8. Write a conditional that will report whether `person1_profession` is \"married,\" \"laborer,\" \"widow,\" or \"unknown profession.\" Test your code by reassigning the variable as indicated below.\n",
      "person1_profession = 'married'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'laborer'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'widow'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'student'\n",
      "if person1_profession == 'married':\n",
      "    print('Person is married.')\n",
      "elif person1_profession == 'laborer':\n",
      "    print('Person is a laborer.')\n",
      "elif person1_profession == 'widow':\n",
      "    print('Person is a widow.')\n",
      "else:\n",
      "    print('Person has unknown profession.')\n",
      "9. Some of the Irish immigrants' names have question marks after them. Let's clean up some of the data and remove the question marks. You can use the Python keyword `in` to test whether a string appears within another string. Print `person2_name` with the question mark and parentheses removed. (Hint: think about f-strings and string methods!)\n",
      "if \"(?)\" in person2_name:\n",
      "    print(f\"{person2_name}\".replace(\"(?)\", \"\"))\n",
      "10. In a few sentences, write about your experiencing using and manipulating the Bellevue Almshouse data after reading Shrout's essay. How, if at all, did using this data influence your understanding of how Python works?\n",
      "#Your thoughts here\n",
      "## Comparison and Conditionals\n",
      "Last lesson, we talked about the four Python data types, including Booleans or True/False statements. We discussed how if we make a variable called `beyonce` and assign it the value \"Grammy award-winner\"...\n",
      "beyonce = \"Grammy award-winner\"\n",
      "...and then we evaluate that statement with the equals opeartor `==`...\n",
      "beyonce == \"Grammy award-winner\"\n",
      "...the operator will return a Boolean. That is to say, it will tell us whether the statement is True or False.\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "beyonce = \"not a Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "| **Comparison Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x == y `         | True if x is equal to y                                                                                |\n",
      "| `x != y `         | True if x is not equal to y                                               |\n",
      "| `x > y  `       |  True if is greater than y                                                        |\n",
      "| `string.endswith('some string')`       |  tests whether string ends with `some string`   \n",
      "| `string.isspace()`       |  tests whether string is a space |\n",
      "| `string.replace('old string', 'new string')`      | replaces `old string` with `new string`                                                                             |\n",
      "| `s.split('delim')`          | returns a list of substrings separated by the given delimiter |\n",
      "| `s.join(list)`         | opposite of split(), joins the elements in the given list together using the string                                                                        |\n",
      "                                                            \n",
      "              # Produce \n",
      "              # ... x is not equal to y\n",
      "x > y                # ... x is greater than y\n",
      "x < y                # ... x is less than y\n",
      "x >= y               # ... x is greater than or equal to y\n",
      "x <= y               # ... x is less than or equal to y\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data\n",
      "print(quiz_greeting_prompt)\n",
      "favorite_author = input(quiz_greeting_prompt)\n",
      "\n",
      "if favorite_author == 'Stephen King':\n",
      "    print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "    \n",
      "elif favorite_author == 'Virginia Woolf':\n",
      "    print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "    \n",
      "elif favorite_author == 'James Baldwin':\n",
      "    print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "    \n",
      "elif favorite_author == 'Stephenie Meyer':\n",
      "    print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "else:\n",
      "    print(f\"You said {favorite_author}. I don't know her\")\n",
      "\n",
      "def author_quiz():\n",
      "    \n",
      "    favorite_author = input('Insert name of immigrant')\n",
      "\n",
      "    if favorite_author == 'Mary Gallagher':\n",
      "        print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "\n",
      "    elif favorite_author == 'Virginia Woolf':\n",
      "        print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "\n",
      "    elif favorite_author == 'James Baldwin':\n",
      "        print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "\n",
      "    elif favorite_author == 'Stephenie Meyer':\n",
      "        print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "    else:\n",
      "        print(\"I don't know her\")\n",
      "def make_lowercase(some_chunk_of_text):\n",
      "    #Your line here\n",
      "    return lowercase_chunk\n",
      "def make_lowercase(some_chunk_of_text):\n",
      "    #Your line here\n",
      "    return lowercase_chunk\n",
      "author_quiz()\n",
      "favorite_author = input('Insert name of immigrant')\n",
      "\n",
      "if favorite_author == 'Mary Gallagher':\n",
      "    print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "    \n",
      "elif favorite_author == 'Virginia Woolf':\n",
      "    print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "    \n",
      "elif favorite_author == 'James Baldwin':\n",
      "    print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "    \n",
      "elif favorite_author == 'Stephenie Meyer':\n",
      "    print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "else:\n",
      "    print(\"I don't know her\")\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_gender = 'm'\n",
      "\n",
      "person1 = 'Mary Gallagher'\n",
      "if person1_gender == 'f':\n",
      "    print('According to the Bellevue gender is female')\n",
      "else:\n",
      "    print('Male')\n",
      "person1_last_name = '   Sanin (?)   '\n",
      "person1_last_name.strip()\n",
      "person1_last_name.replace(\"(?)\", \"\")\n",
      "person_1_children == \n",
      "if \n",
      "if person1_gender == 'f':\n",
      "    print()\n",
      "\n",
      "\"Greetings! This is a Buzzfeed-style quiz that will tell you where you'd like to live based on your favorite author.\n",
      "\n",
      "Please enter your favorite author from the following: James Joyce, Virginia Woolf, Ernest Hemingway, J.K. Rowling, Stephen King, Stephenie Meyer.\"\n",
      "%run Buzzfeed_Author_Quiz.py\n",
      "favorite_author\n",
      "if favorite_author == 'Stephen King':\n",
      "    print(\"A creepy hotel\")\n",
      "Can we guess your favorite [blank] absed on your age?\n",
      "Use a greater than sign, use a less than or equal to sign, \n",
      "favorite_author = input(\"What's your age?\")\n",
      "\n",
      "if int(favorite_author) >  16:\n",
      "    print(\"You'd probably like to live in: a creepy hotel\")\n",
      "    \n",
      "if favorite_author == 'Virginia Woolf':\n",
      "    print(\"You'd probably like to live in: a room of your own in London\")\n",
      "    \n",
      "if favorite_author == 'James Joyce':\n",
      "    print(\"You'd probably like to live in: a pub in Dublin\")\n",
      "    \n",
      "if favorite_author == 'Stephenie Meyer':\n",
      "    print(\"You'd probably like to live in: a rainy town in Washington with mysterious new neighbors\")\n",
      "\n",
      "else:\n",
      "    print(\"I don't know her\")\n",
      "## Comparisons and Conditionals\n",
      "| **Comparison Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x == y `         | `True` if x is equal to y                                                                                |\n",
      "| `x != y `         | `True` if x is not equal to y                                               |\n",
      "| `x > y`       |  `True` if is greater than y                                                        |\n",
      "| `x < y`       |   `True` if x is less than y  \n",
      "| `x >= y`       |   `True` if x is greater than or equal to y |\n",
      "| `x <= y`      | `True` if x is less than or equal to y`                                                                             |\n",
      "                                                                      |\n",
      "                                                            \n",
      "| **Logical Operator** | **Explanation**                                                                                   |\n",
      "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
      "| `x and y`         | `True` if x and y are both True                                                                             |\n",
      "| `x or y`         | `True` if either x or y is True                                              |\n",
      "| `not x`       |  `True` if is x is not True                                                       |\n",
      "                                                            \n",
      "Last lesson, we talked about the four Python data types, which included Booleans or True/False statements. We discussed how if we make a variable called `beyonce` and assign it the value \"Grammy award-winner\"...\n",
      "beyonce = \"Grammy award-winner \"\n",
      "...and then we evaluate that statement with the equals operator `==`...\n",
      "beyonce == \"Grammy award-winner\"\n",
      "type(beyonce == \"Grammy award-winner\")\n",
      "...the operator will return a Boolean, which will tell us whether the statement is True or False.\n",
      "\n",
      "Booleans may seem simple, but they're extremely powerful and function as the engine of a lot of code. They allow us to program the computer to do things based on different \"conditions.\"\n",
      "## If Statement\n",
      "An `if` statement is an instruction to do something *if* a particular condition is True. Both parts of this equation together are called a conditional.\n",
      "\n",
      "A common and simple conditional will consist of two lines. On the first line, you will type the English word `if` followed by an evaluation `beyonce == \"Grammy award-winner\"` and then a colon `:`. On the second line, you will indent (or hit `tab`), then write an instruction to be completed if the condition is met, such as `print(\"Congratulations, Beyonce!\")`\n",
      "beyonce = \"Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "Python is picky about how you format `if` statements. Look what happens if I forget to tab over on the second line or if I forget the colon:\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "print(\"Congratulations, Beyonce!\")\n",
      "if beyonce == \"Grammy award-winner\"\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "## Else Statement\n",
      "You can add even more complexity in a conditional by adding an `else` statement. This will instruct the computer program to do something in case the condition is not met. You format it the same way you format an `if` statement, directly following the `if` statement.\n",
      "beyonce = \"not a Grammy award-winner this year\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Elif Statement\n",
      "Sometimes you want even more nuance to respond to slightly different conditions. For example, if Beyonce was nominated for a Grammy but didn't win, then we might want to express a slightly different sentiment than if she was nominated at all. You can add in this nuance with an `elif` state or else if statement, which will evaluate the first `if` statement first and then *if* that statement is not True, it will evaluate the `elif` statement.\n",
      "beyonce = \"Grammy award-nominee\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "elif beyonce == \"Grammy award-nominee\":\n",
      "    print(\"Ok well at least they nominated you, Beyonce.\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Group Exercise / HW Assignment 3\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's Bellevue Almshouse data. Below I'm displaying the dataset with the Python library \"Pandas\" just so we can get a peek at it. We won't actually be working directly with this CSV file at the moment, and we'll talk about Pandas more in a few lessons.\n",
      "import pandas as pd\n",
      "almshouse_data = pd.read_csv('../data/bellevue_almshouse_modified.csv')\n",
      "almshouse_data.head(20)\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "1. Write an `if` statement that reports whether person1 is younger than 30 years old.\n",
      "#Your Code Here\n",
      "    print('Person is older than 30')\n",
      "if person1_age < 30:\n",
      "    print('Person is younger than 30 years old')\n",
      "else:\n",
      "    print('Person is younger than 30')\n",
      "if person1_age > 30:\n",
      "    print('Person is older than 30')\n",
      "else:\n",
      "    print('Person is younger than 30')\n",
      "person1 = 'Mary Gallagher'\n",
      "if person1_gender == 'f':\n",
      "    print('According to the Bellevue gender is female')\n",
      "else:\n",
      "    print('Male')\n",
      "if \"Child\" in person1_child_status:\n",
      "    print('Person has children')\n",
      "else:\n",
      "    print('Person does not have children')\n",
      "person1_last_name = '   Sanin (?)   '\n",
      "person1_last_name.strip()\n",
      "person1_last_name.replace(\"(?)\", \"\")\n",
      "person_1_children == \n",
      "\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "beyonce = \"not a Grammy award-winner\"\n",
      "if beyonce == \"Grammy award-winner\":\n",
      "    print(\"Congratulations, Beyonce!\")\n",
      "else:\n",
      "    print(\"They messed up, Beyonce.\")\n",
      "## Nifty Code for Class\n",
      "* [Make Random Student Groups](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Randomize-Students.html)\n",
      "## Genius Song Lyric API\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import re\n",
      "import unicodedata\n",
      "from secrets import *\n",
      "import lyricsgenius\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    resp = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_str = resp.text\n",
      "    document = BeautifulSoup(html_str, \"html.parser\")\n",
      "    title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [title.text for title in title_tags]\n",
      "    clean_songs = []\n",
      "    for song in song_titles:\n",
      "    #re.split(r'\\W', song)\n",
      "    #song.rstrip()\n",
      "        #clean_song = song.replace(\" \", \"\")\n",
      "        #clean_song = re.search('(?<=\\\\n).*?(?=\\\\n)', clean_song).group(0)\n",
      "        clean_song = re.match('^[^\\(]+', song).group(0)\n",
      "        clean_song = re.sub('Lyrics', \"\", clean_song).strip()\n",
      "        clean_song = clean_song.strip()\n",
      "        clean_song = unicodedata.normalize(\"NFKD\", clean_song)\n",
      "        clean_songs.append(clean_song)\n",
      "    return clean_songs\n",
      "def download_album_lyrics(artist, album_name): \n",
      "    clean_songs = get_all_songs_from_album(artist, album_name)\n",
      "    genius = lyricsgenius.Genius(client_access_token)\n",
      "    genius.remove_section_headers = True\n",
      "    hyphen_artist = artist.replace(\" \", \"-\")\n",
      "    \n",
      "    album_title = album_name.replace(\" \", \"-\")\n",
      "    album_lyrics = []\n",
      "    for clean_song in clean_songs:\n",
      "        song_title = clean_song\n",
      "        song_title = song_title.replace(\" \", \"-\")\n",
      "        \n",
      "        song_object = genius.search_song(clean_song, artist)\n",
      "\n",
      "        filename = f'{hyphen_artist}-{album_title}/{song_title}.txt'\n",
      "        filename = re.sub('(-){2,}', '', filename)\n",
      "\n",
      "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
      "        \n",
      "        if song_object != None:\n",
      "            lyrics = song_object.lyrics\n",
      "            album_lyrics.append(lyrics)\n",
      "        \n",
      "            with open(filename, 'w') as file_object:\n",
      "                file_object.write(lyrics)\n",
      "        else:\n",
      "            print('No lyrics')\n",
      "    with open(f'{hyphen_artist}-{album_title}/{artist}-{album_title}-Album-Lyrics.txt', 'w') as album_file_object:\n",
      "        for song in album_lyrics:\n",
      "            album_file_object.write(song)\n",
      "        #album_lyrics.append(song_lyrics)\n",
      "        #song_lyrics.save_lyrics(extension='txt', filename=f'{album_title}/{song_title}', sanitize=True)\n",
      "    print(f'✨Download of {artist}\\'s album {album_title} complete.✨')\n",
      "download_album_lyrics(\"Lin-Manuel Miranda\", \"In the Heights Original Broadway Cast Recording\")\n",
      "download_album_lyrics(\"The Beatles\", \"Sgt Peppers Lonely Hearts Club Band\")\n",
      "download_album_lyrics(\"Mac Miller\", \"Circles\")\n",
      "download_album_lyrics(\"Jorja Smith\", \"Lost & Found\")\n",
      "download_album_lyrics(\"Mitski\", \"Puberty 2\")\n",
      "download_album_lyrics(\"Harry Styles\", \"Fine Line\")\n",
      "download_album_lyrics(\"\", \"Fine Line\")\n",
      "download_album_lyrics(\"Original Broadway Cast of Hamilton\", \"Hamilton Original Broadway Cast Recording\")\n",
      "download_album_lyrics(\"Chance The Rapper\", \"Coloring Book\")\n",
      "download_album_lyrics(\"Taylor Swift\", \"Red\")\n",
      "download_album_lyrics(\"Adele\", \"21\")\n",
      "download_album_lyrics(\"Drake\", \"Take Care\")\n",
      "download_album_lyrics(\"Lizzo\", \"Cuz I Love You\")\n",
      "download_album_lyrics(\"Carly Rae Jepsen\", \"Emotion\")\n",
      "\n",
      "## Python Fundamentals\n",
      "Variables, evaluations, conditionals, Data types, Strings\n",
      "number_of_tweets_per_hour\n",
      "## Example Word Count Python Code\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "\n",
      "with open(filepath_of_text, encoding=\"utf-8\") as file_object:\n",
      "    full_text = file_object.read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "count_of_meaningful_words = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = count_of_meaningful_words.most_common(40)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "## Variables\n",
      "Variables are one of the fundamental building blocks of Python. A variable is like a tiny container where you store information that you're going to use later on --- filenames, words, numbers, collections of words and numbers, etc. When you put information into a variable, it's called \"assigning\" a variable. You assign variables with an equals `=` sign. \n",
      "> The \"real\" Python equals notation is two equals signs side-by-side `==`, e.g. `2 * 2 == 4`\n",
      "Let's look at the variables that we used when we counted the most frequent words in Charlotte Perkins-Gilman's \"The Yellow Wallpaper.\"\n",
      "filepath_of_text = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "In the above cell, we assigned the filepath of our \"The Yellow Wallpaper\" text file (\"../texts/The-Yellow-Wallpaper.txt\") to the variable `filepath_of_text`. We can check to see what's \"inside\" the variable by running a cell with the variable's name.\n",
      "\n",
      "> Outside the Jupyter environment, you would need to run `print(filepath_of_text)` to display the variable. You can run the `print` function in Jupyter, too. But you don't need to so, which is convenient.\n",
      "filepath_of_text\n",
      "print(filepath_of_text)\n",
      "We also assigned our list of stopwords to a variable called `nltk_stop_words`.\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "nltk_stop_words\n",
      "Our stopwords variable shows how useful variables can be. Rather than writing out all 179 stopwords in this list, we put them all into a variable and used them there.\n",
      "Same goes for the variable `all_the_words`, where we stored all 6,000+ words from the short story.\n",
      "all_the_words = split_into_words(full_text)\n",
      "\n",
      "all_the_words\n",
      " \n",
      "## Variable Names\n",
      "Though we named our variables `filepath_of_text`, `nltk_stop_words`, and `all_the_words`, we could have named them almost anything else.\n",
      "\n",
      "Variable names can be as long or as short as you want, and they can include:\n",
      "- upper or lower-case letters (A-Z)\n",
      "- digits (0-9)\n",
      "- underscores (_)\n",
      "Instead of `filepath_of_text`, we could have simply named the variable `filepath`.\n",
      "filepath = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "filepath\n",
      "Or we could have gone even simpler and named the filepath `f`.\n",
      "f = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "f\n",
      "As you start to code, you will almost certainly be tempted to use extremely short variables names like `f`. Your fingers will get tired. Your coffee will wear off. You will see other people using variables like `f`. And you'll promise yourself that you'll definitely remember what `f` means.\n",
      "\n",
      "But I must urge you: *resist the temptation of bad variable names at all costs!!*\n",
      "\n",
      "Clear and precisely-named variables will\n",
      "\n",
      "1. Make your code more readable (both to yourself and others)\n",
      "2. Reinforce your understanding of Python and what's happening in the code\n",
      "3. Sharpen your thinking\n",
      "\n",
      "This principle applies to everyone, but I think readable code is *especially* important for students and scholars in the humanities, because we're more used to reading and writing with words. Programming languages are also still quite new and relatively more unfamiliar to humanities folks. When we write clear code, we help build bridges among different communities of people.\n",
      "### Example Python Code ❌ **With Bad Variable Names** ❌\n",
      "For the sake of illustration, here's our same word count Python code with poorly named variables. This code works exactly the same and still gives us the 40 most frequently occurring words, but it's *much* harder to read. Not ideal.\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def sp(t):\n",
      "    lt = t.lower()\n",
      "    sw = re.split(\"\\W+\", lt)\n",
      "    return sw\n",
      "\n",
      "f = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "st = stopwords.words(\"english\")\n",
      "\n",
      "with open(f, encoding=\"utf-8\") as fo:\n",
      "    ft = fo.read()\n",
      "\n",
      "words = sp(ft)\n",
      "words = [w for w in words if w not in st]\n",
      "words = Counter(words)\n",
      "words = words.most_common(40)\n",
      "\n",
      "print(words)\n",
      "### Example Python Code ✨ **With Good Variable Names** ✨\n",
      "import re\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "\n",
      "filepath_of_text = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "\n",
      "with open(filepath_of_text, encoding=\"utf-8\") as file_object:\n",
      "    full_text = file_object.read()\n",
      "\n",
      "all_the_words = split_into_words(full_text)\n",
      "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "count_of_meaningful_words = Counter(meaningful_words)\n",
      "most_frequent_meaningful_words = count_of_meaningful_words.most_common(40)\n",
      "\n",
      "print(most_frequent_meaningful_words)\n",
      "### Off-Limits Names\n",
      "The only variable names that are off-limits are names that are reserved by (or built into) the Python programming language itself, such as `print`, `True`, or `list`. But it's not something to worry too much about. You'll know if a name is reserved when it shows up in green.\n",
      "True = \"../texts/The-Yellow-Wallpaper.txt\"\n",
      "nltk_stop_words = stopwords.words(\"english\")\n",
      "all_words = split_into_words(full_text)\n",
      "full_text = ''\n",
      "meme_text = ''\n",
      "number_of_tweets_per_hour = 3\n",
      "number_of_tweets_per_hour\n",
      "print(number_of_tweets_per_hour)\n",
      "number_of_tweets_per_hour = 15\n",
      "number_of_tweets_per_hour\n",
      "number_of_tweets_per_hour = input(\"How many tweet hours?\")\n",
      "\"Greetings! This is a Buzzfeed-style quiz that will tell you where you'd like to live based on your favorite author.\n",
      "\n",
      "Please enter your favorite author from the following: James Joyce, Virginia Woolf, Ernest Hemingway, J.K. Rowling, Stephen King, Stephenie Meyer.\"\n",
      "quiz_greeting_prompt = \"✨Greetings!✨This is a Buzzfeed-style quiz that will tell you where you should take a vacation based on your favorite author. Please enter your favorite author from the following:\\nJames Joyce, Virginia Woolf, Ernest Hemingway, J.K. Rowling, Stephen King, Stephenie Meyer.\\n\"\n",
      "favorite_author = input(quiz_greeting_prompt)\n",
      "\n",
      "if favorite_author == 'Stephen King':\n",
      "    print(\"✨You'd probably like to live in: a creepy hotel✨\")\n",
      "    \n",
      "elif favorite_author == 'Virginia Woolf':\n",
      "    print(\"✨You'd probably like to live in: a room of your own in London✨\")\n",
      "    \n",
      "elif favorite_author == 'James Baldwin':\n",
      "    print(\"✨You'd probably like to live in: a neighborhood with good jazz in NYC✨\")\n",
      "    \n",
      "elif favorite_author == 'Stephenie Meyer':\n",
      "    print(\"✨You'd probably like to live in: a rainy town in Washington✨\")\n",
      "else:\n",
      "    print(\"I don't know her\")\n",
      "!python Buzzfeed_Author_Quiz.py\n",
      "favorite_author\n",
      "if favorite_author == 'Stephen King':\n",
      "    print(\"A creepy hotel\")\n",
      "favorite_author = input(\"What's your age?\")\n",
      "\n",
      "if int(favorite_author) >  16:\n",
      "    print(\"You'd probably like to live in: a creepy hotel\")\n",
      "    \n",
      "if favorite_author == 'Virginia Woolf':\n",
      "    print(\"You'd probably like to live in: a room of your own in London\")\n",
      "    \n",
      "if favorite_author == 'James Joyce':\n",
      "    print(\"You'd probably like to live in: a pub in Dublin\")\n",
      "    \n",
      "if favorite_author == 'Stephenie Meyer':\n",
      "    print(\"You'd probably like to live in: a rainy town in Washington with mysterious new neighbors\")\n",
      "\n",
      "else:\n",
      "    print(\"I don't know her\")\n",
      "## Evaluations\n",
      "Collections - Lists, Dictionaries\n",
      "For Loops and List Comprehensions \n",
      "Reading and Writing Files\n",
      "Libraries\n",
      "\n",
      "## Data Visualization\n",
      "\n",
      "## Data Visualization\n",
      "\n",
      "## Cultural Datasets\n",
      "## Food 🍔\n",
      "\n",
      "## Other Dataset Compilations\n",
      "\n",
      "\n",
      "## Cultural Datasets\n",
      "## Food 🍔\n",
      "\n",
      "## Other Dataset Compilations\n",
      "\n",
      "\n",
      "# The Command Line\n",
      "Here are some of the important things that you can do from the command line (and why we're learning about it):\n",
      "\n",
      "* Run a Python script\n",
      "* Install software\n",
      "* Connect to a remote server (i.e., use a different computer from your current computer)\n",
      "* Do simple tasks faster and more efficiently\n",
      "* Gain more power and flexibility over your computing experience\n",
      "<img src=\"../images/command-line/welcome.png\" alt=\"The command line\" width=\"100%\" border=2>\n",
      "Before we begin learning about the programming language Python, we need to learn about \"the command line.\" The command line — also referred to as \"the shell,\" \"bash,\" or \"terminal — is the gateway to computational analysis. Think of it like interacting with your computer \"behind the scenes.\"\n",
      "> Technically speaking, \"command line,\" \"shell,\" \"bash,\" and \"terminal\" all mean slightly different things. The Digital Humanities Research Institute provides [a helpful primer on the distinctions](https://github.com/DHRI-Curriculum/glossary/blob/master/sections/command-line.md#glossary).\n",
      "The command line interface is often contrasted with the graphical user interface (GUI), which is the way that most people are familiar with navigating their computers. Let's say you wanted to make a new folder to organize your notes for this class. To do so, you could drag your mouse cursor 🖱️ and click on a button that says \"New Folder,\" which would make a small icon of a folder appear 📁. You might title this folder \"Intro-CA-Notes.\" If you wanted to delete the folder, you could then drag and drop it into a tiny trash can.\n",
      ">GUI is pronounced \"gooey,\" like St. Louis gooey butter cake\n",
      "This is the GUI in action. This is how you interact with your computer *graphically* — with visual icons, movements, and mouse clicks. \n",
      "<img src=\"../images/command-line/GUI-example.gif\" alt=\"Making and deleting a folder with GUI\" width=\"100%\" border=2>\n",
      "\n",
      "You can do all of the above from the command line, as well. Instead of dragging and dropping little folder icons, however, you would type in textual commands, such as `mkdir Intro-CA-Notes` and `rmdir Intro-CA-Notes`. \n",
      "  \n",
      "  \n",
      "  \n",
      "     \n",
      "For more details on these commands, see the \"Command Line Cheatsheet\" below.\n",
      "<img src=\"../images/command-line/CLI-example.gif\" alt=\"Making and deleting a folder with command line\" width=\"100%\" border=2>\n",
      "<p align=center>The command line in action </p>\n",
      "Making a folder from the command line — as opposed to dragging and clicking with your mouse — won't save you a ton of time. Maybe a couple of seconds at most. But in the aggregate, with larger tasks, these seconds start to add up. Since you can automate tasks from the command line, you can transform things that would be tedious and time-consuming to do manually — such as resizing 1000 photos or combining a dozen smaller CSV files into one large file — into useful and efficient scripts.\n",
      "\n",
      "That's just the tip of the iceberg. From the command line, you can also run Python code, install software, connect to a remote server or a [Raspberry Pi](https://www.raspberrypi.org/products/raspberry-pi-4-model-b/), and more. The command line offers you greater power and flexibility over your computing experience -- in ways that we will cover in this class and in ways that also exceed the scope of this class.\n",
      "## Where do I find the command line?\n",
      "Your command line will differ depending on the operating system (OS) that you're using. Most operating systems fall into two different families: the **Unix-like** family and the **Microsoft Windows** family.\n",
      "\n",
      "Mac OS and Linux (as well as Android and Chrome OS) are all part of the Unix-like family, which means that they all share a similar command line. We're going to focus on Mac OS and Microsoft Windows OS because they represent the two big families and because they're the operating systems that you're most likely to be using. \n",
      "## Mac OS\n",
      "If you're using a Mac, you will access the command line through an application called \"Terminal.\" You can find \"Terminal\" under Applications -> Utilities.\n",
      "<img src=\"../images/command-line/Terminal-Applications-Utilities.png\" alt=\"Open Mac Terminal in Applications\" width=\"100%\" border=2>\n",
      "Alternatively, you can find it by clicking Spotlight Search in the top right corner, typing \"Terminal,\" and then double-clicking the application or pressing enter. If you want to keep \"Terminal\" within even handier and speedier reach, you can keep it in your dock by right-clicking the application and selecting Options -> Keep in Dock. \n",
      "from IPython.display import IFrame\n",
      "IFrame(\"../videos/Terminal-Keep-In-Dock.mp4\", width='100%', height='400px')\n",
      "When you open Terminal, you'll see the name of your computer and your username followed by a dollar sign `$`. When I go to the command line, I see `Melanies-MacBook-Pro:~ melaniewalsh$`.\n",
      "\n",
      "You will type commands after the dollar sign `$`.\n",
      "## Windows OS\n",
      "The command line for Windows users, called \"Command Prompt,\" is different from the command line interface for Mac users and the wider Unix-like family. The Windows Command Prompt is not as powerful as the Unix shell in certain ways. However, beginning with Windows 7, Microsoft also introduced \"PowerShell,\" which acts like a more flexible and more powerful command line for Windows users. (You can read more about PowerShell in [Microsoft's official documentation](https://docs.microsoft.com/en-us/powershell/?view=powershell-6)).\n",
      "\n",
      "We're going to focus on PowerShell, as opposed to Command Prompt, for a number of reasons, but one very convenient reason is that PowerShell commands have \"aliases.\" They go by different names, that is, including names that are used by the traditional Windows Command Prompt and by a Mac Terminal.\n",
      "\n",
      ">For example, the PowerShell command that lists all the files and folders in a particular directory is `Get-ChildItem`. The traditional Windows Command Prompt command `dir` and the Unix command `ls` also perform the same function.\n",
      "\n",
      "These aliases will conveniently allow you to learn a little about Unix and Windows commands at the same time. Since we're mostly trying to understand how the command line functions at a broad level, this level of familiarity will suffice for now. In the future, you may want to invest more time in learning the specifics of PowerShell or the Command Prompt.\n",
      "<img src=\"https://www.isunshare.com/images/article/windows-10/5-ways-to-open-windows-powershell-in-windows-10/open-windows-powershell-by-search.png\" alt=\"Open Windows PowerShell\" width=\"100%\" border=2>\n",
      "\n",
      "Further resources: Google's [Windows Command-line Tools](https://developers.google.com/web/shows/ttt/series-2/windows-commandline)\n",
      "## Command Line Cheatsheet\n",
      "📂 = name of your desired directory path (e.g. `/Users/melaniewalsh/coding-project`) <br>\n",
      "📝 = name of your desired file (e.g. `Frankenstein.txt`)\n",
      "\n",
      "| Mac Terminal        | Explanation                                                                                                | Windows Command Prompt       | Windows PowerShell        |\n",
      "|:---------------------------:|:-----------------------------------------------------------------------------------------------------------:|:---------------------------:|:--------------------------------:|\n",
      "| `cd 📂`  | **c**hange **d**irectory, aka move into a different folder                                                | `cd 📂` | `cd 📂`      |\n",
      "| `ls`                      | **l**i**s**t the files and folders in your current **dir**ectory                                          | `dir`                     | `ls` / `dir` / `Get-ChildItem` |\n",
      "| `pwd`                     | show **p**ath of **w**orking **d**irectory, aka the folder that you're in right now                       | `cd`                      | `pwd` / `cd`                   |\n",
      "| `touch 📝`                   | make a new file                                                                                           |                           | `ni 📝`                           |\n",
      "| `mkdir 📂`                   | **m**a**k**e a new **dir**ectory, aka a folder                                                            | `mkdir 📂`                   | `mkdir 📂`                        |\n",
      "| `rm 📝`                      | **r**e**m**ove, aka delete, a file or directory                                                           | `del 📝`                     | `rm 📝` / `del 📝`                   |\n",
      "| `cp 📝(original) 📝(copy)`                      | **c**o**p**y a file or directory                                                                          | `copy 📝(original) 📝(copy)`                    | `cp` / `copy`                  |\n",
      "| `mv 📝(old) 📝(new)`                      | **m**o**v**e or rename a file or directory                                                                | `move 📝(old) 📝(new)` / `ren 📝`            | `move` / `ren `                 |\n",
      "| `cat 📝`                     | show all the contents of a file                                                                           |                           | `cat 📝` / `type 📝`                 |\n",
      "| `less 📝`                    | show snippet of a file that allows you to scroll through the entire thing                                 | `more 📝`                      | `more 📝`                           |\n",
      "| `head 📝`                    | show the first 10 lines of a file (change number of lines by adding `-*a number*` flag, e.g. `head -100`) |                           |    `gc 📝 -head 10`                           |\n",
      "| `tail 📝`                    | show the last 10 lines of a file (change number of lines by adding `-*a number*` flag, e.g. `tail -100`)  |                           | `gc 📝 -tail 10`                                  |\n",
      "| `wc -w -l 📝`                      | show how many **w**ords or lines in a file                            |                           | `gc 📝 \\| Measure-Object -Word –Line`                               |\n",
      "| `man **a command**`                     | show the **man**ual, aka the documentation that tells you what a particular command does                  | `help`                    | `help`                         |\n",
      "| `echo`                    | print text to the command line                                                                            | `echo`                    | `echo`                         |\n",
      "| `grep 📝 or 📂`                    |                                                                                                           | `findstr 📝`                 | `findstr 📝`                      |\n",
      "| `curl -O 📝` or `wget 📝`                    |  **get**, a file from the **w**eb                                                            |                           | `wget 📝 (file from web) -OutFile 📝 (new file name)`         |\n",
      "\n",
      "## Command Line Walk-Through\n",
      "*Note: the `%` and `!` symbols at the beginning of the lines below allow us to access the command line from a Jupyter notebook. If you were using these commands outside the Jupyter environemnt, you would not need to include `%` or `!`*\n",
      "## Basic Navigation and Directory Structure\n",
      "⭐ *You are here!* ⭐ One of the most basic and useful commands is the one that tells you where you are on your computer. Type `pwd` and it will show you the **p**ath of your **w**orking **d**irectory, aka the folder that you're currently in. This might not seem terribly important at the moment. But it is!\n",
      "### Display Path of Working Directory\n",
      "%pwd\n",
      "The output above tells me that I'm in a directory (aka folder) called \"Command Line\" inside another directory called \"Website-Content\" inside another directory called \"Intro-Cultural-Analytics\" (which holds all the files and code for this website) inside another directory called \"melaniewalsh\" inside another directory called \"Users.\" The forward slash `/` before and after each name indicates that the name is a directory (on Windows computers directories are indicated by backslashes `\\`).\n",
      "\n",
      "This path is unique to my personal machine, which you could probably guess because it includes my name. If you run `pwd` live on this website or in a Jupyter notebook in the cloud, it should return `/home/joyvan/content/Command-Line` because `/home/joyvan/` is the \"home\" location of this notebook in the cloud. If you open up a command line on your own personal machine and type `pwd`, it should return your personal \"home\" directory. The \"home\" location on a Mac will be `/Users/your-specific-username`.\n",
      "### List Files and Folders\n",
      "%ls\n",
      "If you want to see what's inside the directory that you're currently in, you can run `ls`, which **l**i**s**t all the files and folders. The output above shows that this directory contains \"The-Command-Line.ipynb\" (this very Jupyter notebook!) as well as another directory called \"very-interesting-folder.\" Hmm it sounds so interesting! Don't you want to find out what's inside?\n",
      "\n",
      "If we want to find out what's inside that directory, we can move into it by using the **c**hange **d**irectory `cd` command and plugging in the name of the directory `very-interesting-folder`.\n",
      "### Change Directory\n",
      "%cd very-interesting-folder/\n",
      "%ls\n",
      "What's inside here? Hmmm some facts. We'll learn how to check out what's written inside the file below. But first...\n",
      "### Relative Path vs Absolute Path\n",
      "There are two ways to to tell your computer where you want to go AKA your desired *path*:\n",
      "\n",
      "* Relative path depends on your current location, often a short or shortened way of writing the path\n",
      "* Absolute path is the full and \"true\" location from the bird's eye view of your home directory, often much longer\n",
      "\n",
      "We used a ***relative*** path when we moved into \"very-interesting-folder\" because we simply typed \"very-interesting-folder\". We were already inside \"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Command-Line/\" so that's all the information we needed.\n",
      "\n",
      "If we wanted to move inside \"very-interesting-folder\" from a completely different place on our computers, we could use an ***absolute*** path: \"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Command-Line/very-interesting-folder\"\n",
      "%cd /Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Command-Line/very-interesting-folder\n",
      "%cd ~/Website-Content/Command-Line/very-interesting-folder\n",
      "It's very common to move up through your directory structure with a relative path, which can you do with two periods `..`\n",
      "*Moving*\n",
      "%cd ..\n",
      "*on*\n",
      "%cd ..\n",
      "*up!*\n",
      "%cd ..\n",
      "%cd ~/Intro-Cultural-Analytics/Website-Content/Command-Line/very-interesting-folder\n",
      "Rather than listing the files and folders inside my present working directory `very-interesting-folder`, I can list the ones in the directory above me:\n",
      "%ls ../\n",
      "Most of our text files in this class live one directory above the Jupyter notebooks, so we're often going to be referencing them by writing \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "\n",
      "* Relative path \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "* Absolute path \"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/texts/music/Beyonce-Lemonade.txt` or `/home/joyvan/Website-Content/texts/music/Beyonce-Lemonade.txt\"\n",
      "## Working with Files and Texts\n",
      "There's a lot you can do with files and texts from the command line!\n",
      "### Download a text file from the web\n",
      "[Project Gutenberg](https://www.gutenberg.org/ebooks/search/) is an amazing resource that hosts tens of thousands of free texts in the public domain. Let's go find \"The Yellow Wallpaper.\"\n",
      "!curl -O http://www.gutenberg.org/files/1952/1952-0.txt\n",
      "!wget http://www.gutenberg.org/files/1952/1952-0.txt\n",
      "#PS C:\\Users> wget http://www.gutenberg.org/files/1952/1952-0.txt -OutFile The-Yellow-Wallpaper.txt\n",
      "%ls\n",
      "### Rename a file\n",
      "%mv 1952-0.txt The-Yellow-Wallpaper.txt\n",
      "%ls\n",
      "### Check file sizes, lengths, and word counts\n",
      "The `-lh` flag shows you, among other things, the sizes of the files in your directory, which is very handy. \n",
      "%ls -lh\n",
      "`The-Yellow-Wallpaper.txt` is 50 kilobytes, and `an-interesting-fact` is 214 byes (tiny!).\n",
      "Let's check how many words are in \"The Yellow Wallpaper.\"\n",
      "!wc -w The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt | Measure-Object –Word\n",
      "Let's check how many lines are in \"The Yellow Wallpaper.\"\n",
      "!wc -l The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt | Measure-Object –Line\n",
      "### Display the contents of a file\n",
      "The `cat` command prints the whole file to your command line.\n",
      "!cat The-Yellow-Wallpaper.txt\n",
      "That's often too much text to handle at one time. So you can also look at the first 10 lines of the file with `head`.\n",
      "!head The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt -head 10\n",
      "Or by adding a flag with a number (`-50`), you can look at the first however many lines you want!\n",
      "!head -50 The-Yellow-Wallpaper.txt\n",
      "The same goes for looking at the end of a file with `tail`\n",
      "!tail The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt -tail 10\n",
      "### Searching inside files\n",
      "!grep \"yellow\" -n The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> findstr -n \"yellow\" The-Yellow-Wallpaper.txt\n",
      "!grep \"yellow\" -n --color The-Yellow-Wallpaper.txt\n",
      "!grep -wc \"yellow\" The-Yellow-Wallpaper.txt\n",
      "!grep -wc \"paper\" The-Yellow-Wallpaper.txt\n",
      "!grep \"wallpaper\" -B 2 -A 2 -n --color The-Yellow-Wallpaper.txt\n",
      "___\n",
      "## Further Resources\n",
      "* The Launch School's [Introduction to the Command Line](https://launchschool.com/books/command_line/read/introduction)\n",
      "* DHRI's [\"Introduction to the Command Line\"](https://github.com/DHRI-Curriculum/command-line)\n",
      "# The Command Line\n",
      "Here are some of the important things that you can do from the command line (and why we're learning about it):\n",
      "\n",
      "* Run a Python script\n",
      "* Install software\n",
      "* Connect to a remote server (i.e., use a different computer from your current computer)\n",
      "* Do simple tasks faster and more efficiently\n",
      "* Gain more power and flexibility over your computing experience\n",
      "<img src=\"../images/command-line/welcome.png\" alt=\"The command line\" width=\"100%\" border=2>\n",
      "Before we begin learning about the programming language Python, we need to learn about \"the command line.\" The command line — also referred to as \"the shell,\" \"bash,\" or \"terminal — is the gateway to computational analysis. Think of it like interacting with your computer \"behind the scenes.\"\n",
      "> Technically speaking, \"command line,\" \"shell,\" \"bash,\" and \"terminal\" all mean slightly different things. The Digital Humanities Research Institute provides [a helpful primer on the distinctions](https://github.com/DHRI-Curriculum/glossary/blob/master/sections/command-line.md#glossary).\n",
      "The command line interface is often contrasted with the graphical user interface (GUI), which is the way that most people are familiar with navigating their computers. Let's say you wanted to make a new folder to organize your notes for this class. To do so, you could drag your mouse cursor 🖱️ and click on a button that says \"New Folder,\" which would make a small icon of a folder appear 📁. You might title this folder \"Intro-CA-Notes.\" If you wanted to delete the folder, you could then drag and drop it into a tiny trash can.\n",
      ">GUI is pronounced \"gooey,\" like St. Louis gooey butter cake\n",
      "This is the GUI in action. This is how you interact with your computer *graphically* — with visual icons, movements, and mouse clicks. \n",
      "<img src=\"../images/command-line/GUI-example.gif\" alt=\"Making and deleting a folder with GUI\" width=\"100%\" border=2>\n",
      "\n",
      "You can do all of the above from the command line, as well. Instead of dragging and dropping little folder icons, however, you would type in textual commands, such as `mkdir Intro-CA-Notes` and `rmdir Intro-CA-Notes`. \n",
      "  \n",
      "  \n",
      "  \n",
      "     \n",
      "For more details on these commands, see the \"Command Line Cheatsheet\" below.\n",
      "<img src=\"../images/command-line/CLI-example.gif\" alt=\"Making and deleting a folder with command line\" width=\"100%\" border=2>\n",
      "<p align=center>The command line in action </p>\n",
      "Making a folder from the command line — as opposed to dragging and clicking with your mouse — won't save you a ton of time. Maybe a couple of seconds at most. But in the aggregate, with larger tasks, these seconds start to add up. Since you can automate tasks from the command line, you can transform things that would be tedious and time-consuming to do manually — such as resizing 1000 photos or combining a dozen smaller CSV files into one large file — into useful and efficient scripts.\n",
      "\n",
      "That's just the tip of the iceberg. From the command line, you can also run Python code, install software, connect to a remote server or a [Raspberry Pi](https://www.raspberrypi.org/products/raspberry-pi-4-model-b/), and more. The command line offers you greater power and flexibility over your computing experience -- in ways that we will cover in this class and in ways that also exceed the scope of this class.\n",
      "## Where do I find the command line?\n",
      "Your command line will differ depending on the operating system (OS) that you're using. Most operating systems fall into two different families: the **Unix-like** family and the **Microsoft Windows** family.\n",
      "\n",
      "Mac OS and Linux (as well as Android and Chrome OS) are all part of the Unix-like family, which means that they all share a similar command line. We're going to focus on Mac OS and Microsoft Windows OS because they represent the two big families and because they're the operating systems that you're most likely to be using. \n",
      "## Mac OS\n",
      "If you're using a Mac, you will access the command line through an application called \"Terminal.\" You can find \"Terminal\" under Applications -> Utilities.\n",
      "<img src=\"../images/command-line/Terminal-Applications-Utilities.png\" alt=\"Open Mac Terminal in Applications\" width=\"100%\" border=2>\n",
      "Alternatively, you can find it by clicking Spotlight Search in the top right corner, typing \"Terminal,\" and then double-clicking the application or pressing enter. If you want to keep \"Terminal\" within even handier and speedier reach, you can keep it in your dock by right-clicking the application and selecting Options -> Keep in Dock. \n",
      "from IPython.display import IFrame\n",
      "IFrame(\"../videos/Terminal-Keep-In-Dock.mp4\", width='100%', height='400px')\n",
      "When you open Terminal, you'll see the name of your computer and your username followed by a dollar sign `$`. When I go to the command line, I see `Melanies-MacBook-Pro:~ melaniewalsh$`.\n",
      "\n",
      "You will type commands after the dollar sign `$`.\n",
      "## Windows OS\n",
      "The command line for Windows users, called \"Command Prompt,\" is different from the command line interface for Mac users and the wider Unix-like family. The Windows Command Prompt is not as powerful as the Unix shell in certain ways. However, beginning with Windows 7, Microsoft also introduced \"PowerShell,\" which acts like a more flexible and more powerful command line for Windows users. (You can read more about PowerShell in [Microsoft's official documentation](https://docs.microsoft.com/en-us/powershell/?view=powershell-6)).\n",
      "\n",
      "We're going to focus on PowerShell, as opposed to Command Prompt, for a number of reasons, but one very convenient reason is that PowerShell commands have \"aliases.\" They go by different names, that is, including names that are used by the traditional Windows Command Prompt and by a Mac Terminal.\n",
      "\n",
      ">For example, the PowerShell command that lists all the files and folders in a particular directory is `Get-ChildItem`. The traditional Windows Command Prompt command `dir` and the Unix command `ls` also perform the same function.\n",
      "\n",
      "These aliases will conveniently allow you to learn a little about Unix and Windows commands at the same time. Since we're mostly trying to understand how the command line functions at a broad level, this level of familiarity will suffice for now. In the future, you may want to invest more time in learning the specifics of PowerShell or the Command Prompt.\n",
      "<img src=\"https://www.isunshare.com/images/article/windows-10/5-ways-to-open-windows-powershell-in-windows-10/open-windows-powershell-by-search.png\" alt=\"Open Windows PowerShell\" width=\"100%\" border=2>\n",
      "\n",
      "Further resources: Google's [Windows Command-line Tools](https://developers.google.com/web/shows/ttt/series-2/windows-commandline)\n",
      "## Command Line Cheatsheet\n",
      "📂 = name of your desired directory path (e.g. `/Users/melaniewalsh/coding-project`) <br>\n",
      "📝 = name of your desired file (e.g. `Frankenstein.txt`)\n",
      "\n",
      "| Mac Terminal        | Explanation                                                                                                | Windows Command Prompt       | Windows PowerShell        |\n",
      "|:---------------------------:|:-----------------------------------------------------------------------------------------------------------:|:---------------------------:|:--------------------------------:|\n",
      "| `cd 📂`  | **c**hange **d**irectory, aka move into a different folder                                                | `cd 📂` | `cd 📂`      |\n",
      "| `ls`                      | **l**i**s**t the files and folders in your current **dir**ectory                                          | `dir`                     | `ls` / `dir` / `Get-ChildItem` |\n",
      "| `pwd`                     | show **p**ath of **w**orking **d**irectory, aka the folder that you're in right now                       | `cd`                      | `pwd` / `cd`                   |\n",
      "| `touch 📝`                   | make a new file                                                                                           |                           | `ni 📝`                           |\n",
      "| `mkdir 📂`                   | **m**a**k**e a new **dir**ectory, aka a folder                                                            | `mkdir 📂`                   | `mkdir 📂`                        |\n",
      "| `rm 📝`                      | **r**e**m**ove, aka delete, a file or directory                                                           | `del 📝`                     | `rm 📝` / `del 📝`                   |\n",
      "| `cp 📝(original) 📝(copy)`                      | **c**o**p**y a file or directory                                                                          | `copy 📝(original) 📝(copy)`                    | `cp` / `copy`                  |\n",
      "| `mv 📝(old) 📝(new)`                      | **m**o**v**e or rename a file or directory                                                                | `move 📝(old) 📝(new)` / `ren 📝`            | `move` / `ren `                 |\n",
      "| `cat 📝`                     | show all the contents of a file                                                                           |                           | `cat 📝` / `type 📝`                 |\n",
      "| `less 📝`                    | show snippet of a file that allows you to scroll through the entire thing                                 | `more 📝`                      | `more 📝`                           |\n",
      "| `head 📝`                    | show the first 10 lines of a file (change number of lines by adding `-*a number*` flag, e.g. `head -100`) |                           |    `gc 📝 -head 10`                           |\n",
      "| `tail 📝`                    | show the last 10 lines of a file (change number of lines by adding `-*a number*` flag, e.g. `tail -100`)  |                           | `gc 📝 -tail 10`                                  |\n",
      "| `wc -w -l 📝`                      | show how many **w**ords or lines in a file                            |                           | `gc 📝 \\| Measure-Object -Word –Line`                               |\n",
      "| `man **a command**`                     | show the **man**ual, aka the documentation that tells you what a particular command does                  | `help`                    | `help`                         |\n",
      "| `echo`                    | print text to the command line                                                                            | `echo`                    | `echo`                         |\n",
      "| `grep 📝 or 📂`                    |                                                                                                           | `findstr 📝`                 | `findstr 📝`                      |\n",
      "| `curl -O 📝` or `wget 📝`                    |  **get**, a file from the **w**eb                                                            |                           | `wget 📝 (file from web) -OutFile 📝 (new file name)`         |\n",
      "\n",
      "## Command Line Walk-Through\n",
      "*Note: the `%` and `!` symbols at the beginning of the lines below allow us to access the command line from a Jupyter notebook. If you were using these commands outside the Jupyter environemnt, you would not need to include `%` or `!`*\n",
      "## Basic Navigation and Directory Structure\n",
      "⭐ *You are here!* ⭐ One of the most basic and useful commands is the one that tells you where you are on your computer. Type `pwd` and it will show you the **p**ath of your **w**orking **d**irectory, aka the folder that you're currently in. This might not seem terribly important at the moment. But it is!\n",
      "### Display Path of Working Directory\n",
      "%pwd\n",
      "The output above tells me that I'm in a directory (aka folder) called \"Command Line\" inside another directory called \"Website-Content\" inside another directory called \"Intro-Cultural-Analytics\" (which holds all the files and code for this website) inside another directory called \"melaniewalsh\" inside another directory called \"Users.\" The forward slash `/` before and after each name indicates that the name is a directory (on Windows computers directories are indicated by backslashes `\\`).\n",
      "\n",
      "This path is unique to my personal machine, which you could probably guess because it includes my name. If you run `pwd` live on this website or in a Jupyter notebook in the cloud, it should return `/home/joyvan/content/Command-Line` because `/home/joyvan/` is the \"home\" location of this notebook in the cloud. If you open up a command line on your own personal machine and type `pwd`, it should return your personal \"home\" directory. The \"home\" location on a Mac will be `/Users/your-specific-username`.\n",
      "### List Files and Folders\n",
      "%ls\n",
      "If you want to see what's inside the directory that you're currently in, you can run `ls`, which **l**i**s**t all the files and folders. The output above shows that this directory contains \"The-Command-Line.ipynb\" (this very Jupyter notebook!) as well as another directory called \"very-interesting-folder.\" Hmm it sounds so interesting! Don't you want to find out what's inside?\n",
      "\n",
      "If we want to find out what's inside that directory, we can move into it by using the **c**hange **d**irectory `cd` command and plugging in the name of the directory `very-interesting-folder`.\n",
      "### Change Directory\n",
      "%cd very-interesting-folder/\n",
      "%ls\n",
      "What's inside here? Hmmm some facts. We'll learn how to check out what's written inside the file below. But first...\n",
      "### Relative Path vs Absolute Path\n",
      "There are two ways to to tell your computer where you want to go AKA your desired *path*:\n",
      "\n",
      "* Relative path depends on your current location, often a short or shortened way of writing the path\n",
      "* Absolute path is the full and \"true\" location from the bird's eye view of your home directory, often much longer\n",
      "\n",
      "We used a ***relative*** path when we moved into \"very-interesting-folder\" because we simply typed \"very-interesting-folder\". We were already inside \"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Command-Line/\" so that's all the information we needed.\n",
      "\n",
      "If we wanted to move inside \"very-interesting-folder\" from a completely different place on our computers, we could use an ***absolute*** path: \"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Command-Line/very-interesting-folder\"\n",
      "%cd /Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/Command-Line/very-interesting-folder\n",
      "%cd ~/Website-Content/Command-Line/very-interesting-folder\n",
      "It's very common to move up through your directory structure with a relative path, which can you do with two periods `..`\n",
      "*Moving*\n",
      "%cd ..\n",
      "*on*\n",
      "%cd ..\n",
      "*up!*\n",
      "%cd ..\n",
      "%cd ~/Intro-Cultural-Analytics/Website-Content/Command-Line/very-interesting-folder\n",
      "Rather than listing the files and folders inside my present working directory `very-interesting-folder`, I can list the ones in the directory above me:\n",
      "%ls ../\n",
      "Most of our text files in this class live one directory above the Jupyter notebooks, so we're often going to be referencing them by writing \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "\n",
      "* Relative path \"../texts/music/Beyonce-Lemonade.txt\"\n",
      "* Absolute path \"/Users/melaniewalsh/Intro-Cultural-Analytics/Website-Content/texts/music/Beyonce-Lemonade.txt` or `/home/joyvan/Website-Content/texts/music/Beyonce-Lemonade.txt\"\n",
      "## Working with Files and Texts\n",
      "There's a lot you can do with files and texts from the command line!\n",
      "### Download a text file from the web\n",
      "[Project Gutenberg](https://www.gutenberg.org/ebooks/search/) is an amazing resource that hosts tens of thousands of free texts in the public domain. Let's go find \"The Yellow Wallpaper.\"\n",
      "!curl -O http://www.gutenberg.org/files/1952/1952-0.txt\n",
      "!wget http://www.gutenberg.org/files/1952/1952-0.txt\n",
      "#PS C:\\Users> wget http://www.gutenberg.org/files/1952/1952-0.txt -OutFile The-Yellow-Wallpaper.txt\n",
      "%ls\n",
      "### Rename a file\n",
      "%mv 1952-0.txt The-Yellow-Wallpaper.txt\n",
      "%ls\n",
      "### Check file sizes, lengths, and word counts\n",
      "The `-lh` flag shows you, among other things, the sizes of the files in your directory, which is very handy. \n",
      "%ls -lh\n",
      "`The-Yellow-Wallpaper.txt` is 50 kilobytes, and `an-interesting-fact` is 214 byes (tiny!).\n",
      "Let's check how many words are in \"The Yellow Wallpaper.\"\n",
      "!wc -w The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt | Measure-Object –Word\n",
      "Let's check how many lines are in \"The Yellow Wallpaper.\"\n",
      "!wc -l The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt | Measure-Object –Line\n",
      "### Display the contents of a file\n",
      "The `cat` command prints the whole file to your command line.\n",
      "!cat The-Yellow-Wallpaper.txt\n",
      "That's often too much text to handle at one time. So you can also look at the first 10 lines of the file with `head`.\n",
      "!head The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt -head 10\n",
      "Or by adding a flag with a number (`-50`), you can look at the first however many lines you want!\n",
      "!head -50 The-Yellow-Wallpaper.txt\n",
      "The same goes for looking at the end of a file with `tail`\n",
      "!tail The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> gc The-Yellow-Wallpaper.txt -tail 10\n",
      "### Searching inside files\n",
      "!grep \"yellow\" -n The-Yellow-Wallpaper.txt\n",
      "#PS C:\\Users> findstr -n \"yellow\" The-Yellow-Wallpaper.txt\n",
      "!grep \"yellow\" -n --color The-Yellow-Wallpaper.txt\n",
      "!grep -wc \"yellow\" The-Yellow-Wallpaper.txt\n",
      "!grep -wc \"paper\" The-Yellow-Wallpaper.txt\n",
      "!grep \"wallpaper\" -B 2 -A 2 -n --color The-Yellow-Wallpaper.txt\n",
      "___\n",
      "## Further Resources\n",
      "* The Launch School's [Introduction to the Command Line](https://launchschool.com/books/command_line/read/introduction)\n",
      "* DHRI's [\"Introduction to the Command Line\"](https://github.com/DHRI-Curriculum/command-line)\n",
      "## HW 4 – Practice with Pandas\n",
      "[Download relevant files here](https://melaniewalsh.org/Pandas.zip)\n",
      "Follow the instructions in the assignment below. Then save your file as \"Your-Last-Name-HW-4-Pandas.ipynb\" and submit it to Canvas by Tuesday at 9am.\n",
      "<img src=\"../images/Intra-American.png\" width=100%, border=2>\n",
      ">Computation\n",
      "could not, it seemed, capture the violent quandary that was the nation’s\n",
      "history of and relationship to human bondage. Contemporary encounters\n",
      "with digital technology have inherited this tension, with researchers\n",
      "struggling to appreciate the inhumanity of bondage and the attendant\n",
      "dehumanization of black lives while also responding to the need for critical,\n",
      "rigorous, and engaged histories of slavery as histories of the present (18).\n",
      "\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson, [\"Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads\"](https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death)\n",
      "The dataset that you're going to be working with in this homework assignment is taken from [The Intra-American Slave Trade Database](https://www.slavevoyages.org/american/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "## Required Reading\n",
      "Before beginning this homework assignment, read through the short [\"Introduction\"](https://www.slavevoyages.org/american/about) to the Intra-American Slave Trade Database as well as the post [\"Voyages and Applied History\"](https://www.slavevoyages.org/voyage/essays). You will return to answer some questions about these readings at the end of the assignment.\n",
      "## Import Pandas\n",
      "**1.**\n",
      "# Your Code Here\n",
      "## Read in CSV File\n",
      "**2.**\n",
      "Make a dataframe called `slave_intra_american_df` and read in the CSV file \"Slave-Voyages-Intra-American-North-America.csv\". Note that the rows in this CSV file are separated by tabs `\\t`, not commas. They will thus require a different delimiter. \n",
      "# Your Code Here\n",
      "## Display the Data\n",
      "**3.**\n",
      "Display the first 15 rows of the data.\n",
      "# Your Code Here\n",
      "Look at a random sample of 7 rows.\n",
      "# Your Code Here\n",
      "## Examine the Data\n",
      "**4.**\n",
      "# Your Code Here\n",
      "**How many rows does this dataset include?**\n",
      "**#** Type Your Answer Here\n",
      "Check the data types of the columns in your dataset\n",
      "# Your Code Here\n",
      "## Rename Column\n",
      "**5.**\n",
      "Rename the \"flag\" column as \"national_affiliation.\" Don't forget to assign it to your original `slave_intra_american_df` dataframe so that the renamed column is saved. Display the first 5 rows of the `slave_intra_american_df` to check your work.\n",
      "# Your Code Here\n",
      "# Your Code Here\n",
      "## Select and Count Columns\n",
      "**6.**\n",
      "Select and display the column \"place_of_slave_disembarkation\"\n",
      "# Your Code Here\n",
      "**7.**\n",
      "Count the values in the column \"place_of_slave_disembarkation\"\n",
      "# Your Code Here\n",
      "## Filter Data\n",
      "**8.**\n",
      "Filter the dataset and display only the rows that included \"New York\" as their \"place_of_slave_disembarkation\". Save this data into a new variable called `new_york`\n",
      "# Your Code Here\n",
      "# Your Code Here\n",
      "## Sort Data\n",
      "**9.**\n",
      "Sort your smaller `new_york` dataframe by the \"year_of_arrival\", from the latest date to the earliest date\n",
      "# Your Code Here\n",
      "## Groupby and Plot\n",
      "**10.**\n",
      "Calculate how many enslaved people were transported by each nation. Group your `new_york` dataframe by \"national_affiliation\" and then calculate the sum of \"total_disembarked\". Then save this data into a variable called `national_totals`\n",
      "# Your Code Here\n",
      "# Your Code Here\n",
      "**11.**\n",
      "Make a bar chart out of the `national_totals` data\n",
      "# Your Code Here\n",
      "## Think Intentionally With The Data\n",
      "**12.**\n",
      "Re-read or re-skim the short [\"Introduction\"](https://www.slavevoyages.org/american/about) to the Intra-American Slave Trade Database as well as the post [\"Voyages and Applied History\"](https://www.slavevoyages.org/voyage/essays). Drawing on these readings and your experience working with the data above, write a 100-word response about how you or another researcher might use this data *intentionally* — in a way that resists the dehumanizing quantification that created it in the first place and contributes to efforts toward justice.\n",
      "**#** Type Your 100-Word Response Here\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "...\n",
      "## HW 3 (Part II) — Lists & For Loops\n",
      "For this HW assignment, we're again going to draw on Anelise Shrout's [Bellevue Almshouse data](https://docs.google.com/spreadsheets/d/1uf8uaqicknrn0a6STWrVfVMScQQMtzYf5I_QyhB9r7I/edit#gid=2057113261). For help with this assignment, refer to [Lists & For Loops](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html). Save your file as \"Your-Last-Name-HW-3-Lists-Loops.ipynb\" and submit it to Canvas.\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Make a list that contains each of the above Irish immigrants' professions and assign to a variable called `professions`\n",
      "#Your Code Here\n",
      "**2.** Extract the second item in the list `professions`. Hint: remember how the Python index works!\n",
      "#Your Code Here\n",
      "**3.** Add the item \"spinster\" to your `professions` list, then print the list.\n",
      "#Your Code Here\n",
      "#Your Code Here\n",
      "**4.** Make a `for` loop that considers each item in the `professions` list and prints \"Person's profession is ___\"\n",
      "#Your Code Here\n",
      "    #Your Code Here\n",
      "**5.** Make a list that contains each of the above Irish immigrants' child statuses and assign to a variable called `child_status`. You can make Margaret Farrell's child status an empty string `''`.\n",
      "#Your Code Here\n",
      "**6.** Extract the third item in the list.\n",
      "#Your Code Here\n",
      "**7.** Make a `for` loop that considers each item in the `child_status` list and prints \"Person has child\" if the person has a child and \"Person does not have child\" if not\n",
      "#Your Code Here\n",
      "  #Your Code Here\n",
      "       #Your Code Here\n",
      "    #Your Code Here\n",
      "        #Your Code Here\n",
      "**8.** Make a list that contains each of the above Irish immigrants' genders and assign to a variable called `gender`\n",
      "#Your Code Here\n",
      "**9.** Add an item to the list called \"not known\"\n",
      "#Your Code Here\n",
      "**10.** Make a `for` loop that considers each item in the `gender` list and prints \"Person is male\" if the person is male, \"Person is female\" if the person is female, and \"Person's gender is not known\" if unknown\n",
      " #Your Code Here\n",
      "     #Your Code Here\n",
      "         #Your Code Here\n",
      "     #Your Code Here\n",
      "         #Your Code Here\n",
      "     #Your Code Here\n",
      "         #Your Code Here\n",
      "## HW 9 — Topic Modeling\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 5 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-9.ipynb\" and submit it to Canvas by Friday, April 17th at 5pm.\n",
      "\n",
      "*This notebook is exactly the same as the [\"Topic Modeling-Text Files\" lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Text-Files.html), and submitting either notebook is fine.\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model 378 obituaries published by *The New York Times*. \n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Note: \"uncomment\" means delete the initial `#`*\n",
      "*Note: the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "Before we topic model the *NYT* obituaries, we need to process the text files and prepare them for analysis. The steps below demonstrate how to process texts if your corpus is a collection of separate text files. In the next lesson, we'll demonstrate how to process texts that come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "files\n",
      "## Process Texts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual text file, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very common Python pattern! We make an empty list `training_data = []`, then we use a `for` loop to iterate through every file path in the list of file paths, then we `open()` and `.read()` each text file associated with that file path, then we processes `little_mallet_wrapper.process_string()` the text, and finally we `.append()` the processed text to our master list.*\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "Here we extract the relevant part of each file name by using [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem), which conveniently extracts just the last part of the file path without the \".txt\" file extension. Because each file name includes the obituary subject's name as well as the year that the subject died, we're going to use this information as a title or label for each obituary.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very simple list comprehension!*\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "obit_titles\n",
      "## Get Training Data Stats\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "According to this little report, we have 378 documents (or obituaries) that average 1345 words in length.\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed obituary texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Topic Model Output Files\n",
      "Finally, we need to tell Little MALLET Wrapper where to find and output all of our topic modeling results. The code below will set Little MALLET Wrapper up to output your results inside a directory called \"topic-model-output\" and a subdirectory called \"NYT-Obits\", all of which will be inside your current directory.\n",
      "\n",
      "If you'd like to change this output location, simply change `output_directory_path` below.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/NYT-Obits'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "Now we import our training data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Finally, we train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 45 seconds to 1 minute to fully train and complete. If you want, you can look at your Terminal or PowerShell while it's running and see what the model looks like as it trains.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "When the topic model finishes, it will output your results to your `output_directory_path`.\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the *NYT* obituaries, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "\n",
      ">*Take a minute to read through every topic. Reflect on what each topic seems to capture as well as how well you think the topics capture the broad themes of the entire collection. Note any oddities, outliers, or inconsistencies.*\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single obituary in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific obituaries.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "If we look at the 32nd topic distribution in this list of `topic_distributions`, which corresponds to Marilyn Monroe's obituary, we will see a list of 15 probabilities. This  list corresponds to the likelihood that each of the 15 topics exists in Marilyn Monroe's obituary.\n",
      "topic_distributions[32]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 0 \"miss film theater movie broadway films\" has a relatively high probability of existing in Marilyn Monroe's obituary `.202` while Topic 5 \"soviet hitler german germany stalin union\" has a relatively low probability `.002`. This seems to comport with what we know about Marilyn Monroe.\n",
      "obituary_to_check = \"1962-Marilyn-Monroe\"\n",
      "\n",
      "obit_number = obit_titles.index(obituary_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {obit_titles[obit_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[obit_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "The darker squares in this heatmap represent a high probability for the corresponding topic (compared to everyone else in the heatmap) and the lighter squares in the heatmap represent a low probability for the corresponding topic. For example, if you scan across the row of Marilyn Monroe, you can see a dark square for the topic \"miss film theater movie theater broadway\". If you scan across the row of Ada Lovelace, an English mathematician who is now recognized as the first computer programmer, according to her [NYT obituary](https://www.nytimes.com/interactive/2018/obituaries/overlooked-ada-lovelace.html), you can see a dark square for \"university professor research science also\".\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the obituaries that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "\n",
      "Because most of the obituaries in our corpus are pretty long, however, it will be more useful for us to simply display the title of each obituary, rather than the entire document—at least as a first step. To do so, we'll first need to make two dictionaries, which will allow us to find the corresponding obituary title and the original text from a given training document.\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "Then we'll make our own function `display_top_titles_per_topic()` that will display the top text titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "**Topic 0 Label**: Hollywood\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Hollywood.\"\n",
      "**Topic 9**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "**Topic 9 Label**: Global Affairs\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Global Affairs.\"\n",
      "**Topic 8**\n",
      "To display the top 7 obituaries with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=8, number_of_documents=7)\n",
      "**Topic 8 Label**: Authors\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Authors.\"\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original obituary texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "**Topic 8**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=8, number_of_documents=3)\n",
      "## Your Turn!\n",
      "Choose a topic from the results above and write down its corresponding topic number below.\n",
      "**Topic: *Your Number Choice Here***\n",
      "**1.** Display the top 6 obituary titles for this topic.\n",
      "#Your Code Here\n",
      "**2.** Display the topic words in the context of the original obituary for these 6 top titles.\n",
      "#Your Code Here\n",
      "**3.** Come up with a label for your topic and write it below:\n",
      "**Topic Label: *Your Label Here***\n",
      "**Reflection**\n",
      "**4.** Why did you label your topic the way you did? What do you think this topic means in the context of all the *NYT* obituaries?\n",
      "**#**Your answer here\n",
      "**5.** What's another collection of texts that you think might be interesting to topic model? Why?\n",
      "**#**Your answer here\n",
      "## HW 10 — Named Entity Recognition\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 4 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-10.ipynb\" and submit it to Canvas by Friday, April 23rd at 5pm.\n",
      "\n",
      "*This notebook is exactly the same as the [\"Named Entity Recognition\" lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Named-Entity-Recognition.html), and submitting either notebook is fine.\n",
      "In this lesson, we're going to learn about a text analysis method called **Named Entity Recognition** (NER). This method will help us computationally identify people, places, and things (of various kinds) in a text or collection of texts.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\" width=\"100%\", border=2>\n",
      "## Why is NER Useful?\n",
      "Named Entity Recognition is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters (something we'll do in a later lesson!). Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping the locations (something we'll also do in a later lesson!).\n",
      "## Natural Language Processing (NLP)\n",
      "Named Entity Recognition is a fundamental task in the field of *natural language processing* (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called *spellcheck*? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!\n",
      "\n",
      "Thanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts are now getting eerily good. (If you don't believe me, check out [this app that will autocomplete your sentences](https://transformer.huggingface.co/doc/gpt2-large/qCNMTfzephfZMBkryTNvSRKQ/edit) with GPT-2, a state-of-the-art text generation model. When I ran it, the model generated a mini-lecture from a \"university professor\" that sounds spookily close to home...)\n",
      "<img src=\"../images/GPT-2.png\", border=2>\n",
      "Open-source NLP tools are getting very good, too. We're going to use one of these open-source tools, the Python library `spaCy`, for our Named Entity Recognition tasks in this lesson.\n",
      "## How spaCy Works\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "The screenshot above shows spaCy correctly identifying named entities in Ada Lovelace's *New York Times* obituary (something that we'll test out for ourselves below). How does spaCy know that \"Ada Lovelace\" is a person and that \"1843\" is a date?\n",
      "Well, spaCy doesn't *know*, not for sure anyway. Instead, spaCy is making a very educated guess. This \"guess\" is based on what spaCy has learned about the English language after seeing lots of other examples.\n",
      "That's a colloquial way of saying: spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand). This is similar to our <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html#1)-LDA-is-an-Unsupervised-Algorithm\">topic modeling work</a> from the previous lesson, except our topic model wasn't using labeled data.\n",
      "\n",
      "The English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n",
      "\n",
      "When spaCy identifies people and places in Ada Lovelace's obituary, in other words, its NLP model is actually making a series of *predictions* about the text based on what it has learned about how people and places function in English-language sentences.\n",
      "## NER with spaCy\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things, and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "`document = nlp(open(filepath, , encoding='utf-8').read())`\n",
      "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
      "\n",
      "In the cell below, we `open()` and `.read()` Ada Lovelace's obituary. Then we run`nlp()` on the text and create our `document`.\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "\n",
      "document = nlp(open(filepath, encoding='utf-8').read())\n",
      "## spaCy Named Entities\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "Above is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels. To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):\n",
      "displacy.render(document, style=\"ent\")\n",
      "From a quick glance at the text above, we can see that spaCy is doing quite well with NER. But it's definitely not perfect.\n",
      "\n",
      "Though spaCy correctly identifies \"Ada Lovelace\" as a `PERSON` in the first sentence, just a few sentences later it labels her as a `WORK_OF_ART`. Though spaCy correctly identifies \"London\" as a place `GPE` a few paragraphs down, it incorrectly identifies \"Jacquard\" as a place `GPE`, too (when really \"Jacquard\" is a type of loom, named after [Marie Jacquard](https://en.wikipedia.org/wiki/Jacquard_machine)). \n",
      "\n",
      "This inconsistency is very important to note and keep in mind. If we wanted to use spaCy's NER for a project, it would almost certainly require manual correction and cleaning. And even then it wouldn't be perfect. That's why understanding the limitations of this tool is so crucial. While spaCy's NER can be very good for identifying entities in broad strokes, it can't be relied upon for anything exact and fine-grained — not out of the box anyway.\n",
      "## Get Named Entities\n",
      "All the named entities in our `document` can be found in the `document.ents` property. If we check out `document.ents`, we can see all the entities from Ada Lovelace's obituary.\n",
      "document.ents\n",
      "Each of the named entities in `document.ents` contains [more information about itself](https://spacy.io/usage/linguistic-features#accessing), which we can access by iterating through the `document.ents` with a simple `for` loop. `For` each `named_entity` in `document.ents`, we will extract the `named_entity` and its corresponding `named_entity.label_`.\n",
      "for named_entity in document.ents:\n",
      "    print(named_entity, named_entity.label_)\n",
      "To extract just the named entities that have been identified as `PERSON`, we can add a simple `if` statement into the mix:\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        print(named_entity)\n",
      "## Practicing with *Lost in the City*\n",
      "For the rest of this lesson, we're going to work with Edward P. Jones's short story collection *Lost in the City*.\n",
      "<img src=\"https://mybinder.org/static/images/logo_social.png\" width=\"150\" align=\"left\", border=2> *If you're using this Jupyter notebook in Binder (in the cloud), please uncomment the cell below and work with only the first story from _Lost in the City_. The Binder notebook is currently having issues loading the entire collection.*\n",
      "#file = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "#document = nlp(open(file).read())\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "document = nlp(open(filepath, encoding=\"utf-8\").read())\n",
      "## Get People\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "To extract and count the people identified in *Lost in the City*, we will follow the same model as above, using an `if` statement that will pull out words only if their \"ent\" label matches \"PERSON.\"\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">*While we demonstrate how to extract named entities in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.* \n",
      "Here's the code all together:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "        \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "Here's the code broken up. We make a list of all the people identified in *Lost in the City*:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "people\n",
      "Then we count the unique people in this list with the `Counter()` module:\n",
      "people_tally = Counter(people)\n",
      "people_tally.most_common()\n",
      "Then we make a dataframe from this list with `pd.DataFrame()`:\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "To write this dataframe (or any dataframe!) to a CSV file, we can use `df.to_csv()`. To create a CSV file of character counts, uncomment the cell below:\n",
      "#df.to_csv(\"Lost-in-the-City-characters.csv\", encoding='utf-8', index=False)\n",
      "## Get Places\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "To extract and count places, we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"GPE\" or \"LOC.\" These are the type labels for \"counties cities, states\" and \"locations, mountain ranges, bodies of water.\"\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"GPE\" or named_entity.label_ == \"LOC\":\n",
      "        places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "Do you notice anything off about this list...?\n",
      "## Get Streets & Parks\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "To extract and count streets and parks (which show up a lot in *Lost in the City*!), we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"FAC.\" This is the type label for \"buildings, airports, highways, bridges, etc.\"\n",
      "streets = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"FAC\":\n",
      "        streets.append(named_entity.text)\n",
      "\n",
      "streets_tally = Counter(streets)\n",
      "\n",
      "df = pd.DataFrame(streets_tally.most_common(), columns = ['street', 'count'])\n",
      "df\n",
      "## Get Works of Art\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the \"ent\" label \"WORK_OF_ART\").\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of named entities in a single line of code! Closely examine the first line of code below:\n",
      "works_of_art = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"WORK_OF_ART\"]\n",
      "\n",
      "art_tally = Counter(works_of_art)\n",
      "\n",
      "df = pd.DataFrame(art_tally.most_common(), columns = ['work_of_art', 'count'])\n",
      "df\n",
      "## Your Turn!\n",
      "Now it's your turn to take a crack at NER with a whole new text!\n",
      "\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "In this section, you're going to extract and count named entities from Barack Obama's memoir *The Audacity of Hope*. We're exploring Obama's memoir because it's chock full of named entities.\n",
      "Read in and process the text file\n",
      "file = \"../texts/literature/Obama-The-Audacity-of-Hope.txt\"\n",
      "\n",
      "document = nlp(open(file, encoding='utf-8').read())\n",
      "**1.** Choose a named entity from the possible spaCy named entities listed above. Extract, count, and make a dataframe from the most frequent named entities (of the type that you've chosen) in *The Audacity of Hope*. If you need help, study the examples above.\n",
      "#Your Code Here 👇 \n",
      "\n",
      "**2.** What is a result from this NER extraction that conformed to your expectations, that you find obvious or predictable? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**3.** What is a result from this NER extraction that defied your expectations, that you find curious or counterintuitive? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**4.** What's an insight that you might be able to glean about *The Audacity of Hope* based on your NER extraction?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "## HW 3 (Part I) — Conditionals and Comparisons\n",
      "[Download this file and Bellevue Almshouse data (not necessary but optional to explore)](https://melaniewalsh.org/Conditionals-Comparisons.zip)\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's [Bellevue Almshouse data](https://www.nyuirish.net/almshouse/the-almshouse-records/). For help with this assignment, refer to [Conditionals & Comparisons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Conditionals-Comparisons.html). Save your file as \"Your-Last-Name-HW-3-Conditionals-Comparisons.ipynb\" and submit it to Canvas.\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "We're going to make a series of variables and assign them values based on the Bellevue Almshouse dataset. Make sure you run these cells.\n",
      "### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Write an `if` statement that reports whether `person1_age` is less than 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "**2.** Write an `if` statement that reports whether `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "**3.** Write an `if` statement that reports whether `person1_age` is less than 30 years old *and* `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old and married.')\n",
      "**4.** Complicate your `if` statement from Question 1 by adding an `else` statement that prints \"Person is older than 30 years old\". Then evaluate whether `person3_age` is less than 30 years old. \n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**5.** Now evaluate whether `person4_age` is less than 30 years using the same code as Question 4.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "Hmmm, with the code as written, it's telling us that Margaret Farrell, who is 30 years old, is *more* than 30 years old. Add an `elif` statement that reports whether the person is exactly 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is exactly 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**6.** Write an `if` statement that will report whether `person1_child_status` includes children.\n",
      "#Your code here\n",
      "    print('Person has children.')\n",
      "**7.** Write one `if` statement that will accurately report whether `person1_child_status` includes children and, separately, if `person2_child_status` includes children. (Hint: think about how you might use the `!=` operator.)\n",
      "if person1_child_status #Your Code Here\n",
      "    print('Person has children.')\n",
      "if person2_child_status #Same Code Here\n",
      "    print('Person has children.')\n",
      "**8.** Write a conditional that will report whether `person1_profession` is \"married,\" \"laborer,\" \"widow,\" or \"unknown profession.\" Test your code by reassigning the variable as indicated below.\n",
      "person1_profession = 'married'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'laborer'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'widow'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'student'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "**9.** Some of the Irish immigrants' names have question marks after them. Let's clean up some of the data and remove the question marks. You can use the Python keyword `in` to test whether a string appears within another string. Print `person2_name` with the question mark and parentheses removed. (Hint: think about f-strings and string methods!)\n",
      "if \"(?)\" in person2_name:\n",
      "    #Your code here\n",
      "**10.** In a few sentences, write about your experiencing using and manipulating the Bellevue Almshouse data after reading Shrout's essay. How, if at all, did using this data influence your understanding of how Python works?\n",
      "**#Your thoughts here**\n",
      "## HW 7 — Twitter Data\n",
      "[Download relevant files here](https://melaniewalsh.org/HW-7-Twitter-Data.zip)\n",
      "Follow the instructions in the assignment below. When you're finished, save this file as \"Your-Last-Name-HW-7.ipynb\" and submit it to Canvas by Tuesday, April 7th at 9am.\n",
      "## Select a Tweet Dataset\n",
      "## Create Your Own Tweet Dataset (Optional) \n",
      "If you want, you can create your own tweet dataset for this assignment. However, this part of the assignment is no longer required.\n",
      "Choose your own search term or query. Collect at least 500 tweets that contain this query.\n",
      "!#Your Code Here\n",
      "Count how many tweets you collected and make sure it's over 500.\n",
      "!#Your Code Here\n",
      "Why did you select this search term or query?\n",
      "**#**Your Answer Here\n",
      "Convert your Twitter JSON file into a CSV file using the twarc utility `json2csv.py`.\n",
      "!#Your Code Here\n",
      "## Pick a Sample Tweet Dataset\n",
      "If you don't create your own tweet dataset, you can choose from two of the tweet datasets provided in the \"HW 7\" zip file.\n",
      "\n",
      "* touch_my_face_tweets.csv — 685 tweets that included the general phrase \"touch my face\" (most responding to the coronavirus-related health recommendation that people not touch their faces) as well as received more than 5 retweets\n",
      "\n",
      "* not_with_bang_tweets.csv — 576 tweets that included the exact phrase \"not with a bang but with a\", which is a phrase that comes from the conclusion of T.S. Eliot's 1925 poem \"The Hollow Men\": This is the way the world ends / Not with a bang but with a whimper.\"\n",
      "## (Required) Read in as Pandas DataFrame\n",
      "**1.** Import Pandas and optionally [set custom pandas display options](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Analysis.html#Read-in-Tweet-CSV-files-with-Pandas)\n",
      "#Your Code Here\n",
      "#Optional Code Here\n",
      "#Optional Code Here\n",
      "#Optional Code Here\n",
      "Read in your tweets CSV file as a Pandas dataframe and name it something other than `your_df`.\n",
      "your_df = #Your Code Here\n",
      "## Filter the DataFrame\n",
      "**2.** Display all the column names in your dataframe.\n",
      "#Your Code Here\n",
      "Filter your dataframe to only 10 columns. Pick which columns you want to include. Make sure you include \"text\" and \"retweet_count.\" (When you run the cell below, right-click and \"Enable Scrolling for Outputs\").\n",
      "#Your Code Here\n",
      "Save your filtered dataframe as `filtered_df`.\n",
      "#Your Code Here\n",
      "## Sort Your Twitter Data by Top Retweets\n",
      "**3.** Sort `filtered_df` by number of retweets, from largest to smallest. (Again make sure to \"Enable Scrolling for Outputs\").\n",
      "#Your code here\n",
      "What is the text of the most retweeted tweet in your dataset?\n",
      "**#**Your most retweeted tweet here\n",
      "## Examine Another Category\n",
      "**4.**  By using `.groupby()`, `.value_counts()`, and/or a filter (e.g. `df['flag']=='USA'`), examine another metadata category in your Twitter dataset.\n",
      "#Your code here\n",
      "What is a finding about your Twitter data that emerges from this calculation/sorting/filtering? Explain in at least a couple of sentences.\n",
      "**#**Your finding here\n",
      "## Overall Analysis and Future Work\n",
      "**5.** What, if anything, can you conclude about these tweets and Twitter users based on your analysis? Given unlimited time and resources, what steps or kinds of analysis would you undertake next? Discuss in a few or more sentences. \n",
      "**#**Your Answer Here\n",
      "## HW 2 — Variables and Data Types\n",
      "[Download necessary Jupyter notebooks and text files here](https://melaniewalsh.org/Intro-CA-Notebooks-V2.zip)\n",
      "Follow the instructions below. When you're finished, save your Jupyter notebook and then upload it to Canvas as an (.ipynb) file. Some of the cells below already have outputs, which will show you what the correct answer should yield.\n",
      "\n",
      "For help with this homework, refer to chapters [Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html), [Data Types](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Data-Types.html), and [String Methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html).\n",
      "**HEADS UP!**\n",
      "🚨 To run the code on this page, you need to run the two cells below first🚨\n",
      "moby_dick = open(\"../texts/literature/Moby-Dick.txt\", encoding=\"utf-8\").read()\n",
      "print(moby_dick )\n",
      "## Remixing Herman Melville's *Moby Dick*\n",
      "**1.** First you're going to extract the first few lines of *Moby Dick*. Slice the string `moby_dick` up to the 323rd character.\n",
      "#Your Code Here\n",
      "**2.** Make a variable called `moby_dick_beginning` and assign it the value of your sliced string up to the 323rd character.\n",
      "#Your Code Here\n",
      "**3.** Check the data type of your variable `moby_dick_beginning`.\n",
      "#Your Code Here\n",
      "**4.** Print the variable `moby_dick_beginning` \n",
      "#Your Code Here\n",
      "**5.** Replace the name \"Ishmael\" with a different name. Then assign the result to a new variable called `moby_dick_remix`.\n",
      "#Your Code Here\n",
      "#Your Code Here\n",
      "**6.** Make another variable called `question` and assign it the value `\"Wait who are you? What are you doing here??\\n\\n\"`\n",
      "\n",
      "(Remember that `\\n` means new line. We're just adding the `\\n`s to make the spacing nicer when we print it.)\n",
      "question = #Your Code Here\n",
      "**7.** Add the variables `question` and `moby_dick_remix` together, then print the result.\n",
      "#Your Code Here\n",
      "## Fact-checking with Herman Melville\n",
      "**8.** Go to Herman Melville's [Wikipedia page](https://en.wikipedia.org/wiki/Herman_Melville) and look up his name, birth year, death year, and occupation. Assign these values to the corresponding variables below. Make sure `name` and `occupation` are strings. Make sure `birth_year` and `death_year` are integers.\n",
      "name = #Your Code Here\n",
      "birth_year = #Your Code Here\n",
      "death_year = #Your Code Here\n",
      "occupation = #Your Code Here\n",
      "**9.** Now make one last variable called `age_at_death` and assign it the value of Melville's age at death. You have to calculate Melville's age at death by using your previously created variables `birth_year` and `death_year`.\n",
      "age_at_death = #Your Code Here\n",
      "**10.** Fill in the missing variable in this f-string and then print it.\n",
      "print(f\"{name} was a {occupation}. He was born in {birth_year} and died in {death_year}, which means that he was {MISSING VARIABLE} when he died.\")\n",
      "Example:\n",
      "## Debugging Practice\n",
      "**11.** Run the cell below. You should get an error message. Explain why you received this error message, then fix the code and run it.\n",
      "print(f\"{name} was a {occupation}. He was born in {birth_year} and died in {death_year}.)\n",
      "**Error explanation**:\n",
      "#Your Explanation Here\n",
      "#Your Fixed Code Here\n",
      "**12.**  Run the cell below. It shouldn't work exactly right. Explain what's wrong with it and then correct it.\n",
      "print(\"{name} was a {occupation}. He was born in {birth_year} and died in {death_year}.\")\n",
      "**Error explanation**:\n",
      "#Your Explanation Here\n",
      "#Your Fixed Code\n",
      "**13.** Try to calculate how old Herman Meville would be if he were alive today by running the two cells below. You should get an error message. Explain why you received this error message\n",
      "current_year = \"2020\"\n",
      "current_year - birth_year\n",
      "**Error explanation**:\n",
      "#Your Explanation Here\n",
      "#Your Fixed Code Here\n",
      "## Review \n",
      "### Command Line\n",
      "pwd\n",
      "**14.** Move one directory up from your current working directory.\n",
      "#Your Code Here\n",
      "**15.** Move back to where you started and list the files in your current working directory.\n",
      "#Your Code Here\n",
      "#Your Code Here\n",
      "## HW 6 — Twitter Collection Setup\n",
      "For this homework assignment, you're going to complete a number of necessary preparation steps in order to collect Twitter data, which will be our subject next week. Follow the instructions at [Twitter Collection Setup](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html) and answer the questions below.\n",
      "\n",
      "🚨 **Don't wait until the last minute for this assignment! Twitter needs to approve your application before you can complete all of the instructions, which can take as long as a day.**\n",
      "\n",
      "Please answer these questions in a plain text file (.txt), save the file as \"Your-Last-Name-HW-6.txt\", and submit it to Canvas by Tuesday, March 10th at 9am.\n",
      "**1.** Reflect on your experience of filling out a Twitter developer application and think about how it compares to the Genius API application process. Why do you you think the Twitter API application process is more involved? Why do you think they ask the specific questions that they do? Discuss in a few sentences.\n",
      "**2.**  Read through Twitter's Developer Agreement and briefly discuss one specific thing that surprises you about these terms. Why does it surprise you?\n",
      "**3.** Once you've gotten your Twitter API keys, you will install and configure twarc. If the configuration process has worked successfully, you will get a happy message on your command line that looks something like this:\n",
      "The credentials for mellymeldubs have been saved to your configuration file at /Users/melaniewalsh/.twarc\n",
      "\n",
      "✨ ✨ ✨ Happy twarcing! ✨ ✨ ✨\n",
      "Copy and paste this message straight from your command line into your HW 6 plain text file.\n",
      "## HW 5 — Functions and Pandas\n",
      "[Download this Jupyter notebook and relevant files here](https://melaniewalsh.org/HW-5.zip)\n",
      "This HW assignment draws on Hannah Andersen and Matt Daniels's \"Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age.\" By following the steps below, you're going to explore and analyze women's proportion of dialogue in film over time. When you're finished, save this file as \"Your-Last-Name-HW-5.ipynb\" and submit it to Canvas by Thursday, February 27th at 9am.\n",
      "**1.** Make a data biography ([inspired by Heather Krause](https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/)) for The Pudding's film dialogue data. Refer to [Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age](https://pudding.cool/2017/03/film-dialogue/) and [\"FAQ for the “Film Dialogue, By Gender” Project](https://medium.com/@matthew_daniels/faq-for-the-film-dialogue-by-gender-project-40078209f751). \n",
      "\n",
      "Remember that you can edit [Markdown](https://guides.github.com/features/mastering-markdown/) cells (like this one) by double-clicking on them and that you can change a cell [from Code to Markdown through the menu bar](../images/Markdown-to-Code.mov).\n",
      "\n",
      "\n",
      "**Where did the data come from? Include at least three specific sources.**\n",
      "**#**Your Text Here\n",
      "**Who collected the data?**\n",
      "**#**Your Text Here\n",
      "**How was the data collected? How was character \"gender\" calculated?**\n",
      "**#**Your Text Here\n",
      "***Why was the data collected?***\n",
      "**#**Your Text Here\n",
      "**2.** Use the code below to make a function called `decade_chunker()` with a parameter called `year` that returns a value called `decade`. The function `decade_chunker` should take in a year value and report what decade that year falls in. For help, refer to [Functions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Functions.html) and [Conditionals & Comparisons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Conditionals-Comparisons.html).\n",
      "#Your Code Here\n",
      "    decade = ''\n",
      "    if year >= 1940 and year < 1950:\n",
      "        decade = '1940s'\n",
      "    elif year >=  1950 and year < 1960:\n",
      "        decade = '1950s'\n",
      "    elif year >= 1960 and year < 1970:\n",
      "        decade = '1960s'\n",
      "    #Your Code Here\n",
      "        decade = '1970s'\n",
      "    elif year >= 1980 and year < 1990:\n",
      "        decade = '1980s'\n",
      "    elif year >= 1990 and year < 2000:\n",
      "        #Your Code Here\n",
      "    elif year >= 2000 and year < 2010:\n",
      "        decade = '2000s'\n",
      "    elif year >= 2010 and year < 2020:\n",
      "        decade = '2010s'\n",
      "    #Your Code Here\n",
      "Test the function `decade_chunker` by running `decade_chunker(1975)`.\n",
      "decade_chunker(1975)\n",
      "**3.**\n",
      "\n",
      "Import Pandas.\n",
      "#Your Code Here\n",
      "Read in the CSV file \"../data/Pudding-Film-Dialogue-Clean.csv\" as `pudding_film_df`. (That's where the data should be if you downloaded the zip file from our course website, but if it's not working, then you might have to re-direct it elsewhere.)\n",
      "#Your Code Here\n",
      "Display a random sample of 10 rows\n",
      "#Your Code Here\n",
      "**4.** Make a new column \"decade\" in `pudding_film_df` by applying the `decade_chunker()` function to the \"release_year\" column. For help, refer to [Applying Functions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Pandas/Pandas-EDA.html#Applying-Functions).\n",
      "#Your Code Here\n",
      "Disaply a random sample of 10 rows.\n",
      "pudding_film_df.sample(10)\n",
      "**5.** Filter the `pudding_film_df` dataframe so that it only includes characters who are women.\n",
      "#Your Code Here\n",
      "Save this dataframe as `women_film_df`.\n",
      "#Your Code Here\n",
      "Display a random sample of 10 rows from `women_film_df`.\n",
      "women_film_df.sample(10)\n",
      "**6.** Find out the top two women who have the highest proportion of dialogue in the entire dataset. Sort `women_film_df`  by \"proportion_of_dialogue\" from the highest proportion to the lowest proportion.\n",
      "#Your Code Here\n",
      "***Output display not included here because it gives away the answer\n",
      "The two women characters in this dataset with the highest proportion of film dialogue are:\n",
      "**1)** **#Your Answer Here** from the movie **#Your Answer Here** with approximately **#Your Answer Here** percent of the dialogue\n",
      "**2)** **#Your Answer Here** from the movie **#Your Answer Here** with approximately **#Your Answer Here** percent of the dialogue\n",
      "**7.** Let's find out how many women characters from each decade appear in this dataset. For `women_film_df`, count the values in the column \"decade\". For help, refer to [Count Values in Columns](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Pandas/Pandas.html#Count-Values-in-Columns). \n",
      "#Your Code Here\n",
      "**8.** Let's look at the percentage of women's dialogue in film across the decades. For `women_film_df`, groupby the column \"decade\" and then calculate the `.sum()` total of \"words\" spoken by women for each decade.\n",
      "#Your Code Here\n",
      "Save this groupby into a variable called `women_dialogue_per_decade`\n",
      "#Your Code Here\n",
      "**9.** The above information isn't very useful unless we compare it to the total number of words spoken across the decades. For `pudding_film_df`, groupby the column \"decade\" and then calculate the `.sum()` total of \"words\" spoken by all characters for each decade.\n",
      "#Your Code Here\n",
      "Save this groupby into a variable called `all_dialogue_per_decade`\n",
      "#Your Code Here\n",
      "**10.** Now divide `women_dialogue_per_decade` by `all_dialogue_per_decade`\n",
      "#Your Code Here\n",
      "Save this calculation into a variable called `women_dialogue_across_decades`\n",
      "#Your Code Here\n",
      "**11.**\n",
      "Make a bar plot of `women_dialogue_across_decades`\n",
      "#Your Code Here\n",
      "## Reflection and Analysis\n",
      "Answer the following questions in a few or more sentences.\n",
      "**12.** What, if anything, can you conclude based on the percentage of women's dialogue across the decades shown in this data — regarding film, the film industry, gender, society and culture, etc.?\n",
      "**#**Your Answer Here\n",
      "**13.** What are some of the limitations of this dataset?\n",
      "**#**Your Answer Here\n",
      "**14.** If you had unlimited time and technical resources, what would be another research question that you would want to pursue with this data or similar data?\n",
      "**#**Your Answer Here\n",
      "**15.** What's another cultural dataset that you could imagine yourself using Pandas with in the future?\n",
      "**#**Your Answer Here\n",
      "## HW 8 — Term Frequency–Inverse Document Frequency\n",
      "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip)\n",
      "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 3 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-8.ipynb\" and submit it to Canvas by Friday, April 9th at 5pm.\n",
      "\n",
      "*This notebook is exactly the same as [the TF-IDF lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/TF-IDF.html), and submitting either notebook is fine.\n",
      "In this lesson, we're going to learn about a text analysis method called **term frequency–inverse document frequency** (tf–idf). This method will help us identify the most unique words in a document from a given corpus. \n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "## Why is tf–idf Useful?\n",
      "\n",
      "## The Basic Math\n",
      "> `term_frequency * inverse_document_frequency`\n",
      "## Breaking Down the Formula\n",
      "\n",
      "> `term_frequency = number of times a given word appears in story or text`\n",
      "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n",
      "\n",
      "> `total_number_of_documents / number_of_documents_with_term`\n",
      "\n",
      "...the result of which we're going to take the logarithm of and then add 1\n",
      "\n",
      "> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n",
      "\n",
      "Do you see how if we flipped the fraction — making it `number_of_documents_with_term /  total_number_of_documents`— that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\"\n",
      "## The Formula in Action\n",
      "**\"said\" vs \"pigeons\"**\n",
      "Using this formula, we're going to calculate and compare the tf–idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*.\n",
      "We need the log() function for our calculation, so we're going to import it from the `math` package.\n",
      "from math import log\n",
      "**\"said\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 13 #number of short stories the contain the word \"said\"\n",
      "term_frequency = 47 #number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**\"pigeons\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 2 #number of short stories the contain the word \"pigeons\"\n",
      "term_frequency = 30 #number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**tf–idf Scores**\n",
      "\n",
      "\"said\" = 50.48<br>\n",
      "\"pigeons\" = 88.38\n",
      "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf–idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them.\n",
      "## tf–idf with scikit-learn\n",
      "## Import Libraries\n",
      "We could continue calculating tf–idf scores in this manner — by doing all the math with Python — but conveniently there's a Python library that can calculate tf–idf scores in just a few lines of code.\n",
      "\n",
      "This library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`.\n",
      "!pip install sklearn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 200)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*.\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "Let's display them to make sure they're correct:\n",
      "text_files, text_titles\n",
      "## Calculate Word Frequency (Optional Step)\n",
      "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf–idf scores, we can compare these two methods and see how tf–idf helps us find more unique words.\n",
      "\n",
      "(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)\n",
      "#Initialize CountVectorizer with desired parameters\n",
      "count_vectorizer= CountVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\n",
      "word_count_vector = count_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the word count vector and sort by title\n",
      "word_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\n",
      "word_count_df = word_count_df.sort_index()\n",
      "\n",
      "#Add column for number of times each word appears in all the documents\n",
      "word_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()\n",
      "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\").\n",
      "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words.\n",
      "word_count_df.sample(10, axis='columns')\n",
      "Let's zoom in on some specific words.\n",
      "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`\n",
      "def get_top_n_counts(dataframe, top_n=10):\n",
      "    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words.\n",
      "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')\n",
      "top_word_freq = get_top_n_counts(word_count_df)\n",
      "top_word_freq\n",
      "## Calculate tf–idf\n",
      "To calculate tf–idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
      "\n",
      "When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
      "### Without Smoothing or Normalization (Not Recommended)\n",
      "Remember how we calculated the tf–idf score for the word \"pigeons\" above?\n",
      "total_number_of_documents = 14 \n",
      "number_of_documents_with_term = 2\n",
      "term_frequency = 30\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "\n",
      "term_frequency * inverse_document_frequency\n",
      "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf–idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn.\n",
      "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. \n",
      "\n",
      "Smoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all.\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Compare Word Frequency and tf–idf Scores\n",
      "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*.\n",
      "First, we're going to merge the top raw word frequency ranks into our top tf–idf dataframe.\n",
      "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')\n",
      "Then we're going to add a column that calculates the change in rank—that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\n",
      "tfidf_compare = tfidf_compare.fillna(\"*new top word*\")\n",
      "Finally, we're going to make some functions that will alter the style of our Pandas dataframe—such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign.\n",
      "def make_positive(value):\n",
      "    if value != '*new top word*':\n",
      "        if float(value) > 0:\n",
      "            value = f'+{round(value)}'\n",
      "    return value\n",
      "\n",
      "def make_bold(value):\n",
      "    return 'font-weight: bold'\n",
      "\n",
      "def color_df(value):\n",
      "    if value == '*new top word*':\n",
      "        color = 'green'    \n",
      "    else:\n",
      "        value = str(value).replace('+', '')\n",
      "        value = float(value)\n",
      "        \n",
      "        if value < 0:\n",
      "            color = 'red'\n",
      "        elif value > 0:\n",
      "            color = 'green'\n",
      "        else:\n",
      "             color = 'black'        \n",
      "    df_style = f'color: {color}; font-weight: bold'\n",
      "    return df_style\n",
      "Now let's display the dataframe and explore which words have become more significant and which words have become less so>\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\n",
      "tfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\n",
      "tfidf_compare_styled\n",
      "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n",
      "\n",
      "*Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below*\n",
      "tfidf_compare[tfidf_compare['word'] == 'said']\n",
      "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer.\n",
      "tfidf_compare[tfidf_compare['word'] == 'pigeons']\n",
      "Words that were not frequent enough to make the top 10 for raw word frequency — such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks — now suddenly show up in the top 10 for tf-idf scores.\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreaming']\n",
      "tfidf_compare[tfidf_compare['word'] == 'gospelteers']\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']\n",
      "## Your Turn!\n",
      "Take a few minutes to explore the dataframe below and then answer the following questions.\n",
      "tfidf_compare\n",
      "**1.** What is the difference between a tf-idf score and raw word frequency?\n",
      "**#** Your answer here\n",
      "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?\n",
      "**#** Your answer here\n",
      "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**#** Your answer here\n",
      "## Save as .txt File\n",
      "## TextEdit\n",
      "If you open a new document in TextEdit, it will open a Rich Text Format (.rtf) by default. Even if you add the file extension \".txt\", it will save it as \".txt.rtf\". \n",
      "<img src=\"../images/plain-text/stubborn-rtf.png\" width=100%, border=2>\n",
      "### Keyboard Shortcut\n",
      "To switch from .rtf to .txt, press `Shift` + `Command` + `T`. \n",
      "<img src=\"../images/plain-text/convert-q.png\" width=100%, border=2>\n",
      "<img src=\"../images/plain-text/txt-fixed.png\" width=100%, border=2>\n",
      "### Preferences\n",
      "Alternatively, you can go to \"Preferences,\" and select \"Plain Text\" as the default document file extension.\n",
      "<img src=\"../images/plain-text/preferences.png\" width=100%, border=2>\n",
      "### ✨ Final product ✨\n",
      "<img src=\"../images/plain-text/final.png\" width=100%, border=2>\n",
      "\n",
      "## HW 5 — Functions and Pandas\n",
      "[Download this Jupyter notebook and relevant files here](https://melaniewalsh.org/HW-5.zip)\n",
      "This HW assignment draws on Hannah Andersen and Matt Daniels's \"Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age.\" By following the steps below, you're going to explore and analyze women's proportion of dialogue in film over time. When you're finished, save this file as \"Your-Last-Name-HW-5.ipynb\" and submit it to Canvas by Thursday, February 27th at 9am.\n",
      "**1.** Make a data biography ([inspired by Heather Krause](https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/)) for The Pudding's film dialogue data. Refer to [Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age](https://pudding.cool/2017/03/film-dialogue/) and [\"FAQ for the “Film Dialogue, By Gender” Project](https://medium.com/@matthew_daniels/faq-for-the-film-dialogue-by-gender-project-40078209f751). \n",
      "\n",
      "Remember that you can edit [Markdown](https://guides.github.com/features/mastering-markdown/) cells (like this one) by double-clicking on them and that you can change a cell [from Code to Markdown through the menu bar](../images/Markdown-to-Code.mov).\n",
      "\n",
      "\n",
      "**Where did the data come from? Include at least three specific sources.**\n",
      "**#**Your Text Here\n",
      "**Who collected the data?**\n",
      "**#**Your Text Here\n",
      "**How was the data collected? How was character \"gender\" calculated?**\n",
      "**#**Your Text Here\n",
      "***Why was the data collected?***\n",
      "**#**Your Text Here\n",
      "**2.** Use the code below to make a function called `decade_chunker()` with a parameter called `year` that returns a value called `decade`. The function `decade_chunker` should take in a year value and report what decade that year falls in. For help, refer to [Functions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Functions.html) and [Conditionals & Comparisons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Conditionals-Comparisons.html).\n",
      "#Your Code Here\n",
      "    decade = ''\n",
      "    if year >= 1940 and year < 1950:\n",
      "        decade = '1940s'\n",
      "    elif year >=  1950 and year < 1960:\n",
      "        decade = '1950s'\n",
      "    elif year >= 1960 and year < 1970:\n",
      "        decade = '1960s'\n",
      "    #Your Code Here\n",
      "        decade = '1970s'\n",
      "    elif year >= 1980 and year < 1990:\n",
      "        decade = '1980s'\n",
      "    elif year >= 1990 and year < 2000:\n",
      "        #Your Code Here\n",
      "    elif year >= 2000 and year < 2010:\n",
      "        decade = '2000s'\n",
      "    elif year >= 2010 and year < 2020:\n",
      "        decade = '2010s'\n",
      "    #Your Code Here\n",
      "Test the function `decade_chunker` by running `decade_chunker(1975)`.\n",
      "decade_chunker(1975)\n",
      "**3.**\n",
      "\n",
      "Import Pandas.\n",
      "#Your Code Here\n",
      "Read in the CSV file \"../data/Pudding-Film-Dialogue-Clean.csv\" as `pudding_film_df`. (That's where the data should be if you downloaded the zip file from our course website, but if it's not working, then you might have to re-direct it elsewhere.)\n",
      "#Your Code Here\n",
      "Display a random sample of 10 rows\n",
      "#Your Code Here\n",
      "**4.** Make a new column \"decade\" in `pudding_film_df` by applying the `decade_chunker()` function to the \"release_year\" column. For help, refer to [Applying Functions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Pandas/Pandas-EDA.html#Applying-Functions).\n",
      "#Your Code Here\n",
      "Disaply a random sample of 10 rows.\n",
      "pudding_film_df.sample(10)\n",
      "**5.** Filter the `pudding_film_df` dataframe so that it only includes characters who are women.\n",
      "#Your Code Here\n",
      "Save this dataframe as `women_film_df`.\n",
      "#Your Code Here\n",
      "Display a random sample of 10 rows from `women_film_df`.\n",
      "women_film_df.sample(10)\n",
      "**6.** Find out the top two women who have the highest proportion of dialogue in the entire dataset. Sort `women_film_df`  by \"proportion_of_dialogue\" from the highest proportion to the lowest proportion.\n",
      "#Your Code Here\n",
      "***Output display not included here because it gives away the answer\n",
      "The two women characters in this dataset with the highest proportion of film dialogue are:\n",
      "**1)** **#Your Answer Here** from the movie **#Your Answer Here** with approximately **#Your Answer Here** percent of the dialogue\n",
      "**2)** **#Your Answer Here** from the movie **#Your Answer Here** with approximately **#Your Answer Here** percent of the dialogue\n",
      "**7.** Let's find out how many women characters from each decade appear in this dataset. For `women_film_df`, count the values in the column \"decade\". For help, refer to [Count Values in Columns](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Pandas/Pandas.html#Count-Values-in-Columns). \n",
      "#Your Code Here\n",
      "**8.** Let's look at the percentage of women's dialogue in film across the decades. For `women_film_df`, groupby the column \"decade\" and then calculate the `.sum()` total of \"words\" spoken by women for each decade.\n",
      "#Your Code Here\n",
      "Save this groupby into a variable called `women_dialogue_per_decade`\n",
      "#Your Code Here\n",
      "**9.** The above information isn't very useful unless we compare it to the total number of words spoken across the decades. For `pudding_film_df`, groupby the column \"decade\" and then calculate the `.sum()` total of \"words\" spoken by all characters for each decade.\n",
      "#Your Code Here\n",
      "Save this groupby into a variable called `all_dialogue_per_decade`\n",
      "#Your Code Here\n",
      "**10.** Now divide `women_dialogue_per_decade` by `all_dialogue_per_decade`\n",
      "#Your Code Here\n",
      "Save this calculation into a variable called `women_dialogue_across_decades`\n",
      "#Your Code Here\n",
      "**11.**\n",
      "Make a bar plot of `women_dialogue_across_decades`\n",
      "#Your Code Here\n",
      "## Reflection and Analysis\n",
      "Answer the following questions in a few or more sentences.\n",
      "**12.** What, if anything, can you conclude based on the percentage of women's dialogue across the decades shown in this data — regarding film, the film industry, gender, society and culture, etc.?\n",
      "**#**Your Answer Here\n",
      "**13.** What are some of the limitations of this dataset?\n",
      "**#**Your Answer Here\n",
      "**14.** If you had unlimited time and technical resources, what would be another research question that you would want to pursue with this data or similar data?\n",
      "**#**Your Answer Here\n",
      "**15.** What's another cultural dataset that you could imagine yourself using Pandas with in the future?\n",
      "**#**Your Answer Here\n",
      "## HW 6 — Twitter Collection Setup\n",
      "For this homework assignment, you're going to complete a number of necessary preparation steps in order to collect Twitter data, which will be our subject next week. Follow the instructions at [Twitter Collection Setup](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html) and answer the questions below.\n",
      "\n",
      "🚨 **Don't wait until the last minute for this assignment! Twitter needs to approve your application before you can complete all of the instructions, which can take as long as a day.**\n",
      "\n",
      "Please answer these questions in a plain text file (.txt), save the file as \"Your-Last-Name-HW-6.txt\", and submit it to Canvas by Tuesday, March 10th at 9am.\n",
      "**1.** Reflect on your experience of filling out a Twitter developer application and think about how it compares to the Genius API application process. Why do you you think the Twitter API application process is more involved? Why do you think they ask the specific questions that they do? Discuss in a few sentences.\n",
      "**2.**  Read through Twitter's Developer Agreement and briefly discuss one specific thing that surprises you about these terms. Why does it surprise you?\n",
      "**3.** Once you've gotten your Twitter API keys, you will install and configure twarc. If the configuration process has worked successfully, you will get a happy message on your command line that looks something like this:\n",
      "The credentials for mellymeldubs have been saved to your configuration file at /Users/melaniewalsh/.twarc\n",
      "\n",
      "✨ ✨ ✨ Happy twarcing! ✨ ✨ ✨\n",
      "Copy and paste this message straight from your command line into your HW 6 plain text file.\n",
      "## HW 3 (Part I) — Conditionals and Comparisons\n",
      "[Download this file and Bellevue Almshouse data (not necessary but optional to explore)](https://melaniewalsh.org/Conditionals-Comparisons.zip)\n",
      "For this group exercise and HW assignment, we're going to draw on Anelise Shrout's [Bellevue Almshouse data](https://www.nyuirish.net/almshouse/the-almshouse-records/). For help with this assignment, refer to [Conditionals & Comparisons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Conditionals-Comparisons.html). Save your file as \"Your-Last-Name-HW-3-Conditionals-Comparisons.ipynb\" and submit it to Canvas.\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "We're going to make a series of variables and assign them values based on the Bellevue Almshouse dataset. Make sure you run these cells.\n",
      "### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Write an `if` statement that reports whether `person1_age` is less than 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "**2.** Write an `if` statement that reports whether `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "**3.** Write an `if` statement that reports whether `person1_age` is less than 30 years old *and* `person1_profession` is \"married.\"\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old and married.')\n",
      "**4.** Complicate your `if` statement from Question 1 by adding an `else` statement that prints \"Person is older than 30 years old\". Then evaluate whether `person3_age` is less than 30 years old. \n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**5.** Now evaluate whether `person4_age` is less than 30 years using the same code as Question 4.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "Hmmm, with the code as written, it's telling us that Margaret Farrell, who is 30 years old, is *more* than 30 years old. Add an `elif` statement that reports whether the person is exactly 30 years old.\n",
      "#Your code here\n",
      "    print('Person is less than 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is exactly 30 years old.')\n",
      "#Your code here\n",
      "    print('Person is more than 30 years old.')\n",
      "**6.** Write an `if` statement that will report whether `person1_child_status` includes children.\n",
      "#Your code here\n",
      "    print('Person has children.')\n",
      "**7.** Write one `if` statement that will accurately report whether `person1_child_status` includes children and, separately, if `person2_child_status` includes children. (Hint: think about how you might use the `!=` operator.)\n",
      "if person1_child_status #Your Code Here\n",
      "    print('Person has children.')\n",
      "if person2_child_status #Same Code Here\n",
      "    print('Person has children.')\n",
      "**8.** Write a conditional that will report whether `person1_profession` is \"married,\" \"laborer,\" \"widow,\" or \"unknown profession.\" Test your code by reassigning the variable as indicated below.\n",
      "person1_profession = 'married'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'laborer'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'widow'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "person1_profession = 'student'\n",
      "#Your code here\n",
      "    print('Person is married.')\n",
      "#Your code here\n",
      "    print('Person is a laborer.')\n",
      "#Your code here\n",
      "    print('Person is a widow.')\n",
      "#Your code here\n",
      "    print('Person has unknown profession.')\n",
      "**9.** Some of the Irish immigrants' names have question marks after them. Let's clean up some of the data and remove the question marks. You can use the Python keyword `in` to test whether a string appears within another string. Print `person2_name` with the question mark and parentheses removed. (Hint: think about f-strings and string methods!)\n",
      "if \"(?)\" in person2_name:\n",
      "    #Your code here\n",
      "**10.** In a few sentences, write about your experiencing using and manipulating the Bellevue Almshouse data after reading Shrout's essay. How, if at all, did using this data influence your understanding of how Python works?\n",
      "**#Your thoughts here**\n",
      "## Topic Modeling — Text Files\n",
      "[Download relevant files here](https://melaniewalsh.org/Topic-Modeling.zip)\n",
      "<img src=\"https://pngimg.com/uploads/palette/palette_PNG68293.png\", border=2>\n",
      "In these lessons, we're learning about a text analysis method called **topic modeling**. This method will help us identify the main topics or discourses within a collection of texts (or within a single text that has been separated into smaller text chunks).\n",
      "In this particular lesson, we're going to use [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), a Python wrapper for [MALLET](http://mallet.cs.umass.edu/topics.php), to topic model 378 obituaries published by *The New York Times*. \n",
      "## What You Need To Get Started\n",
      "*If you're working in this Jupyter notebook on your own computer, you'll need to have both the Java Development Kit and MALLET pre-installed. For set up instructions, please see [the previous lesson](http://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Set-Up.html).\n",
      "\n",
      "*If you're working in this Jupyter notebook in the cloud via Binder, then the Java Development Kit and Mallet will already be installed. You're good to go!\n",
      "## Set MALLET Path\n",
      "Since Little MALLET Wrapper is a Python package built around MALLET, we first need to tell it where the bigger, Java-based MALLET lives.\n",
      "\n",
      "We're going to make a variable called `path_to_mallet` and assign it the file path of our MALLET program. We need to point it, specifically, to the \"mallet\" file inside the \"bin\" folder inside the \"mallet-2.0.8\" folder. \n",
      "Run the cell below if you're working in the cloud via Binder or if MALLET if is located in your current directory:\n",
      "path_to_mallet = 'mallet-2.0.8/bin/mallet'\n",
      "Uncomment and run the cell below if MALLET is in your home directory on a Mac:\n",
      "#path_to_mallet = '~/mallet-2.0.8/bin/mallet' \n",
      "Uncomment and run the cell below if MALLET is in your C:/ directory on a Windows computer:\n",
      "#path_to_mallet = 'C:/mallet-2.0.8/bin/mallet'\n",
      "If MALLET is located in another directory, then set your `path_to_mallet` to that file path.\n",
      "*Note: \"uncomment\" means delete the initial `#`*\n",
      "*Note: the tilde `~` automatically fills in your home directory on Mac/Linux machines*\n",
      "## Import Libraries\n",
      "#!pip install little_mallet_wrapper\n",
      "#!pip install seaborn\n",
      "Now let's `import` the `little_mallet_wrapper` and the data viz library `seaborn`.\n",
      "import little_mallet_wrapper\n",
      "import seaborn\n",
      "We're also going to import [`glob`](https://docs.python.org/3/library/glob.html) and [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) for working with files and the file system.\n",
      "import glob\n",
      "from pathlib import Path\n",
      "## Get Training Data From Text Files\n",
      "Before we topic model the *NYT* obituaries, we need to process the text files and prepare them for analysis. The steps below demonstrate how to process texts if your corpus is a collection of separate text files. In the next lesson, we'll demonstrate how to process texts that come from a CSV file.\n",
      "\n",
      "Note: We're calling these text files our *training data*, because we're *training* our topic model with these texts. The topic model will be learning and extracting topics based on these texts.\n",
      "## NYT Obituaries\n",
      "This dataset of *NYT* obituaries is based on data originally collected by Matt Lavin for his *Programming Historian* [TF-IDF tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf#lesson-dataset). However, I have re-scraped the obituaries so that the subject's name and death year is included in each text file name, and I have added 12 more [\"Overlooked\"](https://www.nytimes.com/interactive/2018/obituaries/overlooked.html) obituaries.\n",
      "To get the necessary text files, we're going to make a variable and assign it the file path for the directory that contains the text files.\n",
      "directory = \"../texts/history/NYT-Obituaries/\"\n",
      "Then we're going to use the `glob.gob()` function to make a list of all (`*`) the `.txt` files in that directory.\n",
      "files = glob.glob(f\"{directory}/*.txt\")\n",
      "files\n",
      "## Process Texts\n",
      "`little_mallet_wrapper.process_string(text, numbers='remove')`\n",
      "Next we process our texts with the function `little_mallet_wrapper.process_string()`. This function will take every individual text file, transform all the text to lowercase as well as remove stopwords, punctuation, and numbers, and then add the processed text to our master list `training_data`.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very common Python pattern! We make an empty list `training_data = []`, then we use a `for` loop to iterate through every file path in the list of file paths, then we `open()` and `.read()` each text file associated with that file path, then we processes `little_mallet_wrapper.process_string()` the text, and finally we `.append()` the processed text to our master list.*\n",
      "training_data = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    processed_text = little_mallet_wrapper.process_string(text, numbers='remove')\n",
      "    training_data.append(processed_text)\n",
      "We're also making a master list of the original text of the obituaries for future reference.\n",
      "original_texts = []\n",
      "for file in files:\n",
      "    text = open(file, encoding='utf-8').read()\n",
      "    original_texts.append(text)\n",
      "## Process Titles\n",
      "Here we extract the relevant part of each file name by using [`Path().stem`](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem), which conveniently extracts just the last part of the file path without the \".txt\" file extension. Because each file name includes the obituary subject's name as well as the year that the subject died, we're going to use this information as a title or label for each obituary.\n",
      "> *Take a moment to study this code and reflect about what's happening here. This is a very simple list comprehension!*\n",
      "obit_titles = [Path(file).stem for file in files]\n",
      "obit_titles\n",
      "## Get Training Data Stats\n",
      "We can get training data summary statisitcs by using the funciton `little_mallet_wrapper.print_dataset_stats()`.\n",
      "little_mallet_wrapper.print_dataset_stats(training_data)\n",
      "According to this little report, we have 378 documents (or obituaries) that average 1345 words in length.\n",
      "## Training the Topic Model\n",
      "`little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)`\n",
      "We're going to train our topic model with the `little_mallet_wrapper.train_topic_model()` function. As you can see above, however, this function requires 6 different arguments and file paths to run properly:\n",
      "\n",
      "- `path_to_mallet`\n",
      "- `path_to_formatted_training_data`\n",
      "- `path_to_model`\n",
      "- `path_to_topic_keys`\n",
      "- `path_to_topic_distributions`\n",
      "- `num_topics`\n",
      "\n",
      "So we have to set a few things up first.\n",
      "## Set Number of Topics\n",
      "We need to make a variable `num_topics` and assign it the number of topics we want returned.\n",
      "num_topics = 15\n",
      "## Set Training Data\n",
      "We already made a variable called `training_data`, which includes all of our processed obituary texts, so we can just set it equal to itself.\n",
      "training_data = training_data\n",
      "## Set Topic Model Output Files\n",
      "Finally, we need to tell Little MALLET Wrapper where to find and output all of our topic modeling results. The code below will set Little MALLET Wrapper up to output your results inside a directory called \"topic-model-output\" and a subdirectory called \"NYT-Obits\", all of which will be inside your current directory.\n",
      "\n",
      "If you'd like to change this output location, simply change `output_directory_path` below.\n",
      "#Change to your desired output directory\n",
      "output_directory_path = 'topic-model-output/NYT-Obits'\n",
      "\n",
      "#No need to change anything below here\n",
      "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
      "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
      "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
      "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
      "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\"\n",
      "## Import Data\n",
      "Now we import our training data with `little_mallet_wrapper.import_data()`.\n",
      "little_mallet_wrapper.import_data(path_to_mallet,\n",
      "                path_to_training_data,\n",
      "                path_to_formatted_training_data,\n",
      "                training_data)\n",
      "## Train Topic Model\n",
      "Finally, we train our topic model with `little_mallet_wrapper.train_topic_model()`. The topic model should take about 45 seconds to 1 minute to fully train and complete. If you want, you can look at your Terminal or PowerShell while it's running and see what the model looks like as it trains.\n",
      "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
      "                      path_to_formatted_training_data,\n",
      "                      path_to_model,\n",
      "                      path_to_topic_keys,\n",
      "                      path_to_topic_distributions,\n",
      "                      num_topics)\n",
      "When the topic model finishes, it will output your results to your `output_directory_path`.\n",
      "## Display Topics and Top Words\n",
      "To examine the 15 topics that the topic model extracted from the *NYT* obituaries, run the cell below. This code uses the `little_mallet_wrapper.load_topic_keys()` function to read and process the MALLET topic model output from your computer, specifically the file \"mallet.topic_keys.15\".\n",
      "\n",
      ">*Take a minute to read through every topic. Reflect on what each topic seems to capture as well as how well you think the topics capture the broad themes of the entire collection. Note any oddities, outliers, or inconsistencies.*\n",
      "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
      "\n",
      "for topic_number, topic in enumerate(topics):\n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")\n",
      "## Load Topic Distributions\n",
      "MALLET also calculates the likely mixture of these topics for every single obituary in the corpus. This mixture is really a probability distribution, that is, the probability that each topic exists in the document. We can use these probability distributions to examine which of the above topics are strongly associated with which specific obituaries.\n",
      "\n",
      "To get the topic distributions, we're going to use the `little_mallet_wrapper.load_topic_distributions()` function, which will read and process the MALLET topic model output, specifically the file \"mallet.topic_distributions.15\". \n",
      "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
      "If we look at the 32nd topic distribution in this list of `topic_distributions`, which corresponds to Marilyn Monroe's obituary, we will see a list of 15 probabilities. This  list corresponds to the likelihood that each of the 15 topics exists in Marilyn Monroe's obituary.\n",
      "topic_distributions[32]\n",
      "It's a bit easier to understand if we pair these probabilities with the topics themselves. As you can see below, Topic 0 \"miss film theater movie broadway films\" has a relatively high probability of existing in Marilyn Monroe's obituary `.202` while Topic 5 \"soviet hitler german germany stalin union\" has a relatively low probability `.002`. This seems to comport with what we know about Marilyn Monroe.\n",
      "obituary_to_check = \"1962-Marilyn-Monroe\"\n",
      "\n",
      "obit_number = obit_titles.index(obituary_to_check)\n",
      "\n",
      "print(f\"Topic Distributions for {obit_titles[obit_number]}\\n\")\n",
      "for topic_number, (topic, topic_distribution) in enumerate(zip(topics, topic_distributions[obit_number])):\n",
      "    print(f\"✨Topic {topic_number} {topic[:6]} ✨\\nProbability: {round(topic_distribution, 3)}\\n\")\n",
      "## Explore Heatmap of Topics and Texts\n",
      "`little_mallet_wrapper.plot_categories_by_topics_heatmap(all_labels,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )`\n",
      "We can visualize and compare these topic probability distributions with a heatmap by using the `little_mallet_wrapper.plot_categories_by_topics_heatmap()` function. This function requires six arguments and file paths:\n",
      "\n",
      "* `all_labels` all the labels for the texts in your collection \n",
      "* `topic_distributions` your list of topic distributions\n",
      "* `topics` your list of topics \n",
      "* `output_directory_path + '/categories_by_topics.pdf'` the file path where you want to save a PDF of the heatmap\n",
      "* `target_labels=target_labels` the sample of texts that you want to visualize \n",
      "* `dim= (10, 9)` the size or dimensions of the heatmap\n",
      "\n",
      "We have everything we need for the heatmap except for our list of `target_labels`, the sample of texts that we'd like to visualize and compare with the heatmap. Below we make our list of desired target labels.\n",
      "target_labels = ['1852-Ada-Lovelace', '1885-Ulysses-Grant',\n",
      "                 '1900-Nietzsche', '1931-Ida-B-Wells', '1940-Marcus-Garvey',\n",
      "                 '1941-Virginia-Woolf', '1954-Frida-Kahlo', '1962-Marilyn-Monroe',\n",
      "                 '1963-John-F-Kennedy', '1964-Nella-Larsen', '1972-Jackie-Robinson',\n",
      "                 '1973-Pablo-Picasso', '1984-Ray-A-Kroc','1986-Jorge-Luis-Borges', '1991-Miles-Davis',\n",
      "                 '1992-Marsha-P-Johnson', '1993-Cesar-Chavez']\n",
      "If you'd like to make a random list of target labels, you can uncomment and run the cell below.\n",
      "#import random\n",
      "#target_labels = random.sample(obit_titles, 10)\n",
      "little_mallet_wrapper.plot_categories_by_topics_heatmap(obit_titles,\n",
      "                                      topic_distributions,\n",
      "                                      topics, \n",
      "                                      output_directory_path + '/categories_by_topics.pdf',\n",
      "                                      target_labels=target_labels,\n",
      "                                      dim= (10, 9)\n",
      "                                     )\n",
      "The darker squares in this heatmap represent a high probability for the corresponding topic (compared to everyone else in the heatmap) and the lighter squares in the heatmap represent a low probability for the corresponding topic. For example, if you scan across the row of Marilyn Monroe, you can see a dark square for the topic \"miss film theater movie theater broadway\". If you scan across the row of Ada Lovelace, an English mathematician who is now recognized as the first computer programmer, according to her [NYT obituary](https://www.nytimes.com/interactive/2018/obituaries/overlooked-ada-lovelace.html), you can see a dark square for \"university professor research science also\".\n",
      "## Display Top Titles Per Topic\n",
      "`little_mallet_wrapper.get_top_docs(training_data,\n",
      "                                    topic_distributions,\n",
      "                                    topic=topic_number,\n",
      "                                    n=number_of_documents)`\n",
      "We can also display the obituaries that have the highest probability for every topic with the `little_mallet_wrapper.get_top_docs()` function.\n",
      "\n",
      "Because most of the obituaries in our corpus are pretty long, however, it will be more useful for us to simply display the title of each obituary, rather than the entire document—at least as a first step. To do so, we'll first need to make two dictionaries, which will allow us to find the corresponding obituary title and the original text from a given training document.\n",
      "training_data_obit_titles = dict(zip(training_data, obit_titles))\n",
      "training_data_original_text = dict(zip(training_data, original_texts))\n",
      "Then we'll make our own function `display_top_titles_per_topic()` that will display the top text titles for every topic. This function accepts a given `topic_number` as well as a desired `number_of_documents` to display.\n",
      "def display_top_titles_per_topic(topic_number=0, number_of_documents=5):\n",
      "    \n",
      "    print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        print(round(probability, 4), training_data_obit_titles[document] + \"\\n\")\n",
      "    return\n",
      "**Topic 0**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 0, we will run:\n",
      "display_top_titles_per_topic(topic_number=0, number_of_documents=5)\n",
      "**Topic 0 Label**: Hollywood\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Hollywood.\"\n",
      "**Topic 9**\n",
      "To display the top 5 obituary titles with the highest probability of containing Topic 9, we will run:\n",
      "display_top_titles_per_topic(topic_number=9, number_of_documents=5)\n",
      "**Topic 9 Label**: Global Affairs\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Global Affairs.\"\n",
      "**Topic 8**\n",
      "To display the top 7 obituaries with the highest probability of containing Topic 8, we will run:\n",
      "display_top_titles_per_topic(topic_number=8, number_of_documents=7)\n",
      "**Topic 8 Label**: Authors\n",
      "Based on the words in this topic and these top obituary texts, I'm going to label this topic \"Authors.\"\n",
      "## Display Topic Words in Context of Original Text\n",
      "Often it's useful to actually look at the document that has ranked highly for a given topic and puzzle out why it ranks so highly.\n",
      "To display the original obituary texts that rank highly for a given topic, with the relevant topic words **bolded** for emphasis, we are going to make the function `display_bolded_topic_words_in_context()`.\n",
      "\n",
      "In the cell below, we're importing two special Jupyter notebook display modules, which will allow us to make the relevant topic words **bolded**, as well as the regular expressions library `re`, which will allow us to find and replace the correct words.\n",
      "from IPython.display import Markdown, display\n",
      "import re\n",
      "\n",
      "def display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3, custom_words=None):\n",
      "\n",
      "    for probability, document in little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents):\n",
      "        \n",
      "        print(f\"✨Topic {topic_number}✨\\n\\n{topics[topic_number]}\\n\")\n",
      "        \n",
      "        probability = f\"✨✨✨\\n\\n**{probability}**\"\n",
      "        obit_title = f\"**{training_data_obit_titles[document]}**\"\n",
      "        original_text = training_data_original_text[document]\n",
      "        topic_words = topics[topic_number]\n",
      "        topic_words = custom_words if custom_words != None else topic_words\n",
      "\n",
      "        for word in topic_words:\n",
      "            if word in original_text:\n",
      "                original_text = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", original_text)\n",
      "\n",
      "        display(Markdown(probability)), display(Markdown(obit_title)), display(Markdown(original_text))\n",
      "    return\n",
      "**Topic 3**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 0 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=3, number_of_documents=3)\n",
      "**Topic 8**\n",
      "To display the top 3 original obituaries with the highest probability of containing Topic 8 and with relevant topic words bolded, we will run:\n",
      "display_bolded_topic_words_in_context(topic_number=8, number_of_documents=3)\n",
      "## Your Turn!\n",
      "Choose a topic from the results above and write down its corresponding topic number below.\n",
      "**Topic: *Your Number Choice Here***\n",
      "**1.** Display the top 6 obituary titles for this topic.\n",
      "#Your Code Here\n",
      "**2.** Display the topic words in the context of the original obituary for these 6 top titles.\n",
      "#Your Code Here\n",
      "**3.** Come up with a label for your topic and write it below:\n",
      "**Topic Label: *Your Label Here***\n",
      "**Reflection**\n",
      "**4.** Why did you label your topic the way you did? What do you think this topic means in the context of all the *NYT* obituaries?\n",
      "**#**Your answer here\n",
      "**5.** What's another collection of texts that you think might be interesting to topic model? Why?\n",
      "**#**Your answer here\n",
      "## HW 8 — Term Frequency–Inverse Document Frequency\n",
      "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip)\n",
      "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 3 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-8.ipynb\" and submit it to Canvas by Friday, April 9th at 5pm.\n",
      "\n",
      "*This notebook is exactly the same as [the TF-IDF lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/TF-IDF.html), and submitting either notebook is fine.\n",
      "In this lesson, we're going to learn about a text analysis method called **term frequency–inverse document frequency** (tf–idf). This method will help us identify the most unique words in a document from a given corpus. \n",
      ">term = word <br>\n",
      ">document = text (or chunk of a text) <br>\n",
      ">corpus = collection of texts <br>\n",
      "## Why is tf–idf Useful?\n",
      "\n",
      "## The Basic Math\n",
      "> `term_frequency * inverse_document_frequency`\n",
      "## Breaking Down the Formula\n",
      "\n",
      "> `term_frequency = number of times a given word appears in story or text`\n",
      "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n",
      "\n",
      "> `total_number_of_documents / number_of_documents_with_term`\n",
      "\n",
      "...the result of which we're going to take the logarithm of and then add 1\n",
      "\n",
      "> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n",
      "\n",
      "Do you see how if we flipped the fraction — making it `number_of_documents_with_term /  total_number_of_documents`— that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\"\n",
      "## The Formula in Action\n",
      "**\"said\" vs \"pigeons\"**\n",
      "Using this formula, we're going to calculate and compare the tf–idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*.\n",
      "We need the log() function for our calculation, so we're going to import it from the `math` package.\n",
      "from math import log\n",
      "**\"said\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 13 #number of short stories the contain the word \"said\"\n",
      "term_frequency = 47 #number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**\"pigeons\"**\n",
      "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\n",
      "number_of_documents_with_term = 2 #number of short stories the contain the word \"pigeons\"\n",
      "term_frequency = 30 #number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "term_frequency * inverse_document_frequency\n",
      "**tf–idf Scores**\n",
      "\n",
      "\"said\" = 50.48<br>\n",
      "\"pigeons\" = 88.38\n",
      "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf–idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them.\n",
      "## tf–idf with scikit-learn\n",
      "## Import Libraries\n",
      "We could continue calculating tf–idf scores in this manner — by doing all the math with Python — but conveniently there's a Python library that can calculate tf–idf scores in just a few lines of code.\n",
      "\n",
      "This library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`.\n",
      "!pip install sklearn\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number.\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 200)\n",
      "pd.set_option(\"max_columns\", 200)\n",
      "pd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)\n",
      "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*.\n",
      "from pathlib import Path  \n",
      "import glob\n",
      "## Set Directory Path\n",
      "Below we're setting the directory filepath that contains all the short story text files that we want to analyze.\n",
      "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\"\n",
      "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles.\n",
      "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
      "text_titles = [Path(text).stem for text in text_files]\n",
      "Let's display them to make sure they're correct:\n",
      "text_files, text_titles\n",
      "## Calculate Word Frequency (Optional Step)\n",
      "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf–idf scores, we can compare these two methods and see how tf–idf helps us find more unique words.\n",
      "\n",
      "(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)\n",
      "#Initialize CountVectorizer with desired parameters\n",
      "count_vectorizer= CountVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\n",
      "word_count_vector = count_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the word count vector and sort by title\n",
      "word_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\n",
      "word_count_df = word_count_df.sort_index()\n",
      "\n",
      "#Add column for number of times each word appears in all the documents\n",
      "word_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()\n",
      "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\").\n",
      "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words.\n",
      "word_count_df.sample(10, axis='columns')\n",
      "Let's zoom in on some specific words.\n",
      "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`\n",
      "def get_top_n_counts(dataframe, top_n=10):\n",
      "    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n",
      "    return pretty_df\n",
      "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words.\n",
      "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')\n",
      "top_word_freq = get_top_n_counts(word_count_df)\n",
      "top_word_freq\n",
      "## Calculate tf–idf\n",
      "To calculate tf–idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
      "\n",
      "When you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf.\n",
      "### Without Smoothing or Normalization (Not Recommended)\n",
      "Remember how we calculated the tf–idf score for the word \"pigeons\" above?\n",
      "total_number_of_documents = 14 \n",
      "number_of_documents_with_term = 2\n",
      "term_frequency = 30\n",
      "inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n",
      "\n",
      "term_frequency * inverse_document_frequency\n",
      "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf–idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn.\n",
      "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "### With Smoothing and Normalization (Defaults/Recommended)\n",
      "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf–idf scores. \n",
      "\n",
      "Smoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all.\n",
      "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\n",
      "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
      "\n",
      "#Plug in \"text_files\" which contains all our short stories\n",
      "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
      "#Make a DataFrame out of the tf–idf vector and sort by title\n",
      "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
      "tfidf_df = tfidf_df.sort_index()\n",
      "\n",
      "#Add column for number of times word appears in all documents\n",
      "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()\n",
      "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\n",
      "tfidf_slice\n",
      "To find out the top 10 words with the highest tf–idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`\n",
      "def get_top_tfidf_scores(series, top_n=10):\n",
      "    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n",
      "    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n",
      "    pretty_df = pretty_df.drop(columns='level_0')\n",
      "    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n",
      "    return pretty_df\n",
      "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf–idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf–idf scores.\n",
      "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')\n",
      "top_tfidf = get_top_tfidf_scores(tfidf_df)\n",
      "top_tfidf\n",
      "## Write to a CSV File\n",
      "filename = \"tfidf_Lost-in-The-City.csv\"\n",
      "top_tfidf.to_csv(filename, encoding='UTF-8', index=False)\n",
      "## Compare Word Frequency and tf–idf Scores\n",
      "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*.\n",
      "First, we're going to merge the top raw word frequency ranks into our top tf–idf dataframe.\n",
      "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')\n",
      "Then we're going to add a column that calculates the change in rank—that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency.\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\n",
      "tfidf_compare = tfidf_compare.fillna(\"*new top word*\")\n",
      "Finally, we're going to make some functions that will alter the style of our Pandas dataframe—such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign.\n",
      "def make_positive(value):\n",
      "    if value != '*new top word*':\n",
      "        if float(value) > 0:\n",
      "            value = f'+{round(value)}'\n",
      "    return value\n",
      "\n",
      "def make_bold(value):\n",
      "    return 'font-weight: bold'\n",
      "\n",
      "def color_df(value):\n",
      "    if value == '*new top word*':\n",
      "        color = 'green'    \n",
      "    else:\n",
      "        value = str(value).replace('+', '')\n",
      "        value = float(value)\n",
      "        \n",
      "        if value < 0:\n",
      "            color = 'red'\n",
      "        elif value > 0:\n",
      "            color = 'green'\n",
      "        else:\n",
      "             color = 'black'        \n",
      "    df_style = f'color: {color}; font-weight: bold'\n",
      "    return df_style\n",
      "Now let's display the dataframe and explore which words have become more significant and which words have become less so>\n",
      "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\n",
      "tfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\n",
      "tfidf_compare_styled\n",
      "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n",
      "\n",
      "*Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below*\n",
      "tfidf_compare[tfidf_compare['word'] == 'said']\n",
      "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer.\n",
      "tfidf_compare[tfidf_compare['word'] == 'pigeons']\n",
      "Words that were not frequent enough to make the top 10 for raw word frequency — such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks — now suddenly show up in the top 10 for tf-idf scores.\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreaming']\n",
      "tfidf_compare[tfidf_compare['word'] == 'gospelteers']\n",
      "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']\n",
      "## Your Turn!\n",
      "Take a few minutes to explore the dataframe below and then answer the following questions.\n",
      "tfidf_compare\n",
      "**1.** What is the difference between a tf-idf score and raw word frequency?\n",
      "**#** Your answer here\n",
      "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?\n",
      "**#** Your answer here\n",
      "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?\n",
      "**#** Your answer here\n",
      "## HW 4 – Practice with Pandas\n",
      "[Download relevant files here](https://melaniewalsh.org/Pandas.zip)\n",
      "Follow the instructions in the assignment below. Then save your file as \"Your-Last-Name-HW-4-Pandas.ipynb\" and submit it to Canvas by Tuesday at 9am.\n",
      "<img src=\"../images/Intra-American.png\" width=100%, border=2>\n",
      ">Computation\n",
      "could not, it seemed, capture the violent quandary that was the nation’s\n",
      "history of and relationship to human bondage. Contemporary encounters\n",
      "with digital technology have inherited this tension, with researchers\n",
      "struggling to appreciate the inhumanity of bondage and the attendant\n",
      "dehumanization of black lives while also responding to the need for critical,\n",
      "rigorous, and engaged histories of slavery as histories of the present (18).\n",
      "\n",
      "> [D]isplaying data\n",
      "alone could not and did not offer the atonement descendants of slaves\n",
      "sought or capture the inhumanity of this archive’s formation. Culling\n",
      "the lives of women and children from the data set required approaching\n",
      "the data with intention. It required a methodology attuned to black life\n",
      "and to dismantling the methods used to create the manifests in the first\n",
      "place, then designing and launching an interface responsive to the desire\n",
      "of descendants of slaves for reparation and redress (65).\n",
      "\n",
      "> Jessica Marie Johnson, [\"Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads\"](https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death)\n",
      "The dataset that you're going to be working with in this homework assignment is taken from [The Intra-American Slave Trade Database](https://www.slavevoyages.org/american/database), part of the [*Slave Voyages* project](https://www.slavevoyages.org/). This dataset was filtered to include only North American disembarkation locations and to include data about the percentage of men, women, and children on the voyages. Some column names have been altered or modified.\n",
      "## Required Reading\n",
      "Before beginning this homework assignment, read through the short [\"Introduction\"](https://www.slavevoyages.org/american/about) to the Intra-American Slave Trade Database as well as the post [\"Voyages and Applied History\"](https://www.slavevoyages.org/voyage/essays). You will return to answer some questions about these readings at the end of the assignment.\n",
      "## Import Pandas\n",
      "**1.**\n",
      "# Your Code Here\n",
      "## Read in CSV File\n",
      "**2.**\n",
      "Make a dataframe called `slave_intra_american_df` and read in the CSV file \"Slave-Voyages-Intra-American-North-America.csv\". Note that the rows in this CSV file are separated by tabs `\\t`, not commas. They will thus require a different delimiter. \n",
      "# Your Code Here\n",
      "## Display the Data\n",
      "**3.**\n",
      "Display the first 15 rows of the data.\n",
      "# Your Code Here\n",
      "Look at a random sample of 7 rows.\n",
      "# Your Code Here\n",
      "## Examine the Data\n",
      "**4.**\n",
      "# Your Code Here\n",
      "**How many rows does this dataset include?**\n",
      "**#** Type Your Answer Here\n",
      "Check the data types of the columns in your dataset\n",
      "# Your Code Here\n",
      "## Rename Column\n",
      "**5.**\n",
      "Rename the \"flag\" column as \"national_affiliation.\" Don't forget to assign it to your original `slave_intra_american_df` dataframe so that the renamed column is saved. Display the first 5 rows of the `slave_intra_american_df` to check your work.\n",
      "# Your Code Here\n",
      "# Your Code Here\n",
      "## Select and Count Columns\n",
      "**6.**\n",
      "Select and display the column \"place_of_slave_disembarkation\"\n",
      "# Your Code Here\n",
      "**7.**\n",
      "Count the values in the column \"place_of_slave_disembarkation\"\n",
      "# Your Code Here\n",
      "## Filter Data\n",
      "**8.**\n",
      "Filter the dataset and display only the rows that included \"New York\" as their \"place_of_slave_disembarkation\". Save this data into a new variable called `new_york`\n",
      "# Your Code Here\n",
      "# Your Code Here\n",
      "## Sort Data\n",
      "**9.**\n",
      "Sort your smaller `new_york` dataframe by the \"year_of_arrival\", from the latest date to the earliest date\n",
      "# Your Code Here\n",
      "## Groupby and Plot\n",
      "**10.**\n",
      "Calculate how many enslaved people were transported by each nation. Group your `new_york` dataframe by \"national_affiliation\" and then calculate the sum of \"total_disembarked\". Then save this data into a variable called `national_totals`\n",
      "# Your Code Here\n",
      "# Your Code Here\n",
      "**11.**\n",
      "Make a bar chart out of the `national_totals` data\n",
      "# Your Code Here\n",
      "## Think Intentionally With The Data\n",
      "**12.**\n",
      "Re-read or re-skim the short [\"Introduction\"](https://www.slavevoyages.org/american/about) to the Intra-American Slave Trade Database as well as the post [\"Voyages and Applied History\"](https://www.slavevoyages.org/voyage/essays). Drawing on these readings and your experience working with the data above, write a 100-word response about how you or another researcher might use this data *intentionally* — in a way that resists the dehumanizing quantification that created it in the first place and contributes to efforts toward justice.\n",
      "**#** Type Your 100-Word Response Here\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "...\n",
      "## HW 7 — Twitter Data\n",
      "[Download relevant files here](https://melaniewalsh.org/HW-7-Twitter-Data.zip)\n",
      "Follow the instructions in the assignment below. When you're finished, save this file as \"Your-Last-Name-HW-7.ipynb\" and submit it to Canvas by Tuesday, March 17th at 9am.\n",
      "## Select a Tweet Dataset\n",
      "## Create Your Own Tweet Dataset (Optional) \n",
      "If you want, you can create your own tweet dataset for this assignment. However, this part of the assignment is no longer required.\n",
      "Choose your own search term or query. Collect at least 500 tweets that contain this query.\n",
      "!#Your Code Here\n",
      "Count how many tweets you collected and make sure it's over 500.\n",
      "!#Your Code Here\n",
      "Why did you select this search term or query?\n",
      "**#**Your Answer Here\n",
      "Convert your Twitter JSON file into a CSV file using the twarc utility `json2csv.py`.\n",
      "!#Your Code Here\n",
      "## Pick a Sample Tweet Dataset\n",
      "If you don't create your own tweet dataset, you can choose from two of the tweet datasets provided in the \"HW 7\" zip file.\n",
      "\n",
      "* touch_my_face_tweets.csv — 685 tweets that included the general phrase \"touch my face\" (most responding to the coronavirus-related health recommendation that people not touch their faces) as well as received more than 5 retweets\n",
      "\n",
      "* not_with_bang_tweets.csv — 576 tweets that included the exact phrase \"not with a bang but with a\", which is a phrase that comes from the conclusion of T.S. Eliot's 1925 poem \"The Hollow Men\": This is the way the world ends / Not with a bang but with a whimper.\"\n",
      "## (Required) Read in as Pandas DataFrame\n",
      "**1.** Import Pandas and optionally [set custom pandas display options](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Analysis.html#Read-in-Tweet-CSV-files-with-Pandas)\n",
      "#Your Code Here\n",
      "#Optional Code Here\n",
      "#Optional Code Here\n",
      "#Optional Code Here\n",
      "Read in your tweets CSV file as a Pandas dataframe and name it something other than `your_df`.\n",
      "your_df = #Your Code Here\n",
      "## Filter the DataFrame\n",
      "**2.** Display all the column names in your dataframe.\n",
      "#Your Code Here\n",
      "Filter your dataframe to only 10 columns. Pick which columns you want to include. Make sure you include \"text\" and \"retweet_count.\" (When you run the cell below, right-click and \"Enable Scrolling for Outputs\").\n",
      "#Your Code Here\n",
      "Save your filtered dataframe as `filtered_df`.\n",
      "#Your Code Here\n",
      "## Sort Your Twitter Data by Top Retweets\n",
      "**3.** Sort `filtered_df` by number of retweets, from largest to smallest. (Again make sure to \"Enable Scrolling for Outputs\").\n",
      "#Your code here\n",
      "What is the text of the most retweeted tweet in your dataset?\n",
      "**#**Your most retweeted tweet here\n",
      "## Examine Another Category\n",
      "**4.**  By using `.groupby()`, `.value_counts()`, and/or a filter (e.g. `df['flag']=='USA'`), examine another metadata category in your Twitter dataset.\n",
      "#Your code here\n",
      "What is a finding about your Twitter data that emerges from this calculation/sorting/filtering? Explain in at least a couple of sentences.\n",
      "**#**Your finding here\n",
      "## Overall Analysis and Future Work\n",
      "**5.** What, if anything, can you conclude about these tweets and Twitter users based on your analysis? Given unlimited time and resources, what steps or kinds of analysis would you undertake next? Discuss in a few or more sentences. \n",
      "**#**Your Answer Here\n",
      "## HW 3 (Part II) — Lists & For Loops\n",
      "For this HW assignment, we're again going to draw on Anelise Shrout's [Bellevue Almshouse data](https://docs.google.com/spreadsheets/d/1uf8uaqicknrn0a6STWrVfVMScQQMtzYf5I_QyhB9r7I/edit#gid=2057113261). For help with this assignment, refer to [Lists & For Loops](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html). Save your file as \"Your-Last-Name-HW-3-Lists-Loops.ipynb\" and submit it to Canvas.\n",
      "<img src=\"../images/bellevue-almshouse-screenshot.png\" width=100%, border=2>\n",
      "### Person 1\n",
      "person1_name = 'Mary Gallagher'\n",
      "person1_age = 28\n",
      "person1_disease = 'recent emigrant'\n",
      "person1_profession = 'married'\n",
      "person1_gender = 'f'\n",
      "person1_child_status = 'Child Alana 10 days'\n",
      "### Person 2\n",
      "person2_name = 'John Sanin(?)'\n",
      "person2_age = 19\n",
      "person2_disease = 'recent emigrant'\n",
      "person2_profession = 'laborer'\n",
      "person2_gender = 'm'\n",
      "person2_child_status = 'Catherine 2 mo'\n",
      "### Person 3\n",
      "person3_name = 'Anthony Clark'\n",
      "person3_age = 60\n",
      "person3_disease = 'recent emigrant'\n",
      "person3_profession = 'laborer'\n",
      "person3_gender = 'm'\n",
      "person3_child_status = 'Charles Riley afed 10 days'\n",
      "### Person 4\n",
      "person4_name = 'Margaret Farrell'\n",
      "person4_age = 30\n",
      "person4_disease = 'recent emigrant'\n",
      "person4_profession = 'widow'\n",
      "person4_gender = 'w'\n",
      "person4_child_status = ''\n",
      "**1.** Make a list that contains each of the above Irish immigrants' professions and assign to a variable called `professions`\n",
      "#Your Code Here\n",
      "**2.** Extract the second item in the list `professions`. Hint: remember how the Python index works!\n",
      "#Your Code Here\n",
      "**3.** Add the item \"spinster\" to your `professions` list, then print the list.\n",
      "#Your Code Here\n",
      "#Your Code Here\n",
      "**4.** Make a `for` loop that considers each item in the `professions` list and prints \"Person's profession is ___\"\n",
      "#Your Code Here\n",
      "    #Your Code Here\n",
      "**5.** Make a list that contains each of the above Irish immigrants' child statuses and assign to a variable called `child_status`. You can make Margaret Farrell's child status an empty string `''`.\n",
      "#Your Code Here\n",
      "**6.** Extract the third item in the list.\n",
      "#Your Code Here\n",
      "**7.** Make a `for` loop that considers each item in the `child_status` list and prints \"Person has child\" if the person has a child and \"Person does not have child\" if not\n",
      "#Your Code Here\n",
      "  #Your Code Here\n",
      "       #Your Code Here\n",
      "    #Your Code Here\n",
      "        #Your Code Here\n",
      "**8.** Make a list that contains each of the above Irish immigrants' genders and assign to a variable called `gender`\n",
      "#Your Code Here\n",
      "**9.** Add an item to the list called \"not known\"\n",
      "#Your Code Here\n",
      "**10.** Make a `for` loop that considers each item in the `gender` list and prints \"Person is male\" if the person is male, \"Person is female\" if the person is female, and \"Person's gender is not known\" if unknown\n",
      " #Your Code Here\n",
      "     #Your Code Here\n",
      "         #Your Code Here\n",
      "     #Your Code Here\n",
      "         #Your Code Here\n",
      "     #Your Code Here\n",
      "         #Your Code Here\n",
      "## Save as .txt File\n",
      "## TextEdit\n",
      "If you open a new document in TextEdit, it will open a Rich Text Format (.rtf) by default. Even if you add the file extension \".txt\", it will save it as \".txt.rtf\". \n",
      "<img src=\"../images/plain-text/stubborn-rtf.png\" width=100%, border=2>\n",
      "### Keyboard Shortcut\n",
      "To switch from .rtf to .txt, press `Shift` + `Command` + `T`. \n",
      "<img src=\"../images/plain-text/convert-q.png\" width=100%, border=2>\n",
      "<img src=\"../images/plain-text/txt-fixed.png\" width=100%, border=2>\n",
      "### Preferences\n",
      "Alternatively, you can go to \"Preferences,\" and select \"Plain Text\" as the default document file extension.\n",
      "<img src=\"../images/plain-text/preferences.png\" width=100%, border=2>\n",
      "### ✨ Final product ✨\n",
      "<img src=\"../images/plain-text/final.png\" width=100%, border=2>\n",
      "\n",
      "## HW 2 — Variables and Data Types\n",
      "[Download necessary Jupyter notebooks and text files here](https://melaniewalsh.org/Intro-CA-Notebooks-V2.zip)\n",
      "Follow the instructions below. When you're finished, save your Jupyter notebook and then upload it to Canvas as an (.ipynb) file. Some of the cells below already have outputs, which will show you what the correct answer should yield.\n",
      "\n",
      "For help with this homework, refer to chapters [Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html), [Data Types](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Data-Types.html), and [String Methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html).\n",
      "**HEADS UP!**\n",
      "🚨 To run the code on this page, you need to run the two cells below first🚨\n",
      "moby_dick = open(\"../texts/literature/Moby-Dick.txt\", encoding=\"utf-8\").read()\n",
      "print(moby_dick )\n",
      "## Remixing Herman Melville's *Moby Dick*\n",
      "**1.** First you're going to extract the first few lines of *Moby Dick*. Slice the string `moby_dick` up to the 323rd character.\n",
      "#Your Code Here\n",
      "**2.** Make a variable called `moby_dick_beginning` and assign it the value of your sliced string up to the 323rd character.\n",
      "#Your Code Here\n",
      "**3.** Check the data type of your variable `moby_dick_beginning`.\n",
      "#Your Code Here\n",
      "**4.** Print the variable `moby_dick_beginning` \n",
      "#Your Code Here\n",
      "**5.** Replace the name \"Ishmael\" with a different name. Then assign the result to a new variable called `moby_dick_remix`.\n",
      "#Your Code Here\n",
      "#Your Code Here\n",
      "**6.** Make another variable called `question` and assign it the value `\"Wait who are you? What are you doing here??\\n\\n\"`\n",
      "\n",
      "(Remember that `\\n` means new line. We're just adding the `\\n`s to make the spacing nicer when we print it.)\n",
      "question = #Your Code Here\n",
      "**7.** Add the variables `question` and `moby_dick_remix` together, then print the result.\n",
      "#Your Code Here\n",
      "## Fact-checking with Herman Melville\n",
      "**8.** Go to Herman Melville's [Wikipedia page](https://en.wikipedia.org/wiki/Herman_Melville) and look up his name, birth year, death year, and occupation. Assign these values to the corresponding variables below. Make sure `name` and `occupation` are strings. Make sure `birth_year` and `death_year` are integers.\n",
      "name = #Your Code Here\n",
      "birth_year = #Your Code Here\n",
      "death_year = #Your Code Here\n",
      "occupation = #Your Code Here\n",
      "**9.** Now make one last variable called `age_at_death` and assign it the value of Melville's age at death. You have to calculate Melville's age at death by using your previously created variables `birth_year` and `death_year`.\n",
      "age_at_death = #Your Code Here\n",
      "**10.** Fill in the missing variable in this f-string and then print it.\n",
      "print(f\"{name} was a {occupation}. He was born in {birth_year} and died in {death_year}, which means that he was {MISSING VARIABLE} when he died.\")\n",
      "Example:\n",
      "## Debugging Practice\n",
      "**11.** Run the cell below. You should get an error message. Explain why you received this error message, then fix the code and run it.\n",
      "print(f\"{name} was a {occupation}. He was born in {birth_year} and died in {death_year}.)\n",
      "**Error explanation**:\n",
      "#Your Explanation Here\n",
      "#Your Fixed Code Here\n",
      "**12.**  Run the cell below. It shouldn't work exactly right. Explain what's wrong with it and then correct it.\n",
      "print(\"{name} was a {occupation}. He was born in {birth_year} and died in {death_year}.\")\n",
      "**Error explanation**:\n",
      "#Your Explanation Here\n",
      "#Your Fixed Code\n",
      "**13.** Try to calculate how old Herman Meville would be if he were alive today by running the two cells below. You should get an error message. Explain why you received this error message\n",
      "current_year = \"2020\"\n",
      "current_year - birth_year\n",
      "**Error explanation**:\n",
      "#Your Explanation Here\n",
      "#Your Fixed Code Here\n",
      "## Review \n",
      "### Command Line\n",
      "pwd\n",
      "**14.** Move one directory up from your current working directory.\n",
      "#Your Code Here\n",
      "**15.** Move back to where you started and list the files in your current working directory.\n",
      "#Your Code Here\n",
      "#Your Code Here\n",
      "## HW 10 — Named Entity Recognition\n",
      "[Download relevant files](https://melaniewalsh.org/spacy.zip)\n",
      "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 4 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-10.ipynb\" and submit it to Canvas by Friday, April 23rd at 5pm.\n",
      "\n",
      "*This notebook is exactly the same as the [\"Named Entity Recognition\" lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Named-Entity-Recognition.html), and submitting either notebook is fine.\n",
      "In this lesson, we're going to learn about a text analysis method called **Named Entity Recognition** (NER). This method will help us computationally identify people, places, and things (of various kinds) in a text or collection of texts.\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\" width=\"100%\", border=2>\n",
      "## Why is NER Useful?\n",
      "Named Entity Recognition is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters (something we'll do in a later lesson!). Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping the locations (something we'll also do in a later lesson!).\n",
      "## Natural Language Processing (NLP)\n",
      "Named Entity Recognition is a fundamental task in the field of *natural language processing* (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called *spellcheck*? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!\n",
      "\n",
      "Thanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts are now getting eerily good. (If you don't believe me, check out [this app that will autocomplete your sentences](https://transformer.huggingface.co/doc/gpt2-large/qCNMTfzephfZMBkryTNvSRKQ/edit) with GPT-2, a state-of-the-art text generation model. When I ran it, the model generated a mini-lecture from a \"university professor\" that sounds spookily close to home...)\n",
      "<img src=\"../images/GPT-2.png\", border=2>\n",
      "Open-source NLP tools are getting very good, too. We're going to use one of these open-source tools, the Python library `spaCy`, for our Named Entity Recognition tasks in this lesson.\n",
      "## How spaCy Works\n",
      "<img src=\"../images/Ada-Lovelace-NER.png\", border=2>\n",
      "The screenshot above shows spaCy correctly identifying named entities in Ada Lovelace's *New York Times* obituary (something that we'll test out for ourselves below). How does spaCy know that \"Ada Lovelace\" is a person and that \"1843\" is a date?\n",
      "Well, spaCy doesn't *know*, not for sure anyway. Instead, spaCy is making a very educated guess. This \"guess\" is based on what spaCy has learned about the English language after seeing lots of other examples.\n",
      "That's a colloquial way of saying: spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand). This is similar to our <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html#1)-LDA-is-an-Unsupervised-Algorithm\">topic modeling work</a> from the previous lesson, except our topic model wasn't using labeled data.\n",
      "\n",
      "The English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n",
      "\n",
      "When spaCy identifies people and places in Ada Lovelace's obituary, in other words, its NLP model is actually making a series of *predictions* about the text based on what it has learned about how people and places function in English-language sentences.\n",
      "## NER with spaCy\n",
      "## Install spaCy\n",
      "To use spaCy, we first need to install the library.\n",
      "!pip install -U spacy\n",
      "## Import Libraries\n",
      "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "We're also going to import the `Counter` module for counting people, places, and things, and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting).\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "pd.set_option(\"max_colwidth\", 400)\n",
      "## Download Language Model\n",
      "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:\n",
      "!python -m spacy download en_core_web_sm\n",
      "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies — such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*\n",
      "## Load Language Model\n",
      "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`.\n",
      "import en_core_web_sm\n",
      "nlp = en_core_web_sm.load()\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "## Create a Processed spaCy Document\n",
      "`document = nlp(open(filepath, , encoding='utf-8').read())`\n",
      "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data — named entities, sentence boundaries, parts of speech — and the rest of our work will be devoted to accessing this information.\n",
      "\n",
      "In the cell below, we `open()` and `.read()` Ada Lovelace's obituary. Then we run`nlp()` on the text and create our `document`.\n",
      "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n",
      "\n",
      "document = nlp(open(filepath, encoding='utf-8').read())\n",
      "## spaCy Named Entities\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "Above is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels. To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):\n",
      "displacy.render(document, style=\"ent\")\n",
      "From a quick glance at the text above, we can see that spaCy is doing quite well with NER. But it's definitely not perfect.\n",
      "\n",
      "Though spaCy correctly identifies \"Ada Lovelace\" as a `PERSON` in the first sentence, just a few sentences later it labels her as a `WORK_OF_ART`. Though spaCy correctly identifies \"London\" as a place `GPE` a few paragraphs down, it incorrectly identifies \"Jacquard\" as a place `GPE`, too (when really \"Jacquard\" is a type of loom, named after [Marie Jacquard](https://en.wikipedia.org/wiki/Jacquard_machine)). \n",
      "\n",
      "This inconsistency is very important to note and keep in mind. If we wanted to use spaCy's NER for a project, it would almost certainly require manual correction and cleaning. And even then it wouldn't be perfect. That's why understanding the limitations of this tool is so crucial. While spaCy's NER can be very good for identifying entities in broad strokes, it can't be relied upon for anything exact and fine-grained — not out of the box anyway.\n",
      "## Get Named Entities\n",
      "All the named entities in our `document` can be found in the `document.ents` property. If we check out `document.ents`, we can see all the entities from Ada Lovelace's obituary.\n",
      "document.ents\n",
      "Each of the named entities in `document.ents` contains [more information about itself](https://spacy.io/usage/linguistic-features#accessing), which we can access by iterating through the `document.ents` with a simple `for` loop. `For` each `named_entity` in `document.ents`, we will extract the `named_entity` and its corresponding `named_entity.label_`.\n",
      "for named_entity in document.ents:\n",
      "    print(named_entity, named_entity.label_)\n",
      "To extract just the named entities that have been identified as `PERSON`, we can add a simple `if` statement into the mix:\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        print(named_entity)\n",
      "## Practicing with *Lost in the City*\n",
      "For the rest of this lesson, we're going to work with Edward P. Jones's short story collection *Lost in the City*.\n",
      "<img src=\"https://mybinder.org/static/images/logo_social.png\" width=\"150\" align=\"left\", border=2> *If you're using this Jupyter notebook in Binder (in the cloud), please uncomment the cell below and work with only the first story from _Lost in the City_. The Binder notebook is currently having issues loading the entire collection.*\n",
      "#file = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n",
      "#document = nlp(open(file).read())\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "document = nlp(open(filepath, encoding=\"utf-8\").read())\n",
      "## Get People\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "To extract and count the people identified in *Lost in the City*, we will follow the same model as above, using an `if` statement that will pull out words only if their \"ent\" label matches \"PERSON.\"\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">*While we demonstrate how to extract named entities in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.* \n",
      "Here's the code all together:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "        \n",
      "people_tally = Counter(people)\n",
      "\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "Here's the code broken up. We make a list of all the people identified in *Lost in the City*:\n",
      "people = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        people.append(named_entity.text)\n",
      "people\n",
      "Then we count the unique people in this list with the `Counter()` module:\n",
      "people_tally = Counter(people)\n",
      "people_tally.most_common()\n",
      "Then we make a dataframe from this list with `pd.DataFrame()`:\n",
      "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\n",
      "df\n",
      "To write this dataframe (or any dataframe!) to a CSV file, we can use `df.to_csv()`. To create a CSV file of character counts, uncomment the cell below:\n",
      "#df.to_csv(\"Lost-in-the-City-characters.csv\", encoding='utf-8', index=False)\n",
      "## Get Places\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "To extract and count places, we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"GPE\" or \"LOC.\" These are the type labels for \"counties cities, states\" and \"locations, mountain ranges, bodies of water.\"\n",
      "places = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"GPE\" or named_entity.label_ == \"LOC\":\n",
      "        places.append(named_entity.text)\n",
      "\n",
      "places_tally = Counter(places)\n",
      "\n",
      "df = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\n",
      "df\n",
      "Do you notice anything off about this list...?\n",
      "## Get Streets & Parks\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "To extract and count streets and parks (which show up a lot in *Lost in the City*!), we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"FAC.\" This is the type label for \"buildings, airports, highways, bridges, etc.\"\n",
      "streets = []\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"FAC\":\n",
      "        streets.append(named_entity.text)\n",
      "\n",
      "streets_tally = Counter(streets)\n",
      "\n",
      "df = pd.DataFrame(streets_tally.most_common(), columns = ['street', 'count'])\n",
      "df\n",
      "## Get Works of Art\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the \"ent\" label \"WORK_OF_ART\").\n",
      "> 🐍 **Python Review** 🐍\n",
      "\n",
      ">We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of named entities in a single line of code! Closely examine the first line of code below:\n",
      "works_of_art = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"WORK_OF_ART\"]\n",
      "\n",
      "art_tally = Counter(works_of_art)\n",
      "\n",
      "df = pd.DataFrame(art_tally.most_common(), columns = ['work_of_art', 'count'])\n",
      "df\n",
      "## Your Turn!\n",
      "Now it's your turn to take a crack at NER with a whole new text!\n",
      "\n",
      "|Type Label|Description|\n",
      "|:---:|:---:|\n",
      "|PERSON|People, including fictional.|\n",
      "|NORP|Nationalities or religious or political groups.|\n",
      "|FAC|Buildings, airports, highways, bridges, etc.|\n",
      "|ORG|Companies, agencies, institutions, etc.|\n",
      "|GPE|Countries, cities, states.|\n",
      "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
      "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
      "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
      "|WORK_OF_ART|Titles of books, songs, etc.|\n",
      "|LAW|Named documents made into laws.|\n",
      "|LANGUAGE|Any named language.|\n",
      "|DATE|Absolute or relative dates or periods.|\n",
      "|TIME|Times smaller than a day.|\n",
      "|PERCENT|Percentage, including ”%“.|\n",
      "|MONEY|Monetary values, including unit.|\n",
      "|QUANTITY|Measurements, as of weight or distance.|\n",
      "|ORDINAL|“first”, “second”, etc.|\n",
      "|CARDINAL|Numerals that do not fall under another type.|\n",
      "\n",
      "In this section, you're going to extract and count named entities from Barack Obama's memoir *The Audacity of Hope*. We're exploring Obama's memoir because it's chock full of named entities.\n",
      "Read in and process the text file\n",
      "file = \"../texts/literature/Obama-The-Audacity-of-Hope.txt\"\n",
      "\n",
      "document = nlp(open(file, encoding='utf-8').read())\n",
      "**1.** Choose a named entity from the possible spaCy named entities listed above. Extract, count, and make a dataframe from the most frequent named entities (of the type that you've chosen) in *The Audacity of Hope*. If you need help, study the examples above.\n",
      "#Your Code Here 👇 \n",
      "\n",
      "**2.** What is a result from this NER extraction that conformed to your expectations, that you find obvious or predictable? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**3.** What is a result from this NER extraction that defied your expectations, that you find curious or counterintuitive? Why?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "**4.** What's an insight that you might be able to glean about *The Audacity of Hope* based on your NER extraction?\n",
      "**#**Your answer here. (Double click this cell to type your answer.)\n",
      "Student Hours with Prof. Walsh <br>\n",
      "Thursday 1-3pm by appt https://melaniewalsh.youcanbook.me // Gates 211\n",
      "\n",
      "TA Study Hall<br>\n",
      "Wednesday 5:30-6:30pm // Rhodes 408 <br>\n",
      "Friday 2:30-3:30pm // Rhodes 597\n",
      "## \\*Revised\\* Course Schedule \n",
      "\n",
      "| Course Unit | Technical Lesson | Date | Technical Reading | Critical/Cultural Reading | Assignments |\n",
      "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| What Is Cultural Analytics? |  | Tues 1/21 |  |  |  |\n",
      "| The Command Line | [The Command Line](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Command-Line/The-Command-Line.html) | Th 1/23 | [The Command Line](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Command-Line/The-Command-Line.html) | [“The Yellow Wallpaper,”](https://www.nlm.nih.gov/exhibition/theliteratureofprescription/exhibitionAssets/digitalDocs/The-Yellow-Wall-Paper.pdf)**\\*** <br>Charlotte Perkins Gilman | [HW 1](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-1-Command-Line.html) <br> (Due Friday 5pm) |\n",
      "| Cultural Data + Python Fundamentals | [Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html) | Tues 1/28 | [How to Use Jupyter](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/How-to-Use-Jupyter.html) // <br> [The Life and Anatomy of a Python Script](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Life-Anatomy-Python-Script.html) | “What Gets Counted Counts,”**\\*** <br>*Data Feminism*, <br>Lauren Klein and Catherine D'Ignazio | [HW 1.5](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-1-5-Installation.html) <br> (Due Tuesday 9am) |\n",
      "|  | [Data Types](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Data-Types.html) & [String Methods](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/String-Methods.html) | Th 1/30 | [Character Encoding](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Character-Encoding.html) | [“I Can Text You A Pile of Poo, But I Can’t Write My Name,”](https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name)**\\*** <br> Aditya Mukerjee | [HW 2](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-2-Variables-Data-Types.html) (Due Sunday 5pm) <br> |\n",
      "|  | [Conditionals & Comparisons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Conditionals-Comparisons.html) | Tues 2/4 |  | <a href=http://crdh.rrchnm.org/essays/v01-10-(re)-humanizing-data/>“(Re)Humanizing Data: Digitally Navigating the Bellevue Almshouse”<a>**\\*** <br> Anelise Hanson Shrout |  |\n",
      "|  | [Lists & Loops](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Lists-Loops.html) | Th 2/6 |  | [“Data Biographies,”](https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/) <br>Heather Krause | HW 3 ([Part I](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-3-Conditionals-Comparisons.html) & [II](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-3-Lists-Loops.html) Friday 5pm) |\n",
      "|  | [Lists & Loops Plus Modules](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html) | Tues 2/11 |  | \"Explore [*Slave Voyages*](https://www.slavevoyages.org/) // <br><a href=https://read.dukeupress.edu/social-text/article/36/4%20(137)/57/137032/Markup-BodiesBlack-Life-Studies-and-Slavery-Death>“Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads”</a>**\\*** <br>Jessica Marie Johnson |  |\n",
      "|  | [Pandas](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Cultural-Data-Analysis/Pandas.html) | Th 2/13 | [Dictionaries](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Dictionaries.html) |  | [HW 4](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-4-Pandas.html) (Due Tuesday 9am) |\n",
      "|  | [Functions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Functions.html) & [Pandas (Exploratory Data Analysis)](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Cultural-Data-Analysis/Pandas-EDA.html) | Tues 2/18 |  | [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”,](https://pudding.cool/2017/03/film-dialogue/) <br>Hannah Anderson and Matt Daniels |  |\n",
      "|  |  | Th 2/20 | [Pandas (Targeted Analysis)](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Cultural-Data-Analysis/Pandas-Targeted.html) | [Film Dialogue FAQ](https://medium.com/@matthew_daniels/faq-for-the-film-dialogue-by-gender-project-40078209f751); Know data biography for The Pudding film dialogue data | [HW 5](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-5-Functions-Pandas.html) (Due following Thursday 9am) |\n",
      "| *February Break* |  | Tues 2/25 |  | *February Break* |  |\n",
      "| Collecting Cultural Data | [Web Scraping](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Web-Scraping.html) | Th 2/27 |  | [“The Largest Vocabulary in Hip-Hop,”](https://pudding.cool/projects/vocabulary/index.html) <br>Matt Daniels |  |\n",
      "|  | [Web Scraping Plus Regular Expressions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Web-Scraping-Plus-Regex.html) | Tues 3/3 |  | [“The Secretive Company That Might End Privacy as We Know It”](https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html)**\\*** <br>Kashmir Hill |  |\n",
      "|  | [APIs](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/APIs.html) & [GitHub](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Git-Github.html) | Th 3/5 | [Install Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git); [A Dead Simple Intro to GitHub for the Non-Technical](https://medium.com/crowdbotics/a-dead-simple-intro-to-github-for-the-non-technical-f9d56410a856) |  | [HW 6](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-6-Twitter-Setup.html) (Due Tuesday 9am) |\n",
      "|  | [Twitter Data Collection](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Collection.html) | Tues 3/10 |  | “#GirlsLikeUs: Trans advocacy and community building online,”**\\*** <br>Sarah J Jackson, Moya Bailey, and Brooke Foucault Welles |  |\n",
      "|  | [Twitter Data Analysis](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Analysis.html) | Th 3/12 |  | [“How China Unleashed Twitter Trolls to Discredit Hong Kong’s Protesters,”](https://www.nytimes.com/interactive/2019/09/18/world/asia/hk-twitter.html) <br>Raymond Zhong, Steven Lee Myers and Jin Wu | [HW 7](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-7-Twitter-Data.html) (Due Tuesday April 7 9am) |\n",
      "| No Class | | Tues 3/17 | |   |  |\n",
      "| No Class |  | Th 3/19 |  |   |  |\n",
      "| No Class |  | Tues 3/24 |  |  |  |\n",
      "| No Class |  | Th 3/26 |  |   |  |\n",
      "| *Spring Break* |  | Tues 3/31 |   | Recommended Reading: [“The Transformation of Gender in English-Language Fiction,”](https://culturalanalytics.org/article/11035) <br>Ted Underwood, David Bamman, and Sabrina Lee |  |\n",
      "| *Spring Break* |  | Th 4/2 |  |  |  |\n",
      "|  | Twitter Data Wrap-Up // The Path Forward | Tues 4/7 |  | Recommended Reading: <a href=\"https://cmci.colorado.edu/~cafi5706/ICWSM2020_datascraping.pdf\">No Robots, Spiders, or Scrapers: Legal and Ethical Regulation of Data Collection Methods in Social Media Terms of Service</a>, Casey Fiesler, Nathan Beard, Brian C. Keegan |  |\n",
      "| Text Analysis | [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/TF-IDF.html) | Th 4/9 |  | | [HW 8](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-8-TF-IDF.html)  (Due Friday) |\n",
      "|  | [Topic Modeling](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html) | Tues 4/14 |  | [“Narrative Paths and Negotiation of Power in Birth Stories,”](https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf)**\\*** <br>Maria Antoniak, David Mimno, and Karen Levy  |  |\n",
      "|  |  | Th 4/16 | |  | [HW 9](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-9-Topic-Modeling.html) + Discussion Post (Due Friday) |\n",
      "|  | [Named Entity Recognition](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Named-Entity-Recognition.html) | Tues 4/21 |  |  Excerpts, *Lost in the City* **\\***, <br> Edward P. Jones| |\n",
      "|  | [Part-of-Speech Tagging](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/POS-Keywords.html) | Th 4/23 |  |[\"Introduction,\"](https://iopn.library.illinois.edu/scalar/lost-in-the-city-a-exploration-of-edward-p-joness-short-fiction-/the-introduction-an-authors-note) [\"A Multimedia Literary Analysis,” (Sections 1-3)](https://iopn.library.illinois.edu/scalar/lost-in-the-city-a-exploration-of-edward-p-joness-short-fiction-/lost-in-the-city---section-2?path=chapter-3-lost-in-the-city) <br> *Lost in the City: An Exploration of Edward P. Jones's Short Fiction* <br>Kenton Rambsy and Peace Ossom-Williamson  | [HW 10](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Homework/HW-10-NER.html) + Discussion Post (Due Friday) |\n",
      "| Network Analysis |  | Tues 4/28 | [Scott Weingart, \"Demystifying Networks\"](http://journalofdigitalhumanities.org/1-1/demystifying-networks-by-scott-weingart/) |  [“Network of Thrones,”](https://www.maa.org/sites/default/files/pdf/Mathhorizons/NetworkofThrones%20%281%29.pdf) <br>Andrew Beveridge and Jie Shan // <br>[\"\"Mathematicians mapped out every Game of Thrones relationship to find the main character,”](https://qz.com/650796/mathematicians-mapped-out-every-game-of-thrones-relationship-to-find-the-main-character/) <br>Adam Epstein \" |  |\n",
      "| Mapping |  | Th 4/30 |  |  [Torn Apart / Separados](http://xpmethod.plaintext.in/torn-apart/volume/2/index) <br>Manan Ahmed et al // <br>['ICE Is Everywhere': Using Library Science to Map the Separation Crisis,](https://www.wired.com/story/ice-is-everywhere-using-library-science-to-map-child-separation/) <br>Emily Dreyfuss // [“How an internet mapping glitch turned a random Kansas farm into a digital hell”](https://splinternews.com/how-an-internet-mapping-glitch-turned-a-random-kansas-f-1793856052) <br>Kashmir Hill | Discussion Post (Due Friday)  |\n",
      "| Final Project |  | Tues 5/5 |  |  |  |\n",
      "| Final Project  |  | Thu 5/7 |  |  |  |\n",
      "|  Final Project | Conclusions | Tues 5/12 |  |  | |\n",
      "|  |  | Monday 5/18 |  |  | [Final Project Due](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Final-Project/Final-Project.html) |\n",
      "\n",
      "## Notable Events Calendar\n",
      "Find our [Notable Events calendar here](https://melaniewalsh.github.io/Intro-Cultural-Analytics/notable-events.html)\n",
      "## Make a Character Network\n",
      "This notebook demonstrates how to create a social network of people mentioned in a text based on how often people appear within a certain distance of one another.\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import networkx \n",
      "import itertools\n",
      "## Load spaCy Language Model\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "nlp = en_core_web_sm.load()\n",
      "## Character Networks For Shorter Texts\n",
      "This section demonstrates how to create a character network from a text if you can process the entire text with spaCy at one time (mostly shorter texts).\n",
      "## Read in Text File\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "## Process Text\n",
      "document = nlp(text)\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Then output this list of spaCy's identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in my_list_of_characters:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe 's from character name\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        #Get the character index number from the text\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 50 characters of one another, add them to the `edge_list`. To change the number of characters, simply change the `threshold_distance` variable below:\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 50\n",
      "\n",
      "#If two people fall within 50 characters of one another, add them to the edge list\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        \n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Lost-in-the-City-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Lost-in-the-City-network.csv\")\n",
      "## Character Networks For Longer Texts\n",
      "This section demonstrates how to create a character network from a text if you cannot process the entire text with spaCy at one time and need to chunk it into smaller documents (mostly longer texts).\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath).read()\n",
      "## Chunk Text By Number of Chunks\n",
      "To chunk text by a specific number of chunks, choose a `number_of_chunks` value and run the cell below. The current default value is 80 chunks. \n",
      "import math\n",
      "number_of_chunks = 80\n",
      "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
      "chunked_text = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
      "The code above is dividing the total number of characters in the text `len(text)` by the number of chunks you want, then rounding up `math.ceil()` to a whole number. This is calculating the necessary chunk size for the number of chunks you want. The final line iterates through the text and creates slices at the necessary chunk size.\n",
      "## Or Chunk Text By Line Breaks\n",
      "#chunked_text= text.split('\\n')\n",
      "## Process Chunked Text\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document(s)\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "document_length = 0\n",
      "\n",
      "for document in chunked_documents:\n",
      "    document_length += len(document.text)\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.text in my_list_of_characters:\n",
      "            person = named_entity.text\n",
      "\n",
      "            #Remove apostrophe 's from character name\n",
      "            person = person.replace(\"’s\", \"\").strip()\n",
      "            \n",
      "            #Get the character index number from the text\n",
      "            person_index =  (document_length - named_entity.start_char)\n",
      "\n",
      "            all_people_matches.append(person)\n",
      "            all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 100 characters of one another, add them to the `edge_list`.\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 100\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Filter Network By Edge Weights\n",
      "character_network[character_network['Weight'] > 2]\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Little-Women-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Little-Women-network.csv\")\n",
      "## Making Interactive Network Visualizations with Bokeh\n",
      "This notebook includes code for creating interactive network visualizations with the Python libraries [NetworkX](https://networkx.github.io/) and [Bokeh](https://docs.bokeh.org/en/latest/index.html). The notebook begins with code for a basic network visualization then progressively demonstrates how to add more information and functionality, such as:\n",
      "\n",
      "- sizing and coloring nodes by degree\n",
      "- sizing and coloring nodes by modularity class\n",
      "- adding responsive highlighting when hovering over nodes and edges\n",
      "- adding node labels\n",
      "\n",
      "For a simpler, single function that includes all of the above functionality, see Make an Interactive Network Viz — Quick Function.\n",
      "## Import Libraries\n",
      "import pandas as pd\n",
      "import networkx\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "## Install and Import Bokeh\n",
      "#!pip install bokeh\n",
      "from bokeh.io import output_notebook, show, save\n",
      "from bokeh.palettes import Spectral8\n",
      "from bokeh.models import (Range1d, Circle, MultiLine,\n",
      "EdgesAndLinkedNodes, NodesAndLinkedEdges, ColumnDataSource, LabelSet)\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.plotting import from_networkx\n",
      "from bokeh.palettes import Blues8, Reds8, Purples8, Oranges8, Viridis256, Spectral11\n",
      "from bokeh.transform import linear_cmap\n",
      "To view interactive Bokeh visualizations in a Jupyter notebook, you need to run this cell:\n",
      "output_notebook()\n",
      "## Create Network From Pandas DataFrame\n",
      "We read in a CSV file of *Game of Thrones* network data from Andrew Beveridge and Jie Shan's paper, [\"Network of Thrones.\"](https://www.maa.org/sites/default/files/pdf/Mathhorizons/NetworkofThrones%20%281%29.pdf)\n",
      "\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "Then we make a network with `networkx.from_pandas_edgelist()`:\n",
      "G = networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Basic Network \n",
      "The code below shows how to make a basic network viz that includes Hover Tooltips (a text box that will display when a user hovers over nodes) as well as Zoom and Pan/Drag functionality.\n",
      "\n",
      "For more details about visualizing network graphs with Bokeh, see [the documentation](https://docs.bokeh.org/en/latest/docs/user_guide/graph.html?highlight=networks).\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [(\"Character\", \"@index\")]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object with spring layout\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node size and color\n",
      "network_graph.node_renderer.glyph = Circle(size=15, fill_color='skyblue')\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "\n",
      "#Add network graph to the plot\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Bokeh Toolbar\n",
      "\n",
      "<img src=\"../images/Bokeh-toolbar.png\", border=2>\n",
      "## Network with Nodes Sized and Colored By Attribute (Degree)\n",
      "The code below shows how to size and color nodes by degree.\n",
      "Calculate degree for each node and add as node attribute\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "Slightly adjust degree so that the nodes with very small degrees are still visible\n",
      "number_to_adjust_by = 5\n",
      "adjusted_node_size = dict([(node, degree+number_to_adjust_by) for node, degree in networkx.degree(G)])\n",
      "networkx.set_node_attributes(G, name='adjusted_node_size', values=adjusted_node_size)\n",
      "Import Bokeh color palettes\n",
      "from bokeh.palettes import Blues8, Reds8, Purples8, Oranges8, Viridis8, Spectral8\n",
      "from bokeh.transform import linear_cmap\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'adjusted_node_size'\n",
      "\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\")\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\\\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as spectrum of color palette)\n",
      "minimum_value_color = min(network_graph.node_renderer.data_source.data[color_by_this_attribute])\n",
      "maximum_value_color = max(network_graph.node_renderer.data_source.data[color_by_this_attribute])\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=linear_cmap(color_by_this_attribute, color_palette, minimum_value_color, maximum_value_color))\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network with Nodes Colored By Attribute (Community)\n",
      "The code below shows how to size and color nodes by modularity class.\n",
      "Calculate communities\n",
      "from networkx.algorithms import community\n",
      "communities = community.greedy_modularity_communities(G)\n",
      "Add modularity class and color as attributes to network graph\n",
      "from bokeh.palettes import Spectral8\n",
      "\n",
      "# Create empty dictionaries\n",
      "modularity_class = {}\n",
      "modularity_color = {}\n",
      "#Loop through each community in the network\n",
      "for community_number, community in enumerate(communities):\n",
      "    #For each member of the community, add their community number and a distinct color\n",
      "    for name in community: \n",
      "        modularity_class[name] = community_number\n",
      "        modularity_color[name] = Spectral8[community_number]\n",
      "# Add modularity class and color as attributes from the network above\n",
      "networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "networkx.set_node_attributes(G, modularity_color, 'modularity_color')\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'modularity_color'\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\"),\n",
      "         (\"Modularity Class\", \"@modularity_class\"),\n",
      "        (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset, tap\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as category from attribute)\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=color_by_this_attribute)\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network with Responsive Highlighting\n",
      "The code below shows how to create responsive highlighting when a user hovers over nodes or edges, which you can read more about in [Bokeh's NetworkX Integration documentation](https://docs.bokeh.org/en/latest/docs/user_guide/graph.html?highlight=networks#interaction-policies). \n",
      "from bokeh.models import EdgesAndLinkedNodes, NodesAndLinkedEdges\n",
      "\n",
      "#Choose colors for node and edge highlighting\n",
      "node_highlight_color = 'white'\n",
      "edge_highlight_color = 'black'\n",
      "\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'modularity_color'\n",
      "\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\"),\n",
      "         (\"Modularity Class\", \"@modularity_class\"),\n",
      "        (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as category from attribute)\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=color_by_this_attribute)\n",
      "#Set node highlight colors\n",
      "network_graph.node_renderer.hover_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "network_graph.node_renderer.selection_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "#Set edge highlight colors\n",
      "network_graph.edge_renderer.selection_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "network_graph.edge_renderer.hover_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "\n",
      "    #Highlight nodes and edges\n",
      "network_graph.selection_policy = NodesAndLinkedEdges()\n",
      "network_graph.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network with Labels\n",
      "The code below shows how to create node labels, which you can read more about in [Bokeh's label documentation](https://docs.bokeh.org/en/latest/docs/user_guide/annotations.html#labels).\n",
      "from bokeh.models import EdgesAndLinkedNodes, NodesAndLinkedEdges\n",
      "from bokeh.models import ColumnDataSource, LabelSet\n",
      "\n",
      "#Choose colors for node and edge highlighting\n",
      "node_highlight_color = 'white'\n",
      "edge_highlight_color = 'black'\n",
      "\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'modularity_color'\n",
      "\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\"),\n",
      "         (\"Modularity Class\", \"@modularity_class\"),\n",
      "        (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as category from attribute)\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=color_by_this_attribute)\n",
      "#Set node highlight colors\n",
      "network_graph.node_renderer.hover_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "network_graph.node_renderer.selection_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.3, line_width=1)\n",
      "#Set edge highlight colors\n",
      "network_graph.edge_renderer.selection_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "network_graph.edge_renderer.hover_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "\n",
      "    #Highlight nodes and edges\n",
      "network_graph.selection_policy = NodesAndLinkedEdges()\n",
      "network_graph.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "#Add Labels\n",
      "x, y = zip(*network_graph.layout_provider.graph_layout.values())\n",
      "node_labels = list(G.nodes())\n",
      "source = ColumnDataSource({'x': x, 'y': y, 'name': [node_labels[i] for i in range(len(x))]})\n",
      "labels = LabelSet(x='x', y='y', text='name', source=source, background_fill_color='white', text_font_size='10px', background_fill_alpha=.7)\n",
      "plot.renderers.append(labels)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network Analysis\n",
      "[Download relevant files here](https://melaniewalsh.org/Network-Analysis.zip)\n",
      "In this lesson, we're going to learn about **network analysis**. Network analysis will help us better understand the complex relationships between groups of people, fictional characters, or other kinds of things.\n",
      "## Import Libraries\n",
      "To analyze network data, we're going to use the Python library [NetworkX](https://networkx.github.io/).\n",
      "!pip install networkx\n",
      "import networkx \n",
      "import pandas as pd\n",
      "pd.set_option('max_rows', 400)\n",
      "import matplotlib.pyplot as plt\n",
      "## *Game of Thrones* Network\n",
      "The network data that we're going to use in this lesson is taken from Andrew Beveridge and Jie Shan's paper, [\"Network of Thrones.\"](https://www.maa.org/sites/default/files/pdf/Mathhorizons/NetworkofThrones%20%281%29.pdf) They calculated how many times each Game of Thrones character appeared within 15 words of another character in *A Storm of Swords*, the third book in the series.\n",
      "\n",
      "For example:\n",
      "> \"It was the bastard **Jon Snow** who had taken that from him, him and his fat friend **Sam Tarly**.\"\n",
      "> \"Lucky it might be, and red it certainly was, but **Ygritte**’s hair was such a tangle that **Jon** was tempted to ask her if she only brushed it at the changing of the seasons.\"\n",
      "> \"**Arya** gave **Gendry** a sideways look. *He said it with me, like **Jon** used to do, back in Winterfell.* She missed **Jon Snow** the most of all her brothers.\"\"\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "## Create a Network From a Pandas DataFrame\n",
      "G = networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Output a Network File\n",
      "networkx.write_graphml(G, 'GOT-network.graphml')\n",
      "## Draw a Simple Network\n",
      "networkx.draw(G)\n",
      "plt.figure(figsize=(8,8))\n",
      "networkx.draw(G, with_labels=True, node_color='skyblue', width=.3, font_size=8)\n",
      "## Calculate Degree\n",
      "Who has the most number of connections in the network?\n",
      "networkx.degree(G)\n",
      "Make the degree values a `dict`ionary, then add it as a network \"attribute\" with `networkx.set_node_attributes()`\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "Make a Pandas dataframe from the degree data `G.nodes(data='degree')`, then sort from highest to lowest\n",
      "degree_df = pd.DataFrame(G.nodes(data='degree'), columns=['node', 'degree'])\n",
      "degree_df = degree_df.sort_values(by='degree', ascending=False)\n",
      "degree_df\n",
      "Plot the nodes with the highest degree values\n",
      "num_nodes_to_inspect = 10\n",
      "degree_df[:num_nodes_to_inspect].plot(x='node', y='degree', kind='barh').invert_yaxis()\n",
      "## Calculate Weighted Degree\n",
      "Who has the most number of connections in the network (if you factor in edge weight)?\n",
      "networkx.degree(G, weight='Weight')\n",
      "Make the weighted degree values a `dict`ionary, then add it as a network \"attribute\" with `networkx.set_node_attributes()`\n",
      "weighted_degrees = dict(networkx.degree(G, weight='Weight'))\n",
      "networkx.set_node_attributes(G, name='weighted_degree', values=weighted_degrees)\n",
      "Make a Pandas dataframe from the degree data `G.nodes(data='weighted_degree')`, then sort from highest to lowest\n",
      "weighted_degree_df = pd.DataFrame(G.nodes(data='weighted_degree'), columns=['node', 'weighted_degree'])\n",
      "weighted_degree_df = weighted_degree_df.sort_values(by='weighted_degree', ascending=False)\n",
      "weighted_degree_df\n",
      "Plot the nodes with the highest weighted degree values\n",
      "num_nodes_to_inspect = 10\n",
      "weighted_degree_df[:num_nodes_to_inspect].plot(x='node', y='weighted_degree', color='orange', kind='barh').invert_yaxis()\n",
      "## Calculate Betweenness Centrality Scores\n",
      "Who connects the most other nodes in the network?\n",
      "networkx.betweenness_centrality(G)\n",
      "betweenness_centrality = networkx.betweenness_centrality(G)\n",
      "Add `betweenness_centrality` (which is already a dictionary) as a network \"attribute\" with `networkx.set_node_attributes()`\n",
      "networkx.set_node_attributes(G, name='betweenness', values=betweenness_centrality)\n",
      "Make a Pandas dataframe from the betweenness data `G.nodes(data='betweenness')`, then sort from highest to lowest\n",
      "betweenness_df = pd.DataFrame(G.nodes(data='betweenness'), columns=['node', 'betweenness'])\n",
      "betweenness_df = betweenness_df.sort_values(by='betweenness', ascending=False)\n",
      "betweenness_df\n",
      "Plot the nodes with the highest betweenness centrality scores\n",
      "num_nodes_to_inspect = 10\n",
      "betweenness_df[:num_nodes_to_inspect].plot(x='node', y='betweenness', color='green', kind='barh').invert_yaxis()\n",
      "## Communities\n",
      "Who forms distinct communities within this network?\n",
      "from networkx.algorithms import community\n",
      "Calculate communities with `community.greedy_modularity_communities()`\n",
      "communities = community.greedy_modularity_communities(G)\n",
      "communities\n",
      "Make a `dict`ionary by looping through the communities and, for each member of the community, adding their community number\n",
      "# Create empty dictionary\n",
      "modularity_class = {}\n",
      "#Loop through each community in the network\n",
      "for community_number, community in enumerate(communities):\n",
      "    #For each member of the community, add their community number\n",
      "    for name in community:\n",
      "        modularity_class[name] = community_number\n",
      "Add modularity class to the network as an attribute\n",
      "networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "Make a Pandas dataframe from modularity class network data `G.nodes(data='modularity_class')`\n",
      "communities_df = pd.DataFrame(G.nodes(data='modularity_class'), columns=['node', 'modularity_class'])\n",
      "communities_df = communities_df.sort_values(by='modularity_class', ascending=False)\n",
      "communities_df\n",
      "Inspect each community in the network\n",
      "communities_df[communities_df['modularity_class'] == 4]\n",
      "communities_df[communities_df['modularity_class'] == 3]\n",
      "communities_df[communities_df['modularity_class'] == 2]\n",
      "communities_df[communities_df['modularity_class'] == 1]\n",
      "communities_df[communities_df['modularity_class'] == 0]\n",
      "Plot a sample of 40 characters with their modularity class indicated by a star\n",
      "import seaborn as sns\n",
      "#Set figure size\n",
      "plt.figure(figsize=(4,12))\n",
      "\n",
      "#Plot a categorical scatter plot from the dataframe communities_df.sample(40)\n",
      "ax =sns.stripplot(x='modularity_class', y='node', data=communities_df.sample(40),\n",
      "              hue='modularity_class', marker='*',size=15)\n",
      "#Set legend outside the plot with bbox_to_anchor\n",
      "ax.legend(loc='upper right',bbox_to_anchor=(1.5, 1), title='Modularity Class')\n",
      "ax.set_title(\"GOT Characters By Modularity Class\\n(Random Sample)\")\n",
      "plt.show()\n",
      "Plot all GOT characters with their modularity class indicated by a star (tak\n",
      "start_time = datetime.datetime.now()\n",
      "\n",
      "plt.figure(figsize=(4,25))\n",
      "\n",
      "ax =sns.stripplot(x='modularity_class', y='node', data=communities_df,\n",
      "              hue='modularity_class', marker='*',size=15)\n",
      "\n",
      "ax.legend(loc='upper right',bbox_to_anchor=(1.5, 1), title='Modularity Class')\n",
      "ax.set_title(\"GOT Characters By Modularity Class\")\n",
      "plt.show()\n",
      "print(datetime.datetime.now() - start_time)\n",
      "## All Network Metrics\n",
      "Create a Pandas dataframe of all network attributes by creating a `dict`ionary of `G.nodes(data=True)`...\n",
      "dict(G.nodes(data=True))\n",
      "...and then [transposing it](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.T.html) (flipping the columns and rows) with `.T`\n",
      "nodes_df = pd.DataFrame(dict(G.nodes(data=True))).T\n",
      "nodes_df\n",
      "nodes_df.sort_values(by='betweenness', ascending=False)\n",
      "## Network Analysis\n",
      "* Network Analysis\n",
      "* Character Networks\n",
      "* Making Interactive Network Visualizations with Bokeh\n",
      "* Make an Interactive Network Viz — Quick Function\n",
      "\n",
      "## Make an Interactive Network Viz — Quick Function\n",
      "This notebook includes a single easy function for creating interactive network visualizations with the Python libraries [NetworkX](https://networkx.github.io/) and [Bokeh](https://docs.bokeh.org/en/latest/index.html). This function combines the various features demonstrated in Making Interactive Network Visualizations with Bokeh.\n",
      "## Import Libraries\n",
      "import pandas as pd\n",
      "import networkx \n",
      "from networkx.algorithms import community\n",
      "import matplotlib.pyplot as plt\n",
      "!pip install \"bokeh > 2\"\n",
      "from bokeh.io import output_notebook, show, save\n",
      "from bokeh.palettes import Spectral8\n",
      "from bokeh.models import (Range1d, Circle, MultiLine,\n",
      "EdgesAndLinkedNodes, NodesAndLinkedEdges, ColumnDataSource, LabelSet)\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.plotting import from_networkx\n",
      "from bokeh.palettes import Blues8, Reds8, Purples8, Oranges8, Viridis256, Spectral11\n",
      "from bokeh.transform import linear_cmap\n",
      "output_notebook()\n",
      "## Read in Network Data\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "G = networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Make an Interactive Network — Quick Function\n",
      "The code below creates a comprehensive function called `make_interactive_network()`. After you run this big cell, you can make networks simply by calling the function `make_interactive_network()` below.\n",
      "def make_interactive_network(G, weight=False, node_size=15,\n",
      "                             node_color='skyblue', color_type=None,\n",
      "                             color_order=False, color_palette=Blues8, title='Network', labels=False,\n",
      "                             save_as=False, node_highlight_color=False, edge_highlight_color=False):\n",
      "    \n",
      "    #Calculate Degree and Adjusted Degree Size\n",
      "    degrees = dict(networkx.degree(G, weight=weight))\n",
      "    networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "    \n",
      "    def adjust(degree, weight):\n",
      "        if weight == False:\n",
      "            return degree + 5\n",
      "        else:\n",
      "            return (degree / 10) + 5\n",
      "    degree_adjusted = dict([(node, adjust(degree, weight)) for node, degree in networkx.degree(G, weight=weight)])\n",
      "    networkx.set_node_attributes(G, name='degree_adjusted', values=degree_adjusted)\n",
      "    \n",
      "    #Calculate Betweenness Centrality\n",
      "    betweenness_centrality = networkx.betweenness_centrality(G)\n",
      "    networkx.set_node_attributes(G, name='betweenness', values=betweenness_centrality)\n",
      "    \n",
      "    #Calculate Modularity Classes\n",
      "    communities = community.greedy_modularity_communities(G)\n",
      "    modularity_class = {name:community_number for community_number, community in enumerate(communities) for name in community}\n",
      "    modularity_color = dict((name, Spectral11[community_number]) if community_number < 11 else (name,'gray') for community_number, community in enumerate(communities) for name in community )\n",
      "    networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "    networkx.set_node_attributes(G, modularity_color, 'modularity_color')\n",
      "    \n",
      "    #Set hover categories\n",
      "    HOVER_TOOLTIPS = [\n",
      "           (\"Character\", \"@index\"),\n",
      "            (\"Degree\", \"@degree\"),\n",
      "            (\"Betweenness\", \"@betweenness{1.111}\"),\n",
      "             (\"Modularity Class\", \"@modularity_class\"),\n",
      "            (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "    ]\n",
      "\n",
      "    #Create a plot — set dimensions, toolbar, and title\n",
      "    plot = figure(tooltips = HOVER_TOOLTIPS, toolbar_location='left',\n",
      "                  tools=\"pan,wheel_zoom,save,reset, tap\", active_scroll='wheel_zoom',\n",
      "                x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "    plot.sizing_mode = 'scale_width'\n",
      "    \n",
      "    #Remove grid and axes from plot\n",
      "    plot.xgrid.visible = False\n",
      "    plot.ygrid.visible = False\n",
      "    plot.axis.visible = False\n",
      "    \n",
      "    #Create a network graph object\n",
      "    # https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "\n",
      "    network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "    \n",
      "    #Set node sizes and colors\n",
      "    if color_order == 'reverse':\n",
      "        color_palette = color_palette[::-1]\n",
      "    if node_color == 'degree' or node_color == 'betweenness':\n",
      "        minimum_value_color = min(network_graph.node_renderer.data_source.data[node_color])\n",
      "        maximum_value_color = max(network_graph.node_renderer.data_source.data[node_color])\n",
      "        node_color = linear_cmap(node_color, color_palette, minimum_value_color, maximum_value_color)\n",
      "    else:\n",
      "        node_color =  node_color\n",
      "    \n",
      "    if node_highlight_color == False:\n",
      "        node_highlight_color = node_color\n",
      "    if edge_highlight_color == False:\n",
      "        edge_highlight_color = 'black'\n",
      "    \n",
      "    #Add nodes\n",
      "    network_graph.node_renderer.glyph = Circle(size=node_size, fill_color=node_color)\n",
      "    \n",
      "    #Add edges\n",
      "    network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.3, line_width=1)\n",
      "    \n",
      "    #Set node highlight colors\n",
      "    network_graph.node_renderer.hover_glyph = Circle(size=node_size, fill_color=node_highlight_color, line_width=2)\n",
      "    network_graph.node_renderer.selection_glyph = Circle(size=node_size, fill_color=node_highlight_color, line_width=2)\n",
      "\n",
      "    #Set edge highlight colors\n",
      "    network_graph.edge_renderer.selection_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "    network_graph.edge_renderer.hover_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "\n",
      "    #Highlight adjacent nodes and edges\n",
      "    network_graph.selection_policy = NodesAndLinkedEdges()\n",
      "    network_graph.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "    plot.renderers.append(network_graph)\n",
      "\n",
      "    #Add labels\n",
      "    if labels == True:\n",
      "        x, y = zip(*network_graph.layout_provider.graph_layout.values())\n",
      "        node_labels = list(G.nodes())\n",
      "        source = ColumnDataSource({'x': x, 'y': y, 'name': [node_labels[i] for i in range(len(x))]})\n",
      "        labels = LabelSet(x='x', y='y', text='name', source=source, background_fill_color='white', text_font_size='10px', background_fill_alpha=.7)\n",
      "        plot.renderers.append(labels)\n",
      "\n",
      "    show(plot)\n",
      "    \n",
      "    #Save as HTML file\n",
      "    if save_as != False:\n",
      "        save(plot, filename=f\"{save_as}\", title=title)\n",
      "## Parameters\n",
      "Here are the parameters that you can change and customize with the `make_interactive_network()` function:\n",
      "\n",
      "`node_size` \n",
      "\n",
      "You can size nodes by attributes such as \"degree_adjusted\", \"degree,\" \"betwenness,\" or another attribute that you add to the network. Or you can uniformly size by a number. **Default = 15**.\n",
      "\n",
      "`node_color`\n",
      "\n",
      "You can color nodes by attributes such as \"degree,\" \"betwenness,\" \"modularity_color,\" or another attribute that you add to the network. Or you can uniformly color nodes by a specific color (e.g. \"green\"). **Default = \"skyblue\"**.\n",
      "\n",
      "`color_order`\n",
      "\n",
      "To reverse the order of a color palette (e.g. so that high degree nodes will be darker rather than lighter), you can set the parameter to \"reverse.\" **Default = False**.\n",
      "\n",
      "`color_palette`\n",
      "\n",
      "You can choose a specific color palette from the following imported palettes: Blues8, Reds8, Purples8, Oranges8, Viridis256, Spectral11. **Default = Blues8**. \n",
      "\n",
      "`title` \n",
      "\n",
      "You can choose a title for your network. **Default = \"Network\"**.\n",
      "\n",
      "`labels`  \n",
      "\n",
      "You can choose to show labels by setting `labels=True`. **Default = False.**\n",
      "\n",
      "`save_as`\n",
      "\n",
      "You can save your network as an HTML file by setting `save_as` to a filename. **Default = False**.\n",
      "\n",
      "`node_highlight_color`  \n",
      "\n",
      "You can select a color that nodes will be highlighted when hovered over or selected by choosing a specific color. **Default = False**.\n",
      "\n",
      "`edge_highlight_color`\n",
      "\n",
      "You can select a color that edges will be highlighted when hovered over or selected by choosing a specific color. **Default = False**.\n",
      "## Examples\n",
      "Below are examples of how you might use the `make_interactive_network()` function.\n",
      "## Basic Network\n",
      "make_interactive_network(G, save_as=\"basic.html\")\n",
      "## Basic Network with Labels\n",
      "make_interactive_network(G, labels=True)\n",
      "## Network with Nodes Sized and Colored By Degree\n",
      "make_interactive_network(G,node_size='degree_adjusted',\n",
      "                         color_palette = Blues8, node_color='degree',\n",
      "                         title='GOT Newtork - Degree')\n",
      "## Network with Nodes Colored by Betweenness Centrality (Color Reversed)\n",
      "make_interactive_network(G, labels=True, node_size=20,\n",
      "                         node_color='betweenness', color_palette=Purples8,\n",
      "                         color_order='reverse',\n",
      "                         title='GOT Newtork - Betweenness Centrality (Color Reversed)')\n",
      "## Network with Nodes Colored By Community\n",
      "make_interactive_network(G, labels=False, node_size=20,\n",
      "                         node_color='modularity_color',\n",
      "                         color_type='category',\n",
      "                         title='GOT Newtork - Communities')\n",
      "## Save Network\n",
      "### HTML\n",
      "To save as an HTML file, simply provide an HTML file name to the `save_as` parameter, as below:\n",
      "make_interactive_network(G, labels=True, node_size=20,\n",
      "                         node_color='modularity_color',\n",
      "                         color_type='category',\n",
      "                         title='GOT Network - Communities',\n",
      "                        save_as = \"GOT-network-communities.html\")\n",
      "### Image\n",
      "To save the network as an image, click the \"Save\" 💾 icon on the Bokeh toolbar. This action will download a .png image of the network at the current view. (For example, if you want an image of a specific section of the network, simply zoom and pan to that section of the network and then save 💾.)\n",
      "<img src=\"../images/Bokeh-how-to-save.png\", border=2>\n",
      "Clicking \"Save\" 💾 at the above view creates the following image:\n",
      "<img src=\"GOT-Arya-Zoom.png\", border=2>\n",
      "## Network Analysis\n",
      "[Download relevant files here](https://melaniewalsh.org/Network-Analysis.zip)\n",
      "In this lesson, we're going to learn about **network analysis**. Network analysis will help us better understand the complex relationships between groups of people, fictional characters, or other kinds of things.\n",
      "## Import Libraries\n",
      "To analyze network data, we're going to use the Python library [NetworkX](https://networkx.github.io/).\n",
      "!pip install networkx\n",
      "import networkx \n",
      "import pandas as pd\n",
      "pd.set_option('max_rows', 400)\n",
      "import matplotlib.pyplot as plt\n",
      "## *Game of Thrones* Network\n",
      "The network data that we're going to use in this lesson is taken from Andrew Beveridge and Jie Shan's paper, [\"Network of Thrones.\"](https://www.maa.org/sites/default/files/pdf/Mathhorizons/NetworkofThrones%20%281%29.pdf) They calculated how many times each Game of Thrones character appeared within 15 words of another character in *A Storm of Swords*, the third book in the series.\n",
      "\n",
      "For example:\n",
      "> \"It was the bastard **Jon Snow** who had taken that from him, him and his fat friend **Sam Tarly**.\"\n",
      "> \"Lucky it might be, and red it certainly was, but **Ygritte**’s hair was such a tangle that **Jon** was tempted to ask her if she only brushed it at the changing of the seasons.\"\n",
      "> \"**Arya** gave **Gendry** a sideways look. *He said it with me, like **Jon** used to do, back in Winterfell.* She missed **Jon Snow** the most of all her brothers.\"\"\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "## Create a Network From a Pandas DataFrame\n",
      "G = networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Output a Network File\n",
      "networkx.write_graphml(G, 'GOT-network.graphml')\n",
      "## Draw a Simple Network\n",
      "networkx.draw(G)\n",
      "plt.figure(figsize=(8,8))\n",
      "networkx.draw(G, with_labels=True, node_color='skyblue', width=.3, font_size=8)\n",
      "## Calculate Degree\n",
      "Who has the most number of connections in the network?\n",
      "networkx.degree(G)\n",
      "Make the degree values a `dict`ionary, then add it as a network \"attribute\" with `networkx.set_node_attributes()`\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "Make a Pandas dataframe from the degree data `G.nodes(data='degree')`, then sort from highest to lowest\n",
      "degree_df = pd.DataFrame(G.nodes(data='degree'), columns=['node', 'degree'])\n",
      "degree_df = degree_df.sort_values(by='degree', ascending=False)\n",
      "degree_df\n",
      "Plot the nodes with the highest degree values\n",
      "num_nodes_to_inspect = 10\n",
      "degree_df[:num_nodes_to_inspect].plot(x='node', y='degree', kind='barh').invert_yaxis()\n",
      "## Calculate Weighted Degree\n",
      "Who has the most number of connections in the network (if you factor in edge weight)?\n",
      "networkx.degree(G, weight='Weight')\n",
      "Make the weighted degree values a `dict`ionary, then add it as a network \"attribute\" with `networkx.set_node_attributes()`\n",
      "weighted_degrees = dict(networkx.degree(G, weight='Weight'))\n",
      "networkx.set_node_attributes(G, name='weighted_degree', values=weighted_degrees)\n",
      "Make a Pandas dataframe from the degree data `G.nodes(data='weighted_degree')`, then sort from highest to lowest\n",
      "weighted_degree_df = pd.DataFrame(G.nodes(data='weighted_degree'), columns=['node', 'weighted_degree'])\n",
      "weighted_degree_df = weighted_degree_df.sort_values(by='weighted_degree', ascending=False)\n",
      "weighted_degree_df\n",
      "Plot the nodes with the highest weighted degree values\n",
      "num_nodes_to_inspect = 10\n",
      "weighted_degree_df[:num_nodes_to_inspect].plot(x='node', y='weighted_degree', color='orange', kind='barh').invert_yaxis()\n",
      "## Calculate Betweenness Centrality Scores\n",
      "Who connects the most other nodes in the network?\n",
      "networkx.betweenness_centrality(G)\n",
      "betweenness_centrality = networkx.betweenness_centrality(G)\n",
      "Add `betweenness_centrality` (which is already a dictionary) as a network \"attribute\" with `networkx.set_node_attributes()`\n",
      "networkx.set_node_attributes(G, name='betweenness', values=betweenness_centrality)\n",
      "Make a Pandas dataframe from the betweenness data `G.nodes(data='betweenness')`, then sort from highest to lowest\n",
      "betweenness_df = pd.DataFrame(G.nodes(data='betweenness'), columns=['node', 'betweenness'])\n",
      "betweenness_df = betweenness_df.sort_values(by='betweenness', ascending=False)\n",
      "betweenness_df\n",
      "Plot the nodes with the highest betweenness centrality scores\n",
      "num_nodes_to_inspect = 10\n",
      "betweenness_df[:num_nodes_to_inspect].plot(x='node', y='betweenness', color='green', kind='barh').invert_yaxis()\n",
      "## Communities\n",
      "Who forms distinct communities within this network?\n",
      "from networkx.algorithms import community\n",
      "Calculate communities with `community.greedy_modularity_communities()`\n",
      "communities = community.greedy_modularity_communities(G)\n",
      "communities\n",
      "Make a `dict`ionary by looping through the communities and, for each member of the community, adding their community number\n",
      "# Create empty dictionary\n",
      "modularity_class = {}\n",
      "#Loop through each community in the network\n",
      "for community_number, community in enumerate(communities):\n",
      "    #For each member of the community, add their community number\n",
      "    for name in community:\n",
      "        modularity_class[name] = community_number\n",
      "Add modularity class to the network as an attribute\n",
      "networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "Make a Pandas dataframe from modularity class network data `G.nodes(data='modularity_class')`\n",
      "communities_df = pd.DataFrame(G.nodes(data='modularity_class'), columns=['node', 'modularity_class'])\n",
      "communities_df = communities_df.sort_values(by='modularity_class', ascending=False)\n",
      "communities_df\n",
      "Inspect each community in the network\n",
      "communities_df[communities_df['modularity_class'] == 4]\n",
      "communities_df[communities_df['modularity_class'] == 3]\n",
      "communities_df[communities_df['modularity_class'] == 2]\n",
      "communities_df[communities_df['modularity_class'] == 1]\n",
      "communities_df[communities_df['modularity_class'] == 0]\n",
      "Plot a sample of 40 characters with their modularity class indicated by a star\n",
      "import seaborn as sns\n",
      "#Set figure size\n",
      "plt.figure(figsize=(4,12))\n",
      "\n",
      "#Plot a categorical scatter plot from the dataframe communities_df.sample(40)\n",
      "ax =sns.stripplot(x='modularity_class', y='node', data=communities_df.sample(40),\n",
      "              hue='modularity_class', marker='*',size=15)\n",
      "#Set legend outside the plot with bbox_to_anchor\n",
      "ax.legend(loc='upper right',bbox_to_anchor=(1.5, 1), title='Modularity Class')\n",
      "ax.set_title(\"GOT Characters By Modularity Class\\n(Random Sample)\")\n",
      "plt.show()\n",
      "Plot all GOT characters with their modularity class indicated by a star (tak\n",
      "start_time = datetime.datetime.now()\n",
      "\n",
      "plt.figure(figsize=(4,25))\n",
      "\n",
      "ax =sns.stripplot(x='modularity_class', y='node', data=communities_df,\n",
      "              hue='modularity_class', marker='*',size=15)\n",
      "\n",
      "ax.legend(loc='upper right',bbox_to_anchor=(1.5, 1), title='Modularity Class')\n",
      "ax.set_title(\"GOT Characters By Modularity Class\")\n",
      "plt.show()\n",
      "print(datetime.datetime.now() - start_time)\n",
      "## All Network Metrics\n",
      "Create a Pandas dataframe of all network attributes by creating a `dict`ionary of `G.nodes(data=True)`...\n",
      "dict(G.nodes(data=True))\n",
      "...and then [transposing it](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.T.html) (flipping the columns and rows) with `.T`\n",
      "nodes_df = pd.DataFrame(dict(G.nodes(data=True))).T\n",
      "nodes_df\n",
      "nodes_df.sort_values(by='betweenness', ascending=False)\n",
      "## Make an Interactive Network Viz — Quick Function\n",
      "This notebook includes a single easy function for creating interactive network visualizations with the Python libraries [NetworkX](https://networkx.github.io/) and [Bokeh](https://docs.bokeh.org/en/latest/index.html). This function combines the various features demonstrated in Making Interactive Network Visualizations with Bokeh.\n",
      "## Import Libraries\n",
      "import pandas as pd\n",
      "import networkx \n",
      "from networkx.algorithms import community\n",
      "import matplotlib.pyplot as plt\n",
      "!pip install \"bokeh > 2\"\n",
      "from bokeh.io import output_notebook, show, save\n",
      "from bokeh.palettes import Spectral8\n",
      "from bokeh.models import (Range1d, Circle, MultiLine,\n",
      "EdgesAndLinkedNodes, NodesAndLinkedEdges, ColumnDataSource, LabelSet)\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.plotting import from_networkx\n",
      "from bokeh.palettes import Blues8, Reds8, Purples8, Oranges8, Viridis256, Spectral11\n",
      "from bokeh.transform import linear_cmap\n",
      "output_notebook()\n",
      "## Read in Network Data\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "G = networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Make an Interactive Network — Quick Function\n",
      "The code below creates a comprehensive function called `make_interactive_network()`. After you run this big cell, you can make networks simply by calling the function `make_interactive_network()` below.\n",
      "def make_interactive_network(G, weight=False, node_size=15,\n",
      "                             node_color='skyblue', color_type=None,\n",
      "                             color_order=False, color_palette=Blues8, title='Network', labels=False,\n",
      "                             save_as=False, node_highlight_color=False, edge_highlight_color=False):\n",
      "    \n",
      "    #Calculate Degree and Adjusted Degree Size\n",
      "    degrees = dict(networkx.degree(G, weight=weight))\n",
      "    networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "    \n",
      "    def adjust(degree, weight):\n",
      "        if weight == False:\n",
      "            return degree + 5\n",
      "        else:\n",
      "            return (degree / 10) + 5\n",
      "    degree_adjusted = dict([(node, adjust(degree, weight)) for node, degree in networkx.degree(G, weight=weight)])\n",
      "    networkx.set_node_attributes(G, name='degree_adjusted', values=degree_adjusted)\n",
      "    \n",
      "    #Calculate Betweenness Centrality\n",
      "    betweenness_centrality = networkx.betweenness_centrality(G)\n",
      "    networkx.set_node_attributes(G, name='betweenness', values=betweenness_centrality)\n",
      "    \n",
      "    #Calculate Modularity Classes\n",
      "    communities = community.greedy_modularity_communities(G)\n",
      "    modularity_class = {name:community_number for community_number, community in enumerate(communities) for name in community}\n",
      "    modularity_color = dict((name, Spectral11[community_number]) if community_number < 11 else (name,'gray') for community_number, community in enumerate(communities) for name in community )\n",
      "    networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "    networkx.set_node_attributes(G, modularity_color, 'modularity_color')\n",
      "    \n",
      "    #Set hover categories\n",
      "    HOVER_TOOLTIPS = [\n",
      "           (\"Character\", \"@index\"),\n",
      "            (\"Degree\", \"@degree\"),\n",
      "            (\"Betweenness\", \"@betweenness{1.111}\"),\n",
      "             (\"Modularity Class\", \"@modularity_class\"),\n",
      "            (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "    ]\n",
      "\n",
      "    #Create a plot — set dimensions, toolbar, and title\n",
      "    plot = figure(tooltips = HOVER_TOOLTIPS, toolbar_location='left',\n",
      "                  tools=\"pan,wheel_zoom,save,reset, tap\", active_scroll='wheel_zoom',\n",
      "                x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "    plot.sizing_mode = 'scale_width'\n",
      "    \n",
      "    #Remove grid and axes from plot\n",
      "    plot.xgrid.visible = False\n",
      "    plot.ygrid.visible = False\n",
      "    plot.axis.visible = False\n",
      "    \n",
      "    #Create a network graph object\n",
      "    # https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "\n",
      "    network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "    \n",
      "    #Set node sizes and colors\n",
      "    if color_order == 'reverse':\n",
      "        color_palette = color_palette[::-1]\n",
      "    if node_color == 'degree' or node_color == 'betweenness':\n",
      "        minimum_value_color = min(network_graph.node_renderer.data_source.data[node_color])\n",
      "        maximum_value_color = max(network_graph.node_renderer.data_source.data[node_color])\n",
      "        node_color = linear_cmap(node_color, color_palette, minimum_value_color, maximum_value_color)\n",
      "    else:\n",
      "        node_color =  node_color\n",
      "    \n",
      "    if node_highlight_color == False:\n",
      "        node_highlight_color = node_color\n",
      "    if edge_highlight_color == False:\n",
      "        edge_highlight_color = 'black'\n",
      "    \n",
      "    #Add nodes\n",
      "    network_graph.node_renderer.glyph = Circle(size=node_size, fill_color=node_color)\n",
      "    \n",
      "    #Add edges\n",
      "    network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.3, line_width=1)\n",
      "    \n",
      "    #Set node highlight colors\n",
      "    network_graph.node_renderer.hover_glyph = Circle(size=node_size, fill_color=node_highlight_color, line_width=2)\n",
      "    network_graph.node_renderer.selection_glyph = Circle(size=node_size, fill_color=node_highlight_color, line_width=2)\n",
      "\n",
      "    #Set edge highlight colors\n",
      "    network_graph.edge_renderer.selection_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "    network_graph.edge_renderer.hover_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "\n",
      "    #Highlight adjacent nodes and edges\n",
      "    network_graph.selection_policy = NodesAndLinkedEdges()\n",
      "    network_graph.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "    plot.renderers.append(network_graph)\n",
      "\n",
      "    #Add labels\n",
      "    if labels == True:\n",
      "        x, y = zip(*network_graph.layout_provider.graph_layout.values())\n",
      "        node_labels = list(G.nodes())\n",
      "        source = ColumnDataSource({'x': x, 'y': y, 'name': [node_labels[i] for i in range(len(x))]})\n",
      "        labels = LabelSet(x='x', y='y', text='name', source=source, background_fill_color='white', text_font_size='10px', background_fill_alpha=.7)\n",
      "        plot.renderers.append(labels)\n",
      "\n",
      "    show(plot)\n",
      "    \n",
      "    #Save as HTML file\n",
      "    if save_as != False:\n",
      "        save(plot, filename=f\"{save_as}\", title=title)\n",
      "## Parameters\n",
      "Here are the parameters that you can change and customize with the `make_interactive_network()` function:\n",
      "\n",
      "`node_size` \n",
      "\n",
      "You can size nodes by attributes such as \"degree_adjusted\", \"degree,\" \"betwenness,\" or another attribute that you add to the network. Or you can uniformly size by a number. **Default = 15**.\n",
      "\n",
      "`node_color`\n",
      "\n",
      "You can color nodes by attributes such as \"degree,\" \"betwenness,\" \"modularity_color,\" or another attribute that you add to the network. Or you can uniformly color nodes by a specific color (e.g. \"green\"). **Default = \"skyblue\"**.\n",
      "\n",
      "`color_order`\n",
      "\n",
      "To reverse the order of a color palette (e.g. so that high degree nodes will be darker rather than lighter), you can set the parameter to \"reverse.\" **Default = False**.\n",
      "\n",
      "`color_palette`\n",
      "\n",
      "You can choose a specific color palette from the following imported palettes: Blues8, Reds8, Purples8, Oranges8, Viridis256, Spectral11. **Default = Blues8**. \n",
      "\n",
      "`title` \n",
      "\n",
      "You can choose a title for your network. **Default = \"Network\"**.\n",
      "\n",
      "`labels`  \n",
      "\n",
      "You can choose to show labels by setting `labels=True`. **Default = False.**\n",
      "\n",
      "`save_as`\n",
      "\n",
      "You can save your network as an HTML file by setting `save_as` to a filename. **Default = False**.\n",
      "\n",
      "`node_highlight_color`  \n",
      "\n",
      "You can select a color that nodes will be highlighted when hovered over or selected by choosing a specific color. **Default = False**.\n",
      "\n",
      "`edge_highlight_color`\n",
      "\n",
      "You can select a color that edges will be highlighted when hovered over or selected by choosing a specific color. **Default = False**.\n",
      "## Examples\n",
      "Below are examples of how you might use the `make_interactive_network()` function.\n",
      "## Basic Network\n",
      "make_interactive_network(G, save_as=\"basic.html\")\n",
      "## Basic Network with Labels\n",
      "make_interactive_network(G, labels=True)\n",
      "## Network with Nodes Sized and Colored By Degree\n",
      "make_interactive_network(G,node_size='degree_adjusted',\n",
      "                         color_palette = Blues8, node_color='degree',\n",
      "                         title='GOT Newtork - Degree')\n",
      "## Network with Nodes Colored by Betweenness Centrality (Color Reversed)\n",
      "make_interactive_network(G, labels=True, node_size=20,\n",
      "                         node_color='betweenness', color_palette=Purples8,\n",
      "                         color_order='reverse',\n",
      "                         title='GOT Newtork - Betweenness Centrality (Color Reversed)')\n",
      "## Network with Nodes Colored By Community\n",
      "make_interactive_network(G, labels=False, node_size=20,\n",
      "                         node_color='modularity_color',\n",
      "                         color_type='category',\n",
      "                         title='GOT Newtork - Communities')\n",
      "## Save Network\n",
      "### HTML\n",
      "To save as an HTML file, simply provide an HTML file name to the `save_as` parameter, as below:\n",
      "make_interactive_network(G, labels=True, node_size=20,\n",
      "                         node_color='modularity_color',\n",
      "                         color_type='category',\n",
      "                         title='GOT Network - Communities',\n",
      "                        save_as = \"GOT-network-communities.html\")\n",
      "### Image\n",
      "To save the network as an image, click the \"Save\" 💾 icon on the Bokeh toolbar. This action will download a .png image of the network at the current view. (For example, if you want an image of a specific section of the network, simply zoom and pan to that section of the network and then save 💾.)\n",
      "<img src=\"../images/Bokeh-how-to-save.png\", border=2>\n",
      "Clicking \"Save\" 💾 at the above view creates the following image:\n",
      "<img src=\"GOT-Arya-Zoom.png\", border=2>\n",
      "## Network Analysis\n",
      "* Network Analysis\n",
      "* Character Networks\n",
      "* Making Interactive Network Visualizations with Bokeh\n",
      "* Make an Interactive Network Viz — Quick Function\n",
      "\n",
      "## Make a Character Network\n",
      "This notebook demonstrates how to create a social network of people mentioned in a text based on how often people appear within a certain distance of one another.\n",
      "#!pip install spacy\n",
      "#!python -m spacy download en_core_web_sm\n",
      "import spacy\n",
      "import en_core_web_sm\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_rows\", 400)\n",
      "import networkx \n",
      "import itertools\n",
      "## Load spaCy Language Model\n",
      "#nlp = spacy.load('en_core_web_sm')\n",
      "nlp = en_core_web_sm.load()\n",
      "## Character Networks For Shorter Texts\n",
      "This section demonstrates how to create a character network from a text if you can process the entire text with spaCy at one time (mostly shorter texts).\n",
      "## Read in Text File\n",
      "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\n",
      "text = open(filepath).read()\n",
      "## Process Text\n",
      "document = nlp(text)\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.label_ == \"PERSON\":\n",
      "        \n",
      "        spacy_identified_people.append(named_entity.text)\n",
      "Then output this list of spaCy's identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for named_entity in document.ents:\n",
      "    if named_entity.text in my_list_of_characters:\n",
      "        person = named_entity.text\n",
      "        \n",
      "        #Remove apostrophe 's from character name\n",
      "        person = person.replace(\"’s\", \"\").strip()\n",
      "        #Get the character index number from the text\n",
      "        person_index = named_entity.start_char\n",
      "        \n",
      "        all_people_matches.append(person)\n",
      "        all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 50 characters of one another, add them to the `edge_list`. To change the number of characters, simply change the `threshold_distance` variable below:\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 50\n",
      "\n",
      "#If two people fall within 50 characters of one another, add them to the edge list\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        \n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Lost-in-the-City-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Lost-in-the-City-network.csv\")\n",
      "## Character Networks For Longer Texts\n",
      "This section demonstrates how to create a character network from a text if you cannot process the entire text with spaCy at one time and need to chunk it into smaller documents (mostly longer texts).\n",
      "filepath = \"../texts/literature/Little-Women.txt\"\n",
      "text = open(filepath).read()\n",
      "## Chunk Text By Number of Chunks\n",
      "To chunk text by a specific number of chunks, choose a `number_of_chunks` value and run the cell below. The current default value is 80 chunks. \n",
      "import math\n",
      "number_of_chunks = 80\n",
      "chunk_size = math.ceil(len(text) / number_of_chunks)\n",
      "chunked_text = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
      "The code above is dividing the total number of characters in the text `len(text)` by the number of chunks you want, then rounding up `math.ceil()` to a whole number. This is calculating the necessary chunk size for the number of chunks you want. The final line iterates through the text and creates slices at the necessary chunk size.\n",
      "## Or Chunk Text By Line Breaks\n",
      "#chunked_text= text.split('\\n')\n",
      "## Process Chunked Text\n",
      "chunked_documents = list(nlp.pipe(chunked_text))\n",
      "## Create or Upload List of Characters\n",
      "If you already have a list of characters that you'd like to identify, skip to the end of this section. If you'd like to identify characters with spaCy's NER tagger, run the code below:\n",
      "spacy_identified_people = []\n",
      "\n",
      "for document in chunked_documents:\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.label_ == \"PERSON\":\n",
      "            spacy_identified_people.append(named_entity.text)\n",
      "Output list of identified people to a CSV file for manual cleaning and editing:\n",
      "pd.DataFrame(Counter(spacy_identified_people).most_common(), columns=['character', 'count']).to_csv('spacy-identified-people.csv', index=False)\n",
      "Read in a CSV file with a cleaned list of characters in a column titled \"character\":\n",
      "my_list_of_characters = pd.read_csv('My-Cleaned-Character-List.csv')['character'].tolist()\n",
      "Uncomment to re-upload the CSV file without cleaning or editing:\n",
      "#my_list_of_characters = pd.read_csv('spacy-identified-people.csv')['character'].tolist()\n",
      "## Find Character, Index in Document(s)\n",
      "Count any named entity that matches a person in the list of characters. Also extract the index number where that person appears in the document, so we can later calculate characters who appear near one another.\n",
      "all_people_matches = []\n",
      "all_people_matches_plus_ids = []\n",
      "document_length = 0\n",
      "\n",
      "for document in chunked_documents:\n",
      "    document_length += len(document.text)\n",
      "    for named_entity in document.ents:\n",
      "        if named_entity.text in my_list_of_characters:\n",
      "            person = named_entity.text\n",
      "\n",
      "            #Remove apostrophe 's from character name\n",
      "            person = person.replace(\"’s\", \"\").strip()\n",
      "            \n",
      "            #Get the character index number from the text\n",
      "            person_index =  (document_length - named_entity.start_char)\n",
      "\n",
      "            all_people_matches.append(person)\n",
      "            all_people_matches_plus_ids.append([person, person_index])\n",
      "## Make List of Edges\n",
      "Compare every character to every other character in the list. If two characters fall within 100 characters of one another, add them to the `edge_list`.\n",
      "edge_list = []\n",
      "\n",
      "threshold_distance = 100\n",
      "\n",
      "#Get all entity matches for a previously identified person\n",
      "for person, another_person in itertools.combinations(all_people_matches_plus_ids, 2):\n",
      "        distance = abs(person[1] - another_person[1])\n",
      "        if distance < threshold_distance:\n",
      "            \n",
      "            if person[0] != another_person[0]:\n",
      "                \n",
      "                edge_list.append((person[0], another_person[0]))\n",
      "## Make Network DataFrame\n",
      "character_df = pd.DataFrame(Counter(edge_list).most_common(), columns=['character_pair', 'Weight'])\n",
      "character_df['Source']=character_df['character_pair'].str[0]\n",
      "character_df['Target']=character_df['character_pair'].str[1]\n",
      "character_network = character_df[['Source', 'Target', 'Weight']]\n",
      "character_network\n",
      "## Filter Network By Edge Weights\n",
      "character_network[character_network['Weight'] > 2]\n",
      "## Output Network as Graphml File\n",
      "G = networkx.from_pandas_edgelist(character_network, \"Source\", \"Target\", \"Weight\")\n",
      "networkx.write_graphml(G, \"Little-Women-network.graphml\")\n",
      "## Output Network as CSV File\n",
      "character_network.to_csv(\"Little-Women-network.csv\")\n",
      "## Making Interactive Network Visualizations with Bokeh\n",
      "This notebook includes code for creating interactive network visualizations with the Python libraries [NetworkX](https://networkx.github.io/) and [Bokeh](https://docs.bokeh.org/en/latest/index.html). The notebook begins with code for a basic network visualization then progressively demonstrates how to add more information and functionality, such as:\n",
      "\n",
      "- sizing and coloring nodes by degree\n",
      "- sizing and coloring nodes by modularity class\n",
      "- adding responsive highlighting when hovering over nodes and edges\n",
      "- adding node labels\n",
      "\n",
      "For a simpler, single function that includes all of the above functionality, see Make an Interactive Network Viz — Quick Function.\n",
      "## Import Libraries\n",
      "import pandas as pd\n",
      "import networkx\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "## Install and Import Bokeh\n",
      "#!pip install bokeh\n",
      "from bokeh.io import output_notebook, show, save\n",
      "from bokeh.palettes import Spectral8\n",
      "from bokeh.models import (Range1d, Circle, MultiLine,\n",
      "EdgesAndLinkedNodes, NodesAndLinkedEdges, ColumnDataSource, LabelSet)\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.plotting import from_networkx\n",
      "from bokeh.palettes import Blues8, Reds8, Purples8, Oranges8, Viridis256, Spectral11\n",
      "from bokeh.transform import linear_cmap\n",
      "To view interactive Bokeh visualizations in a Jupyter notebook, you need to run this cell:\n",
      "output_notebook()\n",
      "## Create Network From Pandas DataFrame\n",
      "We read in a CSV file of *Game of Thrones* network data from Andrew Beveridge and Jie Shan's paper, [\"Network of Thrones.\"](https://www.maa.org/sites/default/files/pdf/Mathhorizons/NetworkofThrones%20%281%29.pdf)\n",
      "\n",
      "got_df = pd.read_csv('../data/got-edges.csv')\n",
      "got_df\n",
      "Then we make a network with `networkx.from_pandas_edgelist()`:\n",
      "G = networkx.from_pandas_edgelist(got_df, 'Source', 'Target', 'Weight')\n",
      "## Basic Network \n",
      "The code below shows how to make a basic network viz that includes Hover Tooltips (a text box that will display when a user hovers over nodes) as well as Zoom and Pan/Drag functionality.\n",
      "\n",
      "For more details about visualizing network graphs with Bokeh, see [the documentation](https://docs.bokeh.org/en/latest/docs/user_guide/graph.html?highlight=networks).\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [(\"Character\", \"@index\")]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object with spring layout\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node size and color\n",
      "network_graph.node_renderer.glyph = Circle(size=15, fill_color='skyblue')\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "\n",
      "#Add network graph to the plot\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Bokeh Toolbar\n",
      "\n",
      "<img src=\"../images/Bokeh-toolbar.png\", border=2>\n",
      "## Network with Nodes Sized and Colored By Attribute (Degree)\n",
      "The code below shows how to size and color nodes by degree.\n",
      "Calculate degree for each node and add as node attribute\n",
      "degrees = dict(networkx.degree(G))\n",
      "networkx.set_node_attributes(G, name='degree', values=degrees)\n",
      "Slightly adjust degree so that the nodes with very small degrees are still visible\n",
      "number_to_adjust_by = 5\n",
      "adjusted_node_size = dict([(node, degree+number_to_adjust_by) for node, degree in networkx.degree(G)])\n",
      "networkx.set_node_attributes(G, name='adjusted_node_size', values=adjusted_node_size)\n",
      "Import Bokeh color palettes\n",
      "from bokeh.palettes import Blues8, Reds8, Purples8, Oranges8, Viridis8, Spectral8\n",
      "from bokeh.transform import linear_cmap\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'adjusted_node_size'\n",
      "\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\")\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\\\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as spectrum of color palette)\n",
      "minimum_value_color = min(network_graph.node_renderer.data_source.data[color_by_this_attribute])\n",
      "maximum_value_color = max(network_graph.node_renderer.data_source.data[color_by_this_attribute])\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=linear_cmap(color_by_this_attribute, color_palette, minimum_value_color, maximum_value_color))\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network with Nodes Colored By Attribute (Community)\n",
      "The code below shows how to size and color nodes by modularity class.\n",
      "Calculate communities\n",
      "from networkx.algorithms import community\n",
      "communities = community.greedy_modularity_communities(G)\n",
      "Add modularity class and color as attributes to network graph\n",
      "from bokeh.palettes import Spectral8\n",
      "\n",
      "# Create empty dictionaries\n",
      "modularity_class = {}\n",
      "modularity_color = {}\n",
      "#Loop through each community in the network\n",
      "for community_number, community in enumerate(communities):\n",
      "    #For each member of the community, add their community number and a distinct color\n",
      "    for name in community: \n",
      "        modularity_class[name] = community_number\n",
      "        modularity_color[name] = Spectral8[community_number]\n",
      "# Add modularity class and color as attributes from the network above\n",
      "networkx.set_node_attributes(G, modularity_class, 'modularity_class')\n",
      "networkx.set_node_attributes(G, modularity_color, 'modularity_color')\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'modularity_color'\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\"),\n",
      "         (\"Modularity Class\", \"@modularity_class\"),\n",
      "        (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset, tap\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as category from attribute)\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=color_by_this_attribute)\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network with Responsive Highlighting\n",
      "The code below shows how to create responsive highlighting when a user hovers over nodes or edges, which you can read more about in [Bokeh's NetworkX Integration documentation](https://docs.bokeh.org/en/latest/docs/user_guide/graph.html?highlight=networks#interaction-policies). \n",
      "from bokeh.models import EdgesAndLinkedNodes, NodesAndLinkedEdges\n",
      "\n",
      "#Choose colors for node and edge highlighting\n",
      "node_highlight_color = 'white'\n",
      "edge_highlight_color = 'black'\n",
      "\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'modularity_color'\n",
      "\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\"),\n",
      "         (\"Modularity Class\", \"@modularity_class\"),\n",
      "        (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as category from attribute)\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=color_by_this_attribute)\n",
      "#Set node highlight colors\n",
      "network_graph.node_renderer.hover_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "network_graph.node_renderer.selection_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.5, line_width=1)\n",
      "#Set edge highlight colors\n",
      "network_graph.edge_renderer.selection_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "network_graph.edge_renderer.hover_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "\n",
      "    #Highlight nodes and edges\n",
      "network_graph.selection_policy = NodesAndLinkedEdges()\n",
      "network_graph.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Network with Labels\n",
      "The code below shows how to create node labels, which you can read more about in [Bokeh's label documentation](https://docs.bokeh.org/en/latest/docs/user_guide/annotations.html#labels).\n",
      "from bokeh.models import EdgesAndLinkedNodes, NodesAndLinkedEdges\n",
      "from bokeh.models import ColumnDataSource, LabelSet\n",
      "\n",
      "#Choose colors for node and edge highlighting\n",
      "node_highlight_color = 'white'\n",
      "edge_highlight_color = 'black'\n",
      "\n",
      "#Choose attributes from G network to size and color by — setting manual size (e.g. 10) or color (e.g. 'skyblue') also allowed\n",
      "size_by_this_attribute = 'adjusted_node_size'\n",
      "color_by_this_attribute = 'modularity_color'\n",
      "\n",
      "#Pick a color palette — Blues8, Reds8, Purples8, Oranges8, Viridis8\n",
      "color_palette = Blues8\n",
      "\n",
      "#Choose a title!\n",
      "title = 'Game of Thrones Network'\n",
      "\n",
      "#Establish which categories will appear when hovering over each node\n",
      "HOVER_TOOLTIPS = [\n",
      "       (\"Character\", \"@index\"),\n",
      "        (\"Degree\", \"@degree\"),\n",
      "         (\"Modularity Class\", \"@modularity_class\"),\n",
      "        (\"Modularity Color\", \"$color[swatch]:modularity_color\"),\n",
      "]\n",
      "\n",
      "#Create a plot — set dimensions, toolbar, and title\n",
      "plot = figure(tooltips = HOVER_TOOLTIPS,\n",
      "              tools=\"pan,wheel_zoom,save,reset\", active_scroll='wheel_zoom',\n",
      "            x_range=Range1d(-10.1, 10.1), y_range=Range1d(-10.1, 10.1), title=title)\n",
      "\n",
      "#Create a network graph object\n",
      "# https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.drawing.layout.spring_layout.html\n",
      "network_graph = from_networkx(G, networkx.spring_layout, scale=10, center=(0, 0))\n",
      "\n",
      "#Set node sizes and colors according to node degree (color as category from attribute)\n",
      "network_graph.node_renderer.glyph = Circle(size=size_by_this_attribute, fill_color=color_by_this_attribute)\n",
      "#Set node highlight colors\n",
      "network_graph.node_renderer.hover_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "network_graph.node_renderer.selection_glyph = Circle(size=size_by_this_attribute, fill_color=node_highlight_color, line_width=2)\n",
      "\n",
      "#Set edge opacity and width\n",
      "network_graph.edge_renderer.glyph = MultiLine(line_alpha=0.3, line_width=1)\n",
      "#Set edge highlight colors\n",
      "network_graph.edge_renderer.selection_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "network_graph.edge_renderer.hover_glyph = MultiLine(line_color=edge_highlight_color, line_width=2)\n",
      "\n",
      "    #Highlight nodes and edges\n",
      "network_graph.selection_policy = NodesAndLinkedEdges()\n",
      "network_graph.inspection_policy = NodesAndLinkedEdges()\n",
      "\n",
      "plot.renderers.append(network_graph)\n",
      "\n",
      "#Add Labels\n",
      "x, y = zip(*network_graph.layout_provider.graph_layout.values())\n",
      "node_labels = list(G.nodes())\n",
      "source = ColumnDataSource({'x': x, 'y': y, 'name': [node_labels[i] for i in range(len(x))]})\n",
      "labels = LabelSet(x='x', y='y', text='name', source=source, background_fill_color='white', text_font_size='10px', background_fill_alpha=.7)\n",
      "plot.renderers.append(labels)\n",
      "\n",
      "show(plot)\n",
      "#save(plot, filename=f\"{title}.html\")\n",
      "## Twitter API Setup\n",
      "To collect Twitter data, we're going to work with the [Twitter API](https://developer.twitter.com/en/docs/basics/getting-started) and [twarc](https://github.com/DocNow/twarc), a Python package for collecting Twitter data through the Twitter API. To access the Twitter API, we first need to:\n",
      "\n",
      "**1.** Apply for a Twitter developer account\n",
      "\n",
      "**2.** Create a Twitter application\n",
      "\n",
      "The developer account will allow us to create an application, which will eventually get us a series of API keys and tokens, which we can then use to access Twitter data.\n",
      "\n",
      "According to Twitter, the reason for this somewhat drawn-out application process is to \"prevent abuse of the Twitter platform\" and \"better understand and serve our developer community.\"\n",
      "\n",
      "After getting our API keys, we then need to\n",
      "\n",
      "**3.** Install twarc\n",
      "\n",
      "**4.** Configure/set up twarc\n",
      "\n",
      "**5.** Download the twarc repository\n",
      "\n",
      "The following instructions will guide you through each part of this 5-step process.\n",
      "<img src=\"../images/Twitter/apply-for-access.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/developer-primary-reason.png\" width=100%, border=2>\n",
      "**4.** The next page of the application will ask: \"This is you, right?\" Confirm that your Twitter username and email are correct, select your country of residence (United States), and come up with a name for your application. It doesn't matter which name you choose. I'd suggest using your first name.\n",
      "<img src=\"../images/Twitter/twitter-use-explanation.png\" width=100%, border=2>\n",
      "\n",
      "<img src=\"../images/Twitter/developer-agreement.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/developer-success.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/app-home-page.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/create-app.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/keys-and-tokens.png\" width=100%, border=2>\n",
      "\n",
      "## 3. Install twarc\n",
      "\n",
      "To install twarc, simply run the following on the command line:\n",
      "!pip install twarc\n",
      "!twarc version\n",
      "## 4. Configure/Set up Twarc\n",
      "Now that twarc is installed on our computers, we need to set it up so that we can collect Twitter data with this tool. We need to submit and save our Twitter API keys into twarc.\n",
      "\n",
      "There are two options for configuring twarc.\n",
      "## Option 1 — Configure Twarc From the Command Line\n",
      "To configure twarc, open up your Terminal or PowerShell and copy and paste `twarc configure` into your command line.\n",
      "!twarc configure\n",
      "Twarc will prompt you to copy and paste in your Twitter consumer key and Twitter consumer secret. Then it will ask you to visit a URL to authorize access to the Twitter account that is associated with your API keys.\n",
      "Please enter your Twitter application credentials from apps.twitter.com:\n",
      "\n",
      "consumer key:\n",
      "\n",
      "consumer secret:\n",
      "\n",
      "Please log into Twitter and visit this URL in your browser:\n",
      "\n",
      "https://api.twitter.com/oauth/authorize?oauth_token=UNIQUE-TOKEN\n",
      "<img src=\"../images/Twitter/Twitter-authorize.png\" width=100%, border=2>\n",
      "\n",
      "Once you click \"Authorize App\", you will be redirected to the URL that is associated with your  API keys (likely our course website). You need to carefully inspect this URL because it actually contains the PIN that you need for the last step of twarc configuration. The URL will look something like this:\n",
      "`https://melaniewalsh.github.io/Intro-Cultural-Analytics/oauthtoken=YOUR-UNIQUE-TOKEN&oauthverifier=THIS-IS-THE-DISPLAYED-PIN-YOU-NEED`\n",
      "You need to copy and paste the part after `oauthverifier=` into the prompt at the command line:\n",
      "After you have authorized the application please enter the displayed PIN:\n",
      "If the PIN works, then you will get a happy successs message. Make sure to copy and paste this message into HW 6.\n",
      "The credentials for mellymeldubs have been saved to your configuration file at /Users/melaniewalsh/.twarc\n",
      "\n",
      "✨ ✨ ✨  Happy twarcing! ✨ ✨ ✨\n",
      "## Option 2 — Configure Twarc in This Notebook\n",
      "Copy your [API keys](https://developer.twitter.com/en/apps) and paste them into the quotation marks below. Also type in your Twitter handle without the @ symbol.\n",
      "twitter_handle = \"\"\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "Then run the two cells below:\n",
      "configuration = f\"\"\"[{twitter_handle}]\n",
      "consumer_key={consumer_key}\n",
      "consumer_secret = {consumer_secret}\n",
      "access_token = {access_token}\n",
      "access_token_secret= {access_token_secret}\n",
      "\"\"\"\n",
      "import os\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \".twarc\")\n",
      "with open(config_filename, \"w\") as file_object:\n",
      "    file_object.write(configuration)\n",
      "To test whether twarc has been properly configured, run a sample search and see if Twitter data gets returned:\n",
      "!twarc search \"something incredibly obscure\"\n",
      "## 5. Download the Twarc Repository \n",
      "Finally, we also need to download the twarc repository from GitHub, because there are a few things in it that aren't included in the version of twarc that's installed through pip. To download the repository, run:\n",
      "!git clone https://github.com/DocNow/twarc.git\n",
      "If Git isn't working for some reason, you can also download the repository as a zip file: https://github.com/DocNow/twarc/archive/master.zip\n",
      "## Reddit Data Collection — With PRAW\n",
      "One way to collect Reddit data is with the Reddit API and [PRAW](https://praw.readthedocs.io/en/latest/getting_started/quick_start.html) (an acronym for **P**ython **R**eddit **W**rapper). \n",
      "!pip install praw\n",
      "import praw\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_colwidth\", 500)\n",
      "## Apply for Reddit API Access\n",
      "To apply for Reddit API access, [read the instructions here](https://www.reddit.com/wiki/api) and then [fill out an application and aggree to the Terms of Use here](https://docs.google.com/forms/d/e/1FAIpQLSezNdDNK1-P8mspSbmtC2r86Ee9ZRbC66u929cG2GX0T9UMyw/viewform). Once you have a Reddit developer account, create a PRAW instance with your client ID, client secret, and Reddit user name.\n",
      "reddit = praw.Reddit(client_id='your client id',\n",
      "                     client_secret='your client secret',\n",
      "                     user_agent='your reddit user name')\n",
      "## Get Reddit Posts From a Subreddit\n",
      "The following code draws from [TannerGilbert's PRAW tutorial code](https://github.com/TannerGilbert/Tutorials/blob/master/Reddit%20Webscraping%20using%20PRAW/Reddit%20API.ipynb).\n",
      "## Hot Posts\n",
      "hot_posts = reddit.subreddit('AmItheAsshole').hot(limit=10)\n",
      "for reddit_post in hot_posts:\n",
      "    print(reddit_post.title)\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').hot(limit=10):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "## Top Posts\n",
      "### By Day\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"day\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "### By Month\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"month\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "### By Year\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"year\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "### By All Time\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"all\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "To get Reddit posts by a specific date range, see the tutorial on using the Puhshift.io\n",
      "## Get Reddit Posts From a Subreddit and Make a DataFrame\n",
      "To see all the attributes that you can retrieve from a single Reddit post, consult [PRAW's \"Submission\" documentation](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html#praw.models.Submission).\n",
      "reddit_posts = []\n",
      "aita_subreddit = reddit.subreddit('AmItheAsshole')\n",
      "\n",
      "for reddit_post in aita_subreddit.top(\"month\", limit=10):\n",
      "    reddit_posts.append([reddit_post.title, reddit_post.score, reddit_post.id, reddit_post.subreddit, reddit_post.url, reddit_post.num_comments, reddit_post.selftext, reddit_post.created_utc])\n",
      "\n",
      "reddit_posts = pd.DataFrame(reddit_posts, columns=['title', 'upvote_score', 'post_id', 'subreddit', 'post_url', 'num_comments', 'post_body', 'full_date'])\n",
      "\n",
      "#Format date\n",
      "reddit_posts['full_date'] = pd.to_datetime(reddit_posts['full_date'], utc=True, unit='s')\n",
      "reddit_posts['date'] = reddit_posts['full_date'].dt.strftime(\"%Y-%m-%d\")\n",
      "reddit_posts\n",
      "## Save to CSV File\n",
      "reddit_posts.to_csv(\"top-reddit-aita-posts.csv\", encoding=\"utf-8\", index=False)\n",
      "## Get Comments From a Post\n",
      "https://praw.readthedocs.io/en/latest/tutorials/comments.html\n",
      "submission = reddit.submission(id=\"gcr7vr\")\n",
      "submission.comments.replace_more(limit=None)\n",
      "\n",
      "for comment in submission.comments:\n",
      "    print(f\"\\nAuthor:\\n{comment.author}\\n\\nComment:\\n{comment.body}\\n\\n-------------------------------------\")\n",
      "def get_comments(row):\n",
      "    submission = reddit.submission(id=row['post_id'])\n",
      "    submission.comments.replace_more(limit=None)\n",
      "    comments = [comment.body for comment in submission.comments]\n",
      "    return comments\n",
      "reddit_posts = []\n",
      "aita_subreddit = reddit.subreddit('Datasets')\n",
      "\n",
      "for reddit_post in aita_subreddit.top(\"month\", limit=10):\n",
      "    reddit_posts.append([reddit_post.title, reddit_post.score, reddit_post.id, reddit_post.subreddit, reddit_post.url, reddit_post.num_comments, reddit_post.selftext, reddit_post.created_utc])\n",
      "\n",
      "reddit_posts = pd.DataFrame(reddit_posts, columns=['title', 'upvote_score', 'post_id', 'subreddit', 'post_url', 'num_comments', 'post_body', 'full_date'])\n",
      "\n",
      "#Format date\n",
      "reddit_posts['full_date'] = pd.to_datetime(reddit_posts['full_date'], utc=True, unit='s')\n",
      "reddit_posts['date'] = reddit_posts['full_date'].dt.strftime(\"%Y-%m-%d\")\n",
      "reddit_posts\n",
      "reddit_posts['comments'] = reddit_posts.apply(get_comments, axis='columns')\n",
      "reddit_posts[['title','post_body', 'num_comments', 'comments']]\n",
      "\n",
      "## Song Genius Data Collection\n",
      "<img src=\"../images/Missy-Under-Construction.png\" width=100%, border=2>\n",
      "- [Genius API](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Genius-API.html)\n",
      "- [LyricsGenius](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Lyrics-Genius.html)\n",
      "- [Get All Song Lyrics From Album](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Get-All-Songs-From-Album.html)\n",
      "- [Simply Song Lyrics Analysis](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Song-Lyrics-Analysis.html)\n",
      "\n",
      "## Reddit Data Collection\n",
      "<img src=\"https://www.redditinc.com/assets/images/site/reddit-logo.png\", border=2>\n",
      "- [Reddit Data Collection With PRAW](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Reddit-Data-Collection-With-Praw.html)\n",
      "- [Reddit Data Collection With Pushshift](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Reddit-Data-Collection-With-Pushshift.html)\n",
      "## Display Images in Pandas DataFrame\n",
      "## Transform Song Titles, Page View Counts, & Album Covers into a DataFrame\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_colwidth\", 100)\n",
      "missy_df = pd.read_csv(\"Missy-Elliott-Album-Cover-Images.csv\")\n",
      "missy_df \n",
      "Just for fun, we can do the same thing but also add links to images of Missy Elliott's album art—and we can actually display those images, too!\n",
      "\n",
      "To display images in a Pandas dataframe, you need to run `from IPython.core.display import HTML` and make the function `get_image_html()`. We're going to take the image URLs and make them into HTML objects.\n",
      "def get_image_html(link):\n",
      "    image_html = f\"<img src='{link}' width='100'>\"\n",
      "    return image_html\n",
      "#Use the function get_image_html()\n",
      "missy_df['album_cover'] = missy_df['album_cover_url'].apply(get_image_html)\n",
      "missy_df\n",
      "from IPython.core.display import HTML\n",
      "If we call `HTML()` on our dataframe and add the method `.to_html(escape=False)` to the dataframe, then it should display the dataframe with viewable images.\n",
      "HTML(missy_df.to_html(escape=False))\n",
      "## From Local Images\n",
      "import pandas as pd\n",
      "ny_df = pd.read_csv(\"my-NY-photos.csv\")\n",
      "ny_df \n",
      "def get_image_html(link):\n",
      "    image_html = f\"<img src='{link}' width='100'>\"\n",
      "    return image_html\n",
      "ny_df['image'] = ny_df['image_path'].apply(get_image_html)\n",
      "ny_df\n",
      "HTML(ny_df.to_html(escape=False))\n",
      "## Getting Cultural Data\n",
      "\n",
      "## Convert Kindle Book to Plain Text File\n",
      "## Purchase and Download Kindle Book\n",
      "- Purchase your Kindle book\n",
      "- Go to your Amazon account -> Manage content and devices\n",
      "<img src=\"../images/Calibre/Manage-content-devices.png\", border=2>\n",
      "- Click on the ellipsis \"...\" next to the text that you want to download and select \"Download & transfer via USB\"\n",
      "<img src=\"../images/Calibre/Download-USB-Dreams.png\", border=2>\n",
      "The file should download to your computer.\n",
      "## Download and Install Calibre\n",
      "Download Calibre: https://calibre-ebook.com/download\n",
      "## Download and Install DeDRM Plugin\n",
      "Download the latest release of DeDRM tools by clicking \"DeDRM_tools_6.7.0.zip\" (or whatever version number is most recent) here: https://github.com/apprenticeharper/DeDRM_tools/releases\n",
      "\n",
      "Unzip the file but do not unzip the plugins inside this folder.\n",
      "## Load DeDRM Plugin\n",
      "Go to Calibre -> Preferences -> Plugins -> Load plugin from file. Then navigate to the DeDRM_Plugin.zip file inside DeDRM_tools_6.7.0 folder. \n",
      "<img src=\"../images/Calibre/Preferences.png\", border=2>\n",
      "<img src=\"../images/Calibre/Plugins.png\", border=2>\n",
      "## Add Kindle Serial Number\n",
      "Finally, add your Kindle serial number to the plugin. To find your Kindle serial number, go to your Amazon account -> Manage content and devices -> Devices\n",
      "<img src=\"../images/Calibre/Serial-Number.png\", border=2>\n",
      "Then go back to Calibre's Preferences -> Plugins. Select \"Show only user installed plugins\", click on the DeDRM plugin and select \"Customize plugin\". Select \"eInk Kindle ebooks\" and add your Kindle serial number.\n",
      "<img src=\"../images/Calibre/Customize-eInk.png\", border=2>\n",
      "## Convert Book to Text File\n",
      "Now drag and drop your Kindle book (.azw) into Calibre. Once it's loaded, select \"Convert books\" form the Calibre menu, which will cause a new window to appear. Then choose \"TXT\" as the desired output format and select \"OK\".\n",
      "<img src=\"../images/Calibre/Output-Format.png\", border=2>\n",
      "You should find the plain text file in a directory called \"Calibre Library\", which should be in your home folder.\n",
      "<img src=\"../images/Calibre/Where-to-Find-Text-File.png\", border=2>\n",
      "## Twitter Data Sharing\n",
      "[Download relevant files here](https://melaniewalsh.org/Twitter-Data-Sharing.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to share Twitter data and access Twitter data that has been shared by others with the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "\n",
      "This lesson presumes that you've already installed and configured twarc (which was covered in [previous lessons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Collection.html#Install-and-Configure-Twarc)).\n",
      "## Tweet IDs\n",
      "Twitter discourages developers and researchers from sharing full Twitter data openly on the web. They instead encourage developers and researchers to share *tweet IDs*:\n",
      "\n",
      "> [If you provide Twitter Content to third parties, including downloadable datasets or via an API, you may only distribute **Tweet IDs**, Direct Message IDs, and/or User IDs.](https://developer.twitter.com/en/developer-terms/policy#4-e)\n",
      "\n",
      "Tweet IDs are unique identifiers assigned to every tweet. They look like a random string of numbers: 1189206626135355397. Each tweet ID can be used to download the full data associated with that tweet (if the tweet still exists). This is a process called \"hydration.\"\n",
      "<img src=\"https://cdn.pixabay.com/photo/2013/07/12/19/24/sapling-154734_960_720.png\" width=100%, border=2>\n",
      "**Hydration: a young tweet ID sprouts into a full tweet (to be read in David Attenborough's voice)**\n",
      "There are actually two reasons that you might want to dehydrate tweets and/or hydrate tweet IDs: first, to responsibly share Twitter data with others and/or access Twitter data shared by others; second, to get more information about the Twitter data that *you yourself collected*.\n",
      "\n",
      "If you collected tweets in real time, for example, you collected those tweets immediately after they were published, which means that they will not contain any retweet or favorite count information. Nobody's had time to retweet them yet! So if you'd like to retroactively get retweet and favorite count information about your tweets, then you would want to dehydrate and rehydrate them.\n",
      "## Dehydrate Tweets\n",
      "`twarc dehydrate tweets.jsonl > tweet_ids.txt`\n",
      "To transform your Twitter data into a list of tweet IDs (so that you can share your data openly on the web), you can run the twarc command `twarc dehydrate` with the name of your JSONL file followed by the output operator `>` and the desired name of your tweet ID text file.\n",
      "\n",
      "> tweet ID —> tweet = hydration <br>\n",
      "> tweet ID <— tweet = dehydration\n",
      "Let's dehydrate the Twitter data that we collected a few weeks ago: a JSONL file of 685 tweets that mentioned the general phrase \"touch my face\" (most responding to public health recommendations that people should avoid touching their faces).\n",
      "!twarc dehydrate touch_my_face_tweets.jsonl > touch_my_face_tweet_ids.txt\n",
      "If we `open()` and `.read()` the tweet IDs file that we just created, it looks something like this:\n",
      "tweet_ids = open(\"touch_my_face_tweet_ids.txt\", encoding=\"utf-8\").read()\n",
      "print(tweet_ids)\n",
      "## Hydrate Tweets\n",
      "`twarc hydrate tweet_ids.txt > tweets.jsonl`\n",
      "To transform a list of tweet IDs into full Twitter data, you can run the twarc command `twarc hydrate` with the name of your tweet IDs text file followed by the output operator `>` and the desired name of your JSONL file.\n",
      "\n",
      "> tweet ID —> tweet = hydration <br>\n",
      "> tweet ID <— tweet = dehydration\n",
      "Now let's re-hydrate the Twitter data that we collected a few weeks ago based on the tweet IDs that we just dehydrated.\n",
      "!twarc hydrate touch_my_face_tweet_ids.txt > touch_my_face_tweets_REHYDRATED.jsonl\n",
      "tweet_json = open(\"touch_my_face_tweets_REHYDRATED.jsonl\", encoding=\"utf-8\").read()\n",
      "print(tweet_json)\n",
      "## Deleted Tweets & The Right To Be Forgotten\n",
      "What happens if someone decides to delete their tweet between the time when the tweet is first collected and the time when the tweet is \"hydrated\"? The deleted tweet will **not** be hydrated. The deleted tweet is no longer be accessible.\n",
      "\n",
      "To see how many tweets might be gone from our dataset, let's look at how many tweets are included in our rehydrated tweet file vs our original tweet file.\n",
      " Mac/Chrome OS\n",
      "!wc -l touch_my_face_tweets_REHYDRATED.jsonl\n",
      "!wc -l touch_my_face_tweets.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" touch_my_face_tweets_REHYDRATED.jsonl\n",
      "!find /v /c \"\" touch_my_face_tweets.jsonl\n",
      "As you can see, our rehydrated tweet file is missing 10 tweets. Those tweets have either been deleted, been made private, or been suspended.\n",
      "## Separate Out Deleted Tweets (From Tweet IDs)\n",
      "`python twarc/utils/tweet_compliance.py tweet_ids.txt > hydrated_tweets.json 2> deleted_tweet_ids.txt`\n",
      "If you're working from someone else's tweet IDs, you can hydrate these tweet IDs and filter out the tweet IDs that have been deleted/made private/suspended by using the twarc utility `twarc/utils/tweet_compliance.py`, followed by the output operator `>`, a JSONL file name for your hydrated tweets, the number `2`, another output operator `>` and a file name for the deleted tweet IDs.\n",
      "!python twarc/utils/tweet_compliance.py touch_my_face_tweet_ids.txt > hydrated_tweets.json 2> touch_my_face_deleted_tweets.txt\n",
      " Mac/Chrome OS\n",
      "!wc -l touch_my_face_deleted_tweets.txt\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" touch_my_face_deleted_tweets.txt\n",
      "## Find Current Status of Tweets (From Tweet JSONL File)\n",
      "`python twarc/utils/deletes.py tweest.jsonl > current_status_of_tweets.txt`\n",
      "If you want to find out the current status of tweets that you've already collected, you can use the twarc utility `twarc/utils/deletes.py` followed by the output operator `>` then the file name for your text file.\n",
      "!python twarc/utils/deletes.py touch_my_face_tweets.jsonl > current_status_of_tweets.txt\n",
      "tweet_current_status = open(\"current_status_of_tweets.txt\", encoding=\"utf-8\").read()\n",
      "print(tweet_current_status)\n",
      "## Update/Enhance Twitter Data with Current Status of Tweets\n",
      "`python twarc/utils/deletes.py --enhance tweets.jsonl > tweets_with_current_status.jsonl`\n",
      "!python twarc/utils/deletes.py --enhance touch_my_face_tweets.jsonl > touch_my_face_tweets_CURRENT_STATUS.jsonl\n",
      "## Where to Find Tweet IDs\n",
      "DocNow Catalog: https://www.docnow.io/catalog/\n",
      "\n",
      "George Washington University Tweet IDs: https://dataverse.harvard.edu/dataverse/gwu-libraries\n",
      "## Get All Song Lyrics From Album\n",
      "!pip install git+https://github.com/johnwmillr/LyricsGenius.git\n",
      "import lyricsgenius\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from pathlib import Path\n",
      "client_access_token = \"YOUR CLIENT ACCESS TOKEN\"\n",
      "def clean_up(song_title):\n",
      "\n",
      "    if \"Ft\" in song_title:\n",
      "        before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "        song_title_before_ft = before_ft_pattern.search(song_title).group(0)\n",
      "        clean_song_title = song_title_before_ft.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    else:\n",
      "        song_title_no_lyrics = song_title.replace(\"Lyrics\", \"\")\n",
      "        clean_song_title = song_title_no_lyrics.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    return clean_song_title\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    \n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    \n",
      "    response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_string = response.text\n",
      "    document = BeautifulSoup(html_string, \"html.parser\")\n",
      "    song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [song_title.text for song_title in song_title_tags]\n",
      "    \n",
      "    clean_songs = []\n",
      "    for song_title in song_titles:\n",
      "        clean_song = clean_up(song_title)\n",
      "        clean_songs.append(clean_song)\n",
      "        \n",
      "    return clean_songs\n",
      "def download_album_lyrics(artist, album_name): \n",
      "    \n",
      "    # Set up LyricsGenius with your Genius API client access token\n",
      "    #client_access_token = Your-Client-Access-Token\n",
      "    LyricsGenius = lyricsgenius.Genius(client_access_token)\n",
      "    LyricsGenius.remove_section_headers = True\n",
      "    \n",
      "    # With the function that we previously created, go to Genius.com and get all song titles for a particular artist's album\n",
      "    clean_songs = get_all_songs_from_album(artist, album_name)\n",
      "    \n",
      "    for song in clean_songs:\n",
      "        \n",
      "        #For each song in the list, search for that song with LyricsGenius\n",
      "        song_object = LyricsGenius.search_song(song, artist)\n",
      "        \n",
      "        #If the song is not empty\n",
      "        if song_object != None:\n",
      "            \n",
      "            #Do some cleaning and prep for the filename of the song\n",
      "            artist_title = artist.replace(\" \", \"-\")\n",
      "            album_title = album_name.replace(\" \", \"-\")\n",
      "            song_title = song.replace(\"/\", \"-\")\n",
      "            song_title = song.replace(\" \", \"-\")\n",
      "            \n",
      "            #Establish the filename for each song inside a directory that begins with the artist's name and album title\n",
      "            custom_filename=f\"{artist_title}_{album_title}/{song_title}\"\n",
      "            \n",
      "            #A line of code that we need to create a directory\n",
      "             #os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
      "            Path(f\"{artist_title}_{album_title}\").mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            #Save the lyrics for the song as a text file\n",
      "            song_object.save_lyrics(filename=custom_filename, extension='txt', sanitize=False)\n",
      "        \n",
      "        #If the song doesn't contain lyrics\n",
      "        else:\n",
      "            print('No lyrics')\n",
      "download_album_lyrics(\"Missy Elliott\", \"Under Construction\")\n",
      "!wget https://download.java.net/java/GA/jdk14/076bab302c7b4508975440c56f6cc26a/36/GPL/openjdk-14_osx-x64_bin.tar.gz\n",
      "!tar xvf openjdk-14_osx-x64_bin.tar.gz\n",
      "\n",
      "## Reddit Data Collection — With Pushshift.io\n",
      "Another way to collect Reddit data is with [Pushshift.io](https://pushshift.io/). If you're looking to collect Reddit data within a certain date range, Pushshift may be a good option since this is not easy to accomplish with PRAW and Reddit's API. Pushshift also does not require an API key or registration.\n",
      "## Import Libraries\n",
      "import requests\n",
      "import json\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_columns\", 80)\n",
      "pd.set_option(\"max_rows\", 100)\n",
      "import datetime\n",
      "## Make Function for Formatting Date Range as UTC Timestamps\n",
      "def format_date(date):\n",
      "    return int(datetime.datetime.strptime(date, \"%m-%d-%Y\").timestamp())\n",
      "## Format API Request URL\n",
      "Below I'm requesting the 50 most upvoted Reddit posts from the subreddit \"AmItheAsshole\" that were published between April 1, 2020 and April 30, 2020 (if they have at least an upvote score of 100).\n",
      "subreddit = \"AmItheAsshole\"\n",
      "start_date = \"04-01-2020\"\n",
      "end_date = \"04-30-2020\"\n",
      "num_posts = 50\n",
      "upvote_score = 100\n",
      "\n",
      "url = f\"\"\"\n",
      "https://api.pushshift.io/reddit/search/submission/\\\n",
      "?subreddit={subreddit}\\\n",
      "&sort=desc\\\n",
      "&sort_type=score\\\n",
      "&after={format_date(start_date)}\\\n",
      "&before={format_date(end_date)}\\\n",
      "&size={num_posts}\\\n",
      "&score=>{upvote_score}\"\"\"\n",
      "NOTE: The backslashes above `\\` allow us to break the URL up onto multiple lines. They're ultimately ignored in the URL.\n",
      "## Send API Request\n",
      "response = requests.get(url)\n",
      "reddit_data = response.json()['data']\n",
      "## Make API Response into DataFrame\n",
      "reddit_data = pd.DataFrame(reddit_data)\n",
      "reddit_data.head(3)\n",
      "## Examine Columns\n",
      "reddit_data.columns\n",
      "## Add Formatted Date Columns\n",
      "reddit_data['full_date'] = pd.to_datetime(reddit_data['created_utc'], utc=True, unit='s')\n",
      "reddit_data['date'] = reddit_data['full_date'].dt.strftime(\"%Y-%m-%d\")\n",
      "## Filter Columns\n",
      "reddit_data[['subreddit', 'date', 'score', 'num_comments','title', 'selftext', 'full_link','url']]\n",
      "## Twitter Data Analysis\n",
      "[Download relevant files here](https://melaniewalsh.org/Collecting-Twitter-Data-v2.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to analyze and explore Twitter data with Pandas and the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "\n",
      "This lesson presumes that you've already installed and configured twarc and collected some Twitter data (covered in the previous lesson).\n",
      "## Read in Tweet CSV files with Pandas\n",
      "import pandas as pd\n",
      "Set Pandas display options so columns are wider and more columns are visible\n",
      "pd.set_option('max_colwidth', 5000)\n",
      "pd.set_option('max_columns', 40)\n",
      "pd.set_option('max_rows', 100)\n",
      "bang_df = pd.read_csv('bang.csv')\n",
      "face_df = pd.read_csv('face.csv')\n",
      "Check what Twitter metadata exists in this CSV file\n",
      "bang_df.columns\n",
      "As you can see above, there is a *lot* of metadata that comes with every tweet!\n",
      "Check the size of dataframe (number of rows = number of tweets)\n",
      "bang_df.shape\n",
      "face_df.shape\n",
      "Preview dataframes\n",
      "bang_df.head()\n",
      "face_df.head()\n",
      "  \n",
      "  \n",
      "## Filter Twitter Data to Only Categories of Interest\n",
      "bang_df[['created_at', 'tweet_type', 'media', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "bang_df = bang_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "face_df = face_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "## Display Links and Images in Twitter Data\n",
      "To display links and images in our Twitter dataframe, run the cells below. We're converting the image URL into an HTML image tag and then displaying our dataframe as an HTML object.\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "bang_df['media'] = bang_df['media'].fillna(\"No Image\")\n",
      "bang_df['media']= bang_df['media'].apply(get_image_html)\n",
      "\n",
      "face_df['media'] = face_df['media'].fillna(\"No Image\")\n",
      "face_df['media']= face_df['media'].apply(get_image_html)\n",
      "**Not With a Bang**\n",
      "HTML(bang_df.to_html(render_links=True, escape=False))\n",
      "**Touch My Face**\n",
      "HTML(face_df.to_html(render_links=True, escape=False))\n",
      "Filter to just text, images, and retweet count\n",
      "HTML(face_df[['media', 'text', 'retweet_count']].to_html(render_links=True, escape=False))\n",
      "## Sort By Top Retweets\n",
      "**Not With a Bang**\n",
      "bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "bang_rt_sorted = bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(bang_rt_sorted.to_html(render_links=True, escape=False))\n",
      "**Touch My Face**\n",
      "face_rt_sorted = face_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(face_rt_sorted.to_html(render_links=True, escape=False))\n",
      "## Identify Top Hashtags\n",
      "`twarc/utils/tags.py tweets.jsonl`\n",
      ", border=2> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Heads up Windows users! Remember that twarc utilities may not work on your computer by default. If you get a UnicodeEncodeError, it's because Windows computers do not use Unicode (UTF-8) by default. However, you can make UTF-8 your default by following [these instructions](https://scholarslab.github.io/learn-twarc/08-win-region-settings) and restarting your comptuer. Then twarc utilities should work.\n",
      "!python twarc/utils/tags.py face.jsonl\n",
      "## Create a Word Cloud\n",
      "`twarc/utils/wordcloud.py tweets.jsonl`\n",
      "!python twarc/utils/wordcloud.py bang.jsonl > not_with_a_bang.html\n",
      "[not_with_a_bang.html](not_with_a_bang.html)\n",
      "%%html\n",
      "<iframe src=\"not_with_a_bang.html\" width=800, height=800></iframe>\n",
      "!python twarc/utils/wordcloud.py face.jsonl > touch_my_face.html\n",
      "%%html\n",
      "<iframe src=\"touch_my_face.html\", width=800, height=800></iframe>\n",
      "## Count Emojis\n",
      "`python twarc/utils/emojis.py tweets.jsonl --number 10`\n",
      "!pip install emoji\n",
      "!python twarc/utils/emojis.py face.jsonl --number 10\n",
      "## Your Turn!\n",
      "Now choose your own Twitter search term or query.\n",
      "## Collect Tweets From Last 7 Days\n",
      "!twarc search \"your search query\" > your_search.jsonl \n",
      "## Count How Many Tweets You Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l your_search.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" your_search.jsonl\n",
      "## Convert Your JSON data to CSV data\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text  your_search.jsonl > your_search.csv\n",
      "## Read in as Pandas dataframe\n",
      "import pandas as pd\n",
      "your_df = pd.read_csv('your_search.csv')\n",
      "## Add Metadata\n",
      "Filter your dataframe and add at least one new metadata column that we haven't explored yet.\n",
      "your_df.columns\n",
      "When you run the cell below, right-click to \"Enable Scrolling for Outputs\" and scroll through to see what the new metadata category looks like. Discuss this category with your group and how you might use it for a Twitter analysis.\n",
      "your_df[['created_at', 'tweet_type', '#YOUR NEW METADATA HERE','media', 'tweet_url', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "Now save your filtered dataframe as `filtered_df`\n",
      "#Your Code Here\n",
      "## Explore Data with Links and Images\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "filtered_df['media'] = filtered_df['media'].fillna(\"No Image\")\n",
      "filtered_df['media']= filtered_df['media'].apply(get_image_html)\n",
      "HTML(filtered_df.to_html(render_links=True, escape=False))\n",
      "## Sort Your Twitter Data by Top Retweets\n",
      "#Your code here\n",
      "What is the most retweeted tweet in your dataset?\n",
      "**#**Your Answer Here\n",
      "## Count Most Frequent Emojis\n",
      "!python twarc/utils/emojis.py your_search.jsonl --number 10\n",
      "## LyricsGenius\n",
      "It's relatively common for software developers and others to create special Python packages that help people work with a particular API—even if they don't work for the company or project that designed the API. Lucky for us, this is the case with the Genius API.\n",
      "\n",
      "A data scientist named John Miller wrote a Python package called [LyricsGenius,](https://github.com/johnwmillr/LyricsGenius) which you can see on GitHub below. \n",
      "<a href=\"https://github.com/johnwmillr/LyricsGenius\", border=2><img src=\"../images/LyricsGenius-Git1.png\" width=100%, border=2></a, border=2>\n",
      "LyricsGenius makes working with the Genius API easier, but it also adds some functionality that is not offered by the Genius API. Remember when I said that companies typically don't offer access to their most lucrative data? Well, the Genius API doesn't offer you a way to get access to song lyrics. That's the bread and butter of the whole website!\n",
      "\n",
      "To solve this pesky problem, LyricsGenius combines the Genius API with the web scraping library BeautifulSoup (which we are now familiar with!) in order to get and save song lyrics.\n",
      "<img src=\"../images/LyricsGenius-Git2.png\" width=100%, border=2>\n",
      "## Install the Package\n",
      "To install LyricsGenius (and get the most updated version from GitHub), run:\n",
      "!pip install git+https://github.com/johnwmillr/LyricsGenius.git\n",
      "Copy and paste your Genius \"Client Access Token\" into the quotation marks below, and run the cell to save your variable :\n",
      "client_access_token = \"INSERT YOUR CLIENT ACCESS TOKEN\"\n",
      "To import and set up LyricsGenius, run:\n",
      "import lyricsgenius\n",
      "LyricsGenius = lyricsgenius.Genius(client_access_token)\n",
      "## Get Songs and Lyrics By a Specific Artist\n",
      "To get the top songs and song lyrics from a specific artist you can use the method `.search_artist()`:\n",
      "artist = LyricsGenius.search_artist(\"Missy Elliott\", max_songs=6)\n",
      "To access the song titles, you can run `artist.songs`:\n",
      "artist.songs\n",
      "Inside each of those songs, LyricsGenius has already saved the song lyrics. You can access these lyrics by looping through `artist.songs` and pulling out `song.lyrics`:\n",
      "for song in artist.songs:\n",
      "    print(song.lyrics)\n",
      "## Get Specific Song and Lyrics By a Specific Artist\n",
      "To get the song lyrics from a specific artist, you can use the method `.search_song()`\n",
      "song = LyricsGenius.search_song(\"Missy Elliott\", \"Work It\")\n",
      "song.lyrics\n",
      "### Save Lyrics to .txt File\n",
      "song.save_lyrics(extension='txt')\n",
      "## Get Songs and Lyrics For a Specific Album\n",
      "As you can see, LyricsGenius is an extremely useful Python package! But one thing that we can't do with LyricsGenius is get all the song lyrics for a particular album.\n",
      "\n",
      "So we're going to use the web scraping functions that we wrote in the last lesson to get all the song titles for a specific album, then use LyricsGenius to get the lyrics for each of those songs, and then save them all as text files in a directory.\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "import lyricsgenius\n",
      "import requests\n",
      "from pathlib import Path\n",
      "### Make RegEx Function To Clean Up Songs\n",
      "def clean_up(song_title):\n",
      "\n",
      "    if \"Ft\" in song_title:\n",
      "        before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "        song_title_before_ft = before_ft_pattern.search(song_title).group(0)\n",
      "        clean_song_title = song_title_before_ft.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    else:\n",
      "        song_title_no_lyrics = song_title.replace(\"Lyrics\", \"\")\n",
      "        clean_song_title = song_title_no_lyrics.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    return clean_song_title\n",
      "### Make Function To Scrape Song Titles For Album\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    \n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    \n",
      "    response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_string = response.text\n",
      "    document = BeautifulSoup(html_string, \"html.parser\")\n",
      "    song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [song_title.text for song_title in song_title_tags]\n",
      "    \n",
      "    clean_songs = []\n",
      "    for song_title in song_titles:\n",
      "        clean_song = clean_up(song_title)\n",
      "        clean_songs.append(clean_song)\n",
      "        \n",
      "    return clean_songs\n",
      "### Make Function To Download Lyrics For All Songs in Album\n",
      "client_access_token = \"YOUR CLIENT ACCESS TOKEN\"\n",
      "def download_album_lyrics(artist, album_name): \n",
      "    \n",
      "    # Set up LyricsGenius with your Genius API client access token\n",
      "    #client_access_token = Your-Client-Access-Token\n",
      "    LyricsGenius = lyricsgenius.Genius(client_access_token)\n",
      "    LyricsGenius.remove_section_headers = True\n",
      "    \n",
      "    # With the function that we previously created, go to Genius.com and get all song titles for a particular artist's album\n",
      "    clean_songs = get_all_songs_from_album(artist, album_name)\n",
      "    \n",
      "    for song in clean_songs:\n",
      "        \n",
      "        #For each song in the list, search for that song with LyricsGenius\n",
      "        song_object = LyricsGenius.search_song(song, artist)\n",
      "        \n",
      "        #If the song is not empty\n",
      "        if song_object != None:\n",
      "            \n",
      "            #Do some cleaning and prep for the filename of the song\n",
      "            artist_title = artist.replace(\" \", \"-\")\n",
      "            album_title = album_name.replace(\" \", \"-\")\n",
      "            song_title = song.replace(\"/\", \"-\")\n",
      "            song_title = song.replace(\" \", \"-\")\n",
      "            \n",
      "            #Establish the filename for each song inside a directory that begins with the artist's name and album title\n",
      "            custom_filename=f\"{artist_title}_{album_title}/{song_title}\"\n",
      "            \n",
      "            #A line of code that we need to create a directory\n",
      "             #os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
      "            Path(f\"{artist_title}_{album_title}\").mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            #Save the lyrics for the song as a text file\n",
      "            song_object.save_lyrics(filename=custom_filename, extension='txt', sanitize=False)\n",
      "        \n",
      "        #If the song doesn't contain lyrics\n",
      "        else:\n",
      "            print('No lyrics')\n",
      "download_album_lyrics(\"Missy Elliott\", \"Under Construction\")\n",
      "## Songs Lyrics Analysis\n",
      "Since we now have access to all these great song lyrics, we might as well run some basic analysis on them! We can use the word frequency code that we introduced during the first couple weeks to count the most frequent words in an entire album.\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "import os\n",
      "import re\n",
      "## Make a Function That Splits Up Words\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "## Make a Function That Counts Top 15 Words in Text File\n",
      "def get_most_frequent_words(filepath_of_text):\n",
      "    \n",
      "    nltk_stop_words = stopwords.words(\"english\")\n",
      "    number_of_desired_words = 15\n",
      "    \n",
      "    full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "    all_the_words = split_into_words(full_text)\n",
      "    meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "    meaningful_words_tally = Counter(meaningful_words)\n",
      "    most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "    return most_frequent_meaningful_words\n",
      "## Make a Function That Counts Top 15 Words in An Entire Dicrectory of Text Files\n",
      "def get_most_frequent_words_directory(directory):\n",
      "    \n",
      "    nltk_stop_words = stopwords.words(\"english\")\n",
      "    number_of_desired_words = 15\n",
      "    meaningful_words_tally = Counter()\n",
      "    \n",
      "    for filepath_of_text in os.listdir(directory):\n",
      "        if filepath_of_text.endswith(\".txt\"):\n",
      "    \n",
      "            full_text = open(f\"{directory}/{filepath_of_text}\", encoding=\"utf-8\").read()\n",
      "\n",
      "            all_the_words = split_into_words(full_text)\n",
      "            meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "            meaningful_words_tally.update(meaningful_words)\n",
      "    \n",
      "    most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "    return most_frequent_meaningful_words\n",
      "get_most_frequent_words_directory(\"Missy-Elliott_Under-Construction\")\n",
      "## Data Viz With Pandas\n",
      "import pandas as pd\n",
      "frequencies = get_most_frequent_words_directory(\"Missy-Elliott_Under-Construction\")\n",
      "word_frequency_df = pd.DataFrame(frequencies)\n",
      "word_frequency_df.columns = ['word', 'word_count']\n",
      "word_frequency_df.plot(x='word', kind='barh', figsize=(10,5), title=\"Album Word Frequencies\", fontsize=20).invert_yaxis()\n",
      "## Words in Context\n",
      "from IPython.display import Markdown, display\n",
      "import glob\n",
      "from pathlib import Path\n",
      "word = \"ti\"\n",
      "for file in glob.glob(f\"{directory_path}/*.txt\"):\n",
      "    text = open(file).read()\n",
      "    for line in text.split(\"\\n\"):\n",
      "        if re.search(f\"\\\\b{word}\\\\b\", line):\n",
      "            line_with_bolding = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", line, flags=re.IGNORECASE)\n",
      "            display(Markdown(line_with_bolding))\n",
      "word = \"oh\"\n",
      "for file in glob.glob(f\"{directory_path}/*.txt\"):\n",
      "    song_title = Path(file).stem.replace(\"-\", \" \")\n",
      "    \n",
      "    text = open(file).read()\n",
      "    for line in text.split(\"\\n\"):\n",
      "        if re.search(f\"\\\\b{word}\\\\b\", line):\n",
      "            line_with_bolding = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", line, flags=re.IGNORECASE)\n",
      "            display(Markdown(f\"Line: {line_with_bolding} <br> From: {song_title} \"))\n",
      "word = \"Missy\"\n",
      "for file in glob.glob(f\"{directory_path}/*.txt\"):\n",
      "    song_title = Path(file).stem.replace(\"-\", \" \")\n",
      "    \n",
      "    text = open(file).read()\n",
      "    for line in text.split(\"\\n\"):\n",
      "        if re.search(f\"\\\\b{word}\\\\b\", line, flags=re.IGNORECASE):\n",
      "            line_with_bolding = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", line, flags=re.IGNORECASE)\n",
      "            display(Markdown(f\"Line: {line_with_bolding} <br> From: {song_title} \"))\n",
      "## Web Scraping\n",
      "[Download relevant files here](https://melaniewalsh.org/Web-Scraping.zip)\n",
      "Inspired by web scraping lessons from [Lauren Klein](https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class4-web-scraping-complete.ipynb) and [Allison Parrish](https://github.com/aparrish/dmep-python-intro/blob/master/scraping-html.ipynb)\n",
      "How did the data journalists of *The Pudding* actually collect the necessary screenplay and rap song data for their visualizations and analyses? \n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100%, border=2>\n",
      "<img src='../images/Pudding-rap-viz.png' width=100%, border=2>\n",
      "Well, they almost certainly used some form of web scraping. Web scraping is a way of computationally extracting data from the internet. It's one of the two ways of computationally collecting data from the internet that we're going to discuss in this class.\n",
      "## Why Do We Need To Scrape At All?\n",
      "To understand the necessity and significance of web scraping, let's walk through the likely data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
      "\n",
      "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red! These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project.\n",
      "Let's take a look:\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "urls\n",
      "This is an extremely useful dataset! But how can we actually use these URLs to get workable, computationally tractable text data? Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file....\n",
      "\n",
      "But that route would be suuuuper slow and painstaking, not to mention that we would lose some crucial data along the way—for example, information that might help us automatically distinguish the title of the movie from the screenplay itself. It would be much better if we could programmatically access the text data attached to every URL.\n",
      "## Responses and Requests\n",
      "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*.\n",
      "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%, border=2>\n",
      "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
      "<img src=\"../images/request-response.png\" width=100%, border=2>\n",
      "import requests\n",
      "## `.get()`\n",
      "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
      "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")\n",
      "## HTTP Status Code\n",
      "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error.\n",
      "response\n",
      "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL...\n",
      "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")\n",
      "<img src=\"../images/Ghostboogers.png\" width=100%, border=2>\n",
      "bad_response\n",
      "## Grab the `.text`\n",
      "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below.\n",
      "html_string = response.text\n",
      "Voila! Here's the screenplay now in a variable.\n",
      "print(html_string)\n",
      "## Looping Requests\n",
      "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies.\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "sample_urls = urls.sample(10)\n",
      "sample_urls\n",
      "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code.\n",
      "def scrape_screenplay(url):\n",
      "    response = requests.get(url)\n",
      "    html_string = response.text\n",
      "    return html_string\n",
      "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay.\n",
      "for url in sample_urls['script_url']:\n",
      "    full_screenplay = scrape_screenplay(url)\n",
      "    sample_screenplay = full_screenplay[:900]\n",
      "    print(f\"\\n🎬🎬🎬🎬🎬🎬🎬\\n{sample_screenplay}\\n🎬🎬🎬🎬🎬🎬🎬\\n\")\n",
      "## BeautifulSoup & HTML\n",
      "Not all web pages will be as easy to scrape as these screenplay files, however. Let's say we wanted to scrape the lyrics for Missy Elliott's song \"The Rain (Supa Dupa Fly)\" (1997) from *Genius*.\n",
      "<img src=\"../images/Missy-Elliott.png\" width=100%, border=2>\n",
      "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Ghostbusters* page and that it contains a lot of information beyond the lyrics. Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too.\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called BeautifulSoup, which parses HTML documents.\n",
      "\n",
      "To understand BeautifulSoup and HTML, we're going to briefly depart from our Missy Elliot lyrics challenge to consider a much simpler website. (But we will return to Missy soon!) This toy website was made by the poet, programmer, and professor Allison Parrish explicitly for the purposes of teaching BeautifulSoup.\n",
      "## HTML\n",
      "Parrish's website is titled \"Kittens and the TV Shows They Love,\" and it can be found at the following URL: http://static.decontextualize.com/kittens.html Let's check it out.\n",
      "<img src=\"../images/kittens-web.png\" width=100%, border=2>\n",
      "If we use our requests library on this Kittens TV website, this is what we get:\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "### HTML Tags\n",
      "This is an HTML document. HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. \n",
      "| <img\\, border=2> | Image                         |\n",
      "\n",
      "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Kittens and the TV Shows They Love</h1>`\n",
      "### HTML Attributes, Classes, and IDs\n",
      "HTML elements sometimes come with even more information inside a tag. This will often be a keyword (like `class` or `id`) followed by an equals sign `=` and a further descriptor such as `<div class=\"kitten\">`\n",
      "We need to know about tags as well as attributes, classes, and IDs because this is how we're going to extract specific HTML data with BeautifulSoup.\n",
      "## BeautifulSoup\n",
      "from bs4 import BeautifulSoup\n",
      "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) that we want to use, which will always be `\"html.parser\"` for our purposes.\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "document\n",
      "## `.find()` HTML Elements\n",
      "We can use the `.find()` method to find and extract certain elements, such as a main header.\n",
      "document.find(\"h1\")\n",
      "If we want only the text contained between those tags, we can use `.text` to extract just the text.\n",
      "document.find(\"h1\").text\n",
      "type(document.find(\"h1\").text)\n",
      "Find the HTML element that contains an image. Hint: the HTML image tag is \"img\"\n",
      "document.find(\"img\")\n",
      "## `.find_all()` HTML Elements\n",
      "You can also extract multiple HTML elements at a time with `.find_all()`\n",
      "document.find_all(\"img\")\n",
      "document.find_all(\"div\", attrs={\"class\": \"kitten\"})\n",
      "document.find(\"h2\").text\n",
      "document.find_all(\"h2\")\n",
      "Let's try to extact the text from all the header2 elements:\n",
      "document.find_all(\"h2\").text\n",
      "Uh oh. That didn't work! In order to extract text data from multiple HTML elements, we need a `for` loop and some list-building.\n",
      "all_h2_headers = document.find_all(\"h2\")\n",
      "all_h2_headers\n",
      "First we will make an empty list called `h2_headers`. Then `for` each `header` in `all_h2_headers`, we will grab the `.text`, put it into a variable called `header_contents`, then `.append()` it to our `h2_headers` list.\n",
      "h2_headers = []\n",
      "for header in all_h2_headers:\n",
      "    header_contents = header.text\n",
      "    h2_headers.append(header_contents)\n",
      "h2_headers\n",
      "### List Comprehension?\n",
      "How might we transform this exact same `for` loop into a one-line list comprehension instead? Refer back to [List Comprehensions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to jog your memory.\n",
      "#Your Code Here\n",
      "**Check answer here**\n",
      "h2_headers = [header.text for header in all_h2_headers]\n",
      "h2_headers\n",
      "## Inspect The HTML 🧐\n",
      "Most times if you're looking to extract something from an HTML document, it's best to use your \"Inspect\" capabilities in your web browser. You can hover over elements that you're interested in and find that specific element in the HTML.\n",
      "<img src=\"../images/inspect.png\" width=100%, border=2>\n",
      "For example, if we hover over the main header:\n",
      "<img src=\"../images/inspect-h1.png\" width=100%, border=2>\n",
      "Or if we hover over a link:\n",
      "<img src=\"../images/inspect-a.png\" width=100%, border=2>\n",
      "## Back to Missy Elliott — Your Turn!\n",
      "Ok so now we've learned a little bit about how to use BeautifulSoup to parse HTML documents. So how would we apply what we've learned to extract Missy Elliott lyrics?\n",
      "<img src=\"../images/Missy-Elliott.png\" width=100%, border=2>\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_str = response.text\n",
      "\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "document\n",
      "What HTML element do we need to \"find\" to extract the song lyrics?\n",
      "missy_lyrics = #Your Code Here\n",
      "**Check answer here**\n",
      "missy_lyrics = document.find(\"p\")\n",
      "print(missy_lyrics)\n",
      "What HTML element do we need to \"find\" to extract the title?\n",
      "song_title = #Your Code Here\n",
      "print(song_title)\n",
      "import jsonlines\n",
      "import json\n",
      "import pandas as pd\n",
      "from pandas.io.json import json_normalize\n",
      "pd.set_option(\"max_columns\", 400)\n",
      "bang = pd.read_json(\"not_with_bang_tweets.jsonl\", lines=True)\n",
      "with jsonlines.open(\"not_with_bang_tweets.jsonl\") as reader:\n",
      "    data = []\n",
      "    for obj in reader:\n",
      "        data.append(jsonlines.Reader.read(reader))\n",
      "json_normalize(data, sep=\"_\")\n",
      "\n",
      "## Application Programming Interfaces (APIs)\n",
      "[Download relevant files here](https://melaniewalsh.org/APIs.zip)\n",
      "## Pros and Cons\n",
      "\n",
      "## Twitter Data Collection\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "- [Twitter API Set Up](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html)\n",
      "- [Twitter Data Collection](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Collection.html)\n",
      "- [Twitter Data Analysis](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Analysis.html)\n",
      "- [Twitter Data Sharing](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Sharing.html)\n",
      "## Configure Twarc in the Cloud\n",
      "twitter_handle = \"\"\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "twitter_handle = \"mellymeldubs\"\n",
      "consumer_key= \"HoZXZP6eW5CW86V9EcEw4oBcP\"\n",
      "consumer_secret = \"JKvzWsuXWsgDuAFeBW7i5MVLyDZ60AhXwSD0slBTQ70SX8Byga\"\n",
      "access_token = \"285395514-ZzvKkqW5fJrolpwqyRiNPDtaQ43R517c050APBQ0\"\n",
      "access_token_secret= \"m6ItBTrTV5SHWwXPSpDujWOpILIARaijHrtixxKWsHRKn\"\n",
      "!open ~/.twarc\n",
      "configuration = f\"\"\"[{twitter_handle}]\n",
      "consumer_key={consumer_key}\n",
      "consumer_secret = {consumer_secret}\n",
      "access_token = {access_token}\n",
      "access_token_secret= {access_token_secret}\n",
      "\"\"\"\n",
      "\n",
      "!open \n",
      "## Collecting Twitter Data\n",
      "[Download relevant files here](https://melaniewalsh.org/Collecting-Twitter-Data-v2.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to collect Twitter data with the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "## Install and Configure Twarc\n",
      "Because twarc relies on Twitter's API, we need to apply for a Twitter developer account and create a Twitter application before we use it. You can find instructions for the application process and for installing and configuring twarc here: [Twitter Collection Setup](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html).\n",
      "🚨Skip this section if you've already configured twarc!!🚨\n",
      "\n",
      "You can configure twarc by running `twarc conifgure` on the command line. Or you can type your Twitter handle (without the @ symbol) and [API keys](https://developer.twitter.com/en/apps) into the quotation marks below and run the cell.\n",
      "#Insert Your Twitter API Info here\n",
      "\n",
      "twitter_handle = \"\"\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "\n",
      "#The Code That Will Configure Twarc\n",
      "configuration = f\"\"\"[{twitter_handle}]\n",
      "consumer_key={consumer_key}\n",
      "consumer_secret = {consumer_secret}\n",
      "access_token = {access_token}\n",
      "access_token_secret= {access_token_secret}\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \".twarc\")\n",
      "with open(config_filename, \"w\") as file_object:\n",
      "    file_object.write(configuration)\n",
      "## Twitter API (Free Version)\n",
      "With the free version of the Twitter API, there are basically two ways to collect your own Twitter data: in real time or ~7 days in the past. To get data any further in the past requires a paid version of the Twitter API. Twarc allows you to collect tweets both in real time and ~7 days in the past.\n",
      "## Collect Tweets in Real Time\n",
      "## Twarc From the Command Line\n",
      "The easiest way to collect tweets with twarc is to use the command line. To collect tweets in real time, you can use the command `twarc filter`, followed by a search query, then the output operator `>` and a filename of your choosing with the \".jsonl\" file extension (which outputs your Twitter data to this JSONL file).\n",
      "`twarc filter \"search term\" > my_file.jsonl`\n",
      "For example, to collect tweets in real time that include the word \"coronavirus,\" you would run:\n",
      "`twarc  filter \"coronavirus\" > coronavirus_filter.jsonl`\n",
      "## Starting and Stopping Twarc\n",
      "If you run `twarc filter` from your command line, `twarc` will keep running until you explicitly stop the process. You can stop a process on the command line by typing `Ctrl + C`.\n",
      "As you may recall, we can run command line functions in Jupyter notebooks by putting an exclamation point `!` at the beginning of a cell. For some reason, however, `!twarc filter` and `!twarc search` don't play very well in Jupyter notebooks (or at least they don't play well consistently). Sometimes when you start running them, they won't stop—even when you hit the stop button or try to interrupt the kernel (the equivalent of `Ctrl + C`).\n",
      "\n",
      "Because of this unpredictability, I recommend that you 1) open your Terminal or PowerShell 2) navigate to the directory that contains this Jupyter notebook 3) and experiment with the twarc code below by copying it and pasting it into your command line, where you can more easily stop the processes.\n",
      "<img src=\"../images/twarc-filter-powershell.png\" width=100%, border=2>\n",
      "<img src=\"../images/twarc-filter-Terminal.png\" widht=100%, border=2>\n",
      "Run a live collection of tweets that include the word \"coronavirus\":\n",
      "`twarc  filter \"coronavirus\" > coronavirus_filter.jsonl`\n",
      "To stop this process:\n",
      "`Ctrl + C`\n",
      "Run a live collection of tweets that include the word \"Shakespeare\":\n",
      "`twarc filter \"Shakespeare\" > shakespeare_filter.jsonl`\n",
      "To stop this process:\n",
      "`Ctrl + C`\n",
      "## Check Number of Tweets Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l coronavirus_filter.jsonl\n",
      "!wc -l shakespeare_filter.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" coronavirus_filter.jsonl\n",
      "!find /v /c \"\" shakespeare_filter.jsonl\n",
      "## Collect Tweets From Past 7 days\n",
      "## Twarc From the Command Line\n",
      "To collect tweets from approximately 7 days in the past, you can use the command `twarc search`, followed by a search query, then the output operator `>` and a filename of your choosing with the \".jsonl\" file extension (which outputs your Twitter data to this JSONL file).\n",
      "Run a collection of tweets from the past ~7 days that include the word \"coronavirus\" for 10 seconds:\n",
      "`twarc search \"coronavirus\" > coronavirus_search.jsonl` \n",
      "Run a collection of tweets from the last ~7 days that include the word \"Shakespeare\":\n",
      "`twarc search \"Shakespeare\" > shakespeare_search.jsonl`\n",
      "## Check Number of Tweets Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l coronavirus_search.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" coronavirus_search.jsonl\n",
      "## Crafting a Good Twitter Query\n",
      "We made relatively simple queries to Twitter's API in the examples above. But there are more specific and more complex ways to make queries.\n",
      "\n",
      "To craft a good Twitter search query, it's important to understand and explore these myriad ways. A researcher named Igor Brigadir has compiled a wonderful resource that details many of the Twitter API search operators: https://github.com/igorbrigadir/twitter-advanced-search/blob/master/README.md\n",
      "## Search for Exact Phrases\n",
      "`twarc search \"\\\"an exact phrase\\\"\"`\n",
      "You can search for an *exact* phrase in a tweet by including the phrase in escaped `\\` quotation marks, as above.\n",
      "**\"Not with a bang but with a...\"**\n",
      "The first phrase that we're going to search for comes from the conclusion of T.S. Eliot's 1925 [poem \"The Hollow Men\"](https://msu.edu/~jungahre/transmedia/the-hollow-men.html):\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">**Not with a bang but with a whimper.**\n",
      "You've probably heard these lines before, even if you didn't know that they were written by the modernist poet T.S. Eliot. This phrase is a striking example of a bit of literary, poetic language that has gone \"viral\" in 21st-century American culture, both on and off the internet.\n",
      "Since there aren't a ton of tweets from the past 7 days that included the phrase \"not with a bang but with a\", we can run this `twarc search` from our Jupyter notebook. Because there is a small and finite number of tweets to be collected, this search will complete in a relatively short amount of time, and we don't have to worry about it running forever.\n",
      "!twarc search \"\\\"not with a bang but with a\\\"\" > not_with_bang_tweets.jsonl\n",
      "## Search for General Phrases\n",
      "**\"Touch my face\"**\n",
      "The other phrase we're going to search for comes from public health recommendations about preventing the spread of the coronavirus: that people should avoid touching their faces. Many people are, in light of these recommendations, discovering that it's actually very difficult not to touch your own face.\n",
      "\n",
      "Now the avoidance of touching one's face has sprouted up as a funny Twitter meme. These various \"touch my face\" memes serves as an interesting example of how online communities produce comedy and levity even in times of stress and crisis.\n",
      "%%html\n",
      "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Working on not touching my face :) <a href=\"https://t.co/qfyNdrDReh\">pic.twitter.com/qfyNdrDReh</a></p>&mdash; Hannah (@McBBQSauce) <a href=\"https://twitter.com/McBBQSauce/status/1235700933801242626?ref_src=twsrc%5Etfw\">March 5, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
      "Similarly, there aren't a ton of tweets from the past 7 days that included the phrase \"touch my face\", so we can also run this `twarc search` from our Jupyter notebook.\n",
      "!twarc search \"touch my face min_retweets:5\" > touch_my_face_tweets.jsonl \n",
      "  \n",
      "Great! Now we have some Twitter data. But before we dive into analysis, we need to complete one more step. We need to convert this JSON data to CSV data, which will be easier for us to work with. Luckily, there's a twarc \"utility\" for this very purpose.\n",
      "## Get Twarc Utilities\n",
      "There are a number of twarc \"utilities\" that enable you to manipulate and analyze Twitter data. With these utilities, you can do things such as convert JSON data to CSV data, count up the most frequent emojis used in tweets, make a network visualization of tweets and Twitter users, and more.\n",
      "\n",
      "These utilities are not available from the `pip install twarc` installation. To access the twarc utilities, you'll need to `git clone` the [twarc GitHub repository](https://github.com/DocNow/twarc) or download it as a zip file.\n",
      "\n",
      "The twarc repository should already be downloaded in your relevant files, but if you uncomment the line below, you can also clone the repository with this line of code.\n",
      "#!git clone https://github.com/DocNow/twarc.git\n",
      "## Use Twarc Utilities\n",
      "`python twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "To use a twarc utility, you need to call `python` from the command line and then include the utility's file path (they all should be in the \"twarc/utils\" subfolder).\n",
      "\n",
      "Note that if your Jupyter notebook is in exactly the same directory as the \"twarc\" repository, then you can run the code as above. However, if your Jupyter notebook is somewhere else, you will have to direct it to the correct location of \"twarc/utils\". For example`python /Users/melaniewalsh/twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "## Convert JSON to CSV\n",
      "To convert a JSON file to a CSV file, you can run `python twarc/utils/json2csv.py` followed by the JSONL filename, the output operator `>` and your desired filename for the CSV file.\n",
      "`python twarc/utils/json2csv.py json_file.jsonl > csv_file.csv`\n",
      ", border=2> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Heads up Windows users! The twarc utility json2csv.py will probably not work on your computer by default. You'll likely get a UnicodeEncodeError because Windows computers do not use Unicode (UTF-8) by default. However, you can make UTF-8 your default by following [these instructions](https://scholarslab.github.io/learn-twarc/08-win-region-settings) and restarting your comptuer. Then json2csv.py should work.\n",
      "\n",
      "Make \"bang.jsonl\" into \"bang.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text not_with_bang_tweets.jsonl > not_with_bang_tweets.csv\n",
      "Make \"face.jsonl\" into \"face.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text touch_my_face_tweets.jsonl > touch_my_face_tweets.csv\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text bang.jsonl > bang_experiment.csv\n",
      "## (Optional) Twarc From Python/Jupyter Notebooks\n",
      "Though I recommend collecting tweets from the command line, you can also use twarc as a Python library and run it in a Jupyter notebook. To import twarc, run `from twarc import Twarc` (as in the cell below). We're also going to import a library called JSON to help us output a JSON file.\n",
      "from twarc import Twarc\n",
      "import json\n",
      "## Configure Twarc\n",
      "To use Twarc as a Python library, you'll once again need to configure twarc with your [API keys](https://developer.twitter.com/en/apps) (\\*sigh\\*). Copy and paste them into the quotation marks below.\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "Quick tip! If you've already set up your Twitter API keys with `twarc configure`, you can find your API keys by running `open ~/.twarc` (Mac/Chrome OS) from the command line or using the code below (Mac/Chrome OS/Windows):\n",
      " Mac/Chrome OS\n",
      "!open ~/.twarc\n",
      " Mac/Chrome OS <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='center-left', border=2> Windows \n",
      "import os\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \".twarc\")\n",
      "print(open(config_filename).read())\n",
      "These commands will open/print the \".twarc\" document that hosts your API keys, and you can simply copy and paste the correct information into the variables in the cell above, then run the cell below.\n",
      "twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "## Make Live Tweet Collection Function\n",
      "Below I've written a Python function called `collect_live_tweets()` that uses `twarc.filter()`. This function accepts a search query, the number of tweets that you want to collect, and a filename with a .jsonl extension. This function will output your Twitter data to a file with this filename.\n",
      "def collect_live_tweets(search_query, number_of_desired_tweets, filename):    \n",
      "    \n",
      "    twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "\n",
      "    tweets = []\n",
      "    with open(filename, 'w', encoding='utf-8') as outfile:\n",
      "        for tweet in twarc.filter(search_query):\n",
      "            if len(tweets) < number_of_desired_tweets:\n",
      "                tweets.append(tweet)\n",
      "                json.dump(tweet, outfile)\n",
      "                outfile.write('\\n')\n",
      "            else:\n",
      "                break\n",
      "    return\n",
      "## Run Live Tweet Collection Function\n",
      "collect_live_tweets(\"coronavirus\", 100, \"coronavirus_filter.jsonl\")\n",
      "Below I've written a Python function called `collect_past_tweets()` that uses `twarc.search()`. This function accepts a search query, the maximum number of tweets that you want to collect, and a filename with a .jsonl extension. This function will output your Twitter data to a file with this filename. \n",
      "## Make Past Tweet Collection Function\n",
      "def collect_past_tweets(search_query, number_of_max_tweets, filename):    \n",
      "    \n",
      "    twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "\n",
      "    tweets = []\n",
      "    with open(filename, 'w', encoding='utf-8') as outfile:\n",
      "        for tweet in twarc.search(search_query):\n",
      "            if len(tweets) < number_of_max_tweets:\n",
      "                tweets.append(tweet)\n",
      "                json.dump(tweet, outfile)\n",
      "                outfile.write('\\n')\n",
      "            else:\n",
      "                break\n",
      "    return\n",
      "## Run Past Tweet Collection Function\n",
      "collect_past_tweets(\"coronavirus\", 1000, \"coronavirus_search.jsonl\")\n",
      "collect_past_tweets(\"\\\"not with a bang but with a\\\"\", 1000, \"bang.jsonl\")\n",
      "collect_past_tweets(\"touch my face min_retweets:10\", 2000, \"face.jsonl\")\n",
      "  \n",
      "## Web Scraping Plus Regular Expressions\n",
      "[Download relevant files here](https://melaniewalsh.org/Web-Scraping-Plus-Regex.zip)\n",
      "In this lesson, we're going to build a web scraping function called `get_all_songs_from_album` that will accept any artist name and album title (as long as they appear on Genius.com) and return a list of songs from that artist's album. In the next lesson, we're going to use these lists of songs to scrape lyrics for entire albums.\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "We're again going to use the `requests` library and the `BeautifulSoup` library to scrape our list of album songs. The first album that we're going to scrape is Missy Elliott's \"Under Construction\" (2002), which debuted at No. 3 on The Billboard Top 200 charts.\n",
      "<img src=\"../images/Missy-Under-Construction.png\" width=100%, border=2>\n",
      "response = requests.get(\"https://genius.com/albums/Missy-elliott/Under-construction\")\n",
      "html_string = response.text\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "### Your Turn!\n",
      "We want to extract just the song titles from Missy Elliott's album \"Under Construction.\" Turn on your web browser's \"Inspect\" function and find the HTML tag associated with each song title.\n",
      "song_title_tags = document.find_all(\"h3\")\n",
      "song_title_tags\n",
      "Now write a `for` loop that extracts the text from each song title tag and `.appends()` it to a list called `song_titles`:\n",
      "song_titles = []\n",
      "for song in song_title_tags:\n",
      "    song_title_missy = song.text\n",
      "    song_titles.append(song_title_missy)\n",
      "song_titles\n",
      "\n",
      "Now transform that same `for` loop into a list comprehension.\n",
      "missy_song_titles = [song.text for song in song_title_tags]\n",
      "missy_song_titles\n",
      "Are there things in your list that aren't song titles??? If so, use `.find_all()` with more specific HTML attributes `attrs={}`\n",
      "song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "missy_song_titles = [#Your Code Here]\n",
      "missy_song_titles\n",
      " \n",
      "## Regular Expressions + String Methods\n",
      "Great! Now we have have our list of song titles from Missy Elliot's album \"Under Construction.\" But if you notice, these song titles are pretty messy, and we need to clean them up.\n",
      "\n",
      "To do so, we're going to use built-in string methods and a Python library called `re`, short for regular expressions. Regular expressions are basically like a very sophisticated find-and-replace. Regular expression are not exclusive to Python and are used in many programming languages as well as in search engines, text editors, and word processors.\n",
      "import re\n",
      "To practice with regular expressions, we're going to use a sample messy song title from our messy song titles list.\n",
      "sample_song = \"\\n              Back in the Day (Ft.\\xa0JAY-Z)\\n              Lyrics\\n\"\n",
      "Remember the string method `.replace()`? With this built-in string method, we can easily get rid of the new line characters `\\n` or the word \"Lyrics\" from our `sample_song`, which is very useful.\n",
      "sample_song.replace(\"\\n\", \"\")\n",
      "sample_song.replace(\"Lyrics\", \"\")\n",
      "## `re.sub()`\n",
      "However, with regular expressions, we can replace strings with even more power and flexibility.\n",
      "To replace a string with regular expressions, we use `re.sub(old_pattern, new_pattern, text_that_contains_pattern)`. We can do exactly the same thing that we did with the built-in string method `.replace()`.\n",
      "sample_song\n",
      "re.sub(\"\\n\", \"\", sample_song)\n",
      "## Special RegEx Characters\n",
      "But regular expressions have certain characters with special pattern-matching powers, which is what allows us to do more cleaning, manipulating, and searching than with basic string methods. Below are some of the special regular expression characters.\n",
      "\n",
      "| Regular Expression Pattern       | Matches |\n",
      "|:---------------------------:|:-----------------------------------------------------------------------------------------------------------:|\n",
      "| `.` | any character                                         | \n",
      "| `\\w` | word                                         | \n",
      "| `\\W`                      | NOT word                                           |  \n",
      "| `\\d` | digit                                         | \n",
      "| `\\D`                      | NOT digit                                           | \n",
      "| `\\s` | whitespace                                         | \n",
      "| `\\S`                      | NOT whitespace                                          | \n",
      "| `[abc]`                      | Any of abc                                         |\n",
      "| `[^abc]`                      | Not any of abc                                         | \n",
      "| `(abc)`                      | Specific capture of \"abc\"                                         \n",
      "| `+`                      | 1 or more instances                                       | \n",
      "| `*`                      | 0 or more instances                                         | \n",
      "| `?`                      | 0 or 1 instance                                        | \n",
      "                   \n",
      "\n",
      "You can explore and experiment with regular expression characters and combinations at [Regexr.com](https://regexr.com/4vhf1).\n",
      "We can replace anything that is not a word `\\W` with \" \":\n",
      "re.sub(\"\\W\", \" \", sample_song)\n",
      "Replace anything that is a word `\\w` with \" \":\n",
      "re.sub(\"\\w\", \" \", sample_song)\n",
      "The character `+` means \"match one or more instance\" of the pattern, which allows us to remove multiple not word patterns in a row.\n",
      "re.sub(\"\\W+\", \" \", sample_song)\n",
      "## `re.compile()`\n",
      "An efficient way to build and save a regular expression pattern is with `re.compile()`\n",
      "not_word_pattern = re.compile(\"\\W+\")\n",
      "re.sub(not_word_pattern, \" \", sample_song)\n",
      "## `re.search()`\n",
      "In addition to replacing text, we can also find and return text. With `re.search()`, we can find and return any particular pattern. The `re.search()` function returns something called a \"match object,\" which we can access with `.group()`.\n",
      "For example, searching with the pattern `\\w+` will return the very first word in `sample_song`:\n",
      "word_pattern = re.compile(\"\\w+\")\n",
      "word_pattern.search(sample_song)\n",
      "word_pattern.search(sample_song).group(0)\n",
      "## `re.findall()`\n",
      "The function `re.findall()` will return a list of every instance of a particular pattern.\n",
      "word_pattern = re.compile(\"\\w+\")\n",
      "word_pattern.findall(sample_song)\n",
      "When you combine special regular expression characters, you can make your pattern matching very specific and very powerful. If we had a document that contains a bunch of email addresses, we could use the pattern `[\\w.]+@[\\w.]+` to find and extract the words that appear on other side of the `@` character, aka [find and extract all the email addresses](https://regexr.com/4vhfa).\n",
      "text_with_emails = \"The important email addresses are important@cool.com, signficant@sweet.org\"\n",
      "extracted_emails = re.findall('[\\w.]+@[\\w.]+', text_with_emails)\n",
      "extracted_emails\n",
      "## Match Before a Certain String\n",
      "For our song titles, we might want to [extract everything that comes before \"(Ft.)\"](regexr.com/4vhfg) because we don't care as much about the featured artists, and because the featured artists makes the song titles really long. To match everything that comes before a certain string, we can use the pattern `.*(?=desired_pattern)` which matches 0 or more `*` of any character `.` that comes before `(?=)` the string \"desired_pattern.\"\n",
      "before_ft_pattern = re.compile(\".*(?=Ft)\")\n",
      "before_ft_pattern.search(sample_song).group(0)\n",
      "## Backslash Escape Characters\n",
      "Nice! We got everything before the featured artist. Well, almost. We still have a weird, lingering open parentheses. That's because we were matching \"Ft\" not \"(Ft\". Let's match everything before \"(Ft\" instead.\n",
      "\n",
      "To do so, we're going to have to make a slight adjustment. Remember that parentheses `()` are special regular expression characters. To make clear that we mean a literal parentheses and not a special regular expression character, we have to use an escape backslash `\\` before the character. \n",
      "before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "clean_sample_song_title = before_ft_pattern.search(sample_song).group(0)\n",
      "clean_sample_song_title \n",
      "## Strip Leading and Trailing Whitespace\n",
      "The last thing we'll do to clean up our song title is to use the built-in string method `.strip()` which strips leading and trailing whitespace.\n",
      "clean_sample_song_title.strip()\n",
      "## Build Functions and Put It All Together\n",
      "Let's put all this cleaning together in a function called `clean_up`. It will match and strip everything before \"(Ft.)\" if the song title contains a featured artist, and it will remove the word \"Lyrics\" and strip whitespace if the song title does not contain \"(Ft.)\".\n",
      "## `clean_up()`\n",
      "def clean_up(song_title):\n",
      "\n",
      "    if \"Ft\" in song_title:\n",
      "        before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "        song_title_before_ft = before_ft_pattern.search(song_title).group(0)\n",
      "        clean_song_title = song_title_before_ft.strip()\n",
      "    \n",
      "    else:\n",
      "        song_title_no_lyrics = song_title.replace(\"Lyrics\", \"\")\n",
      "        clean_song_title = song_title_no_lyrics.strip()\n",
      "    \n",
      "    return clean_song_title\n",
      "[clean_up(song) for song in missy_song_titles]\n",
      "## `get_all_songs_from_album()`\n",
      "We were able to extract the song titles for Missy Elliott's album \"Under Construction.\" Success! But now we want to make a function that can do the same thing for any artist and album title.\n",
      "\n",
      "Take a look at [Beyonce's album \"Lemonade\"](https://genius.com/albums/Beyonce/Lemonade) on Genius.com and see how the web page compares to Missy Elliott's \"Under Construction.\" They look extremely similar, right? Because all Genius album pages are identical, we can use the same code that we did for Missy Elliott and just substitute in different artist and album names with an f-string for the Genius URL:\n",
      "\n",
      "`f\"https://genius.com/albums/{artist}/{album_name}\"`\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    \n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    \n",
      "    response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_string = response.text\n",
      "    document = BeautifulSoup(html_string, \"html.parser\")\n",
      "    song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [song_title.text for song_title in song_title_tags]\n",
      "    \n",
      "    clean_songs = []\n",
      "    for song_title in song_titles:\n",
      "        clean_song = clean_up(song_title)\n",
      "        clean_songs.append(clean_song)\n",
      "        \n",
      "    return clean_songs\n",
      "get_all_songs_from_album('Beyonce', 'Lemonade')\n",
      "get_all_songs_from_album('Taylor Swift', 'Red')\n",
      "get_all_songs_from_album('Mitski', 'Be The Cowboy')\n",
      "## Your Turn!\n",
      "get_all_songs_from_album('#Your Choice of Artist', '#Your Choice of Album')\n",
      "## Song Genius API\n",
      "[Download relevant files here](https://melaniewalsh.org/APIs.zip)\n",
      "In the previous lessons, we collected internet data by **\"scraping\"** the surface of Genius.com web pages. We extracted song titles and lyrics based on HTML tags — in other words, based on how the Genius web pages appeared when we, as users, navigated to those pages. But there's another major way of collecting internet data called **Application Programming Interfaces (APIs)**. In this lesson, we're going to use the Genius API and LyricsGenius— a Python package that a data scientist named John Miller created to work with the Genius API—to scrape all the song lyrics for any album on Genius.com.\n",
      "<img src=\"../images/Genius-API.png\" width=100%, border=2>\n",
      "## API Keys\n",
      "To use the Genius API, you need a special API key (specifically a \"Client Access Token\"), which is kind of like a password. Many APIs require authentication keys to gain access to them. To get your necessary Genius API keys, you need to navigate to the following URL: https://genius.com/api-clients.\n",
      "\n",
      "You'll be prompted to sign up for [a Genius account](https://genius.com/signup_or_login), which is required to gain API access. Signing up for a Genius account is free and easy. You just need a Genius nickname (which must be one word), an email address, and a password.\n",
      "<img src=\"../images/Genius-login.png\" width=100%, border=2>\n",
      "Once you're signed in, you should be taken to https://genius.com/api-clients, where you need to click the button that says \"New API Client.\"\n",
      "<img src=\"../images/Genius-New-API.png\" width=100%, border=2>\n",
      "After clicking \"New API Client,\" you'll be prompted to fill out a short form about the \"App\" that you need the Genius API for. You only need to fill out \"App Name\" and \"App Website URL.\"\n",
      "<img src=\"../images/New-API-Client.png\" width=100%, border=2>\n",
      "It doesn't really matter what you type in. You can simply put \"Song Lyrics Project\" for the \"App Name\" and the URL for our course website \"https://melaniewalsh.github.io/Intro-Cultural-Analytics/\" for the \"App Website URL.\"\n",
      "When you click \"Save,\" you'll be given a series of API Keys: a \"Client ID\" and a \"Client Secret.\" To generate your \"Client Access Token,\" which is the API key that we'll be using in this notebook, you need to click \"Generate Access Token\".\n",
      "<img src=\"../images/Access-Token.png\" width=100%, border=2>\n",
      "Finally, copy and paste your \"Client Access Token\" into the quotation marks below, and run the cell to save your variable \n",
      "client_access_token = \"INSERT YOUR CLIENT ACCESS TOKEN\"\n",
      "### Protecting Your API Key\n",
      "For this lesson, if you just copy and paste your Genius API key into your Jupyter notebook, everything should be fine. But that's actually not the best way of storing your API keys. If you published this notebook to GitHub, for example, other people might be able to read and use/steal your API key.\n",
      "\n",
      "For this reason, it's best practice to keep your API keys away from your code, such as in another file. For example, I made a new Python file called \"api_key.py\" that contains just one variable `your_client_access_token = \"MY API KEY\"`, and I can import this variable into my notebook with `import api_key`. \n",
      "import api_key\n",
      "By importing this Python file/module, I get access to the variable `your_client_access_token` without ever explicitly typing my secret API token in this notebook. If I wanted to publish this notebook to GitHub, then I could ignore or leave out the \"api_key.py\" file that actually contains my Client Access Token.\n",
      "api_key.your_client_access_token\n",
      "client_access_token = api_key.your_client_access_token\n",
      "## Making an API Request\n",
      "Making an API request looks a lot like typing a specially-formatted URL. That's kind of what it is. But instead of getting a rendered HTML web page in return, you get some data in return.\n",
      "\n",
      "There are a few different ways that we can query the Genius API, all of which are discussed in the [Genius API documentation](https://docs.genius.com/#songs-h2). The way we're going to cover in this lesson is [the basic search](https://docs.genius.com/#search-h2), which allows you to get a bunch of Genius data about any artist or songs that you search for, and it looks something like this:\n",
      "`http://api.genius.com/search?q={search_term}&access_token={client_access_token}`\n",
      "Sticking with our Missy Elliott theme/obsession, we're going to search for Genius data about Missy Elliott.\n",
      "\n",
      "First we're going to assign the string \"Missy Elliott\" to the variable `search_term`. Then we're going to make an f-string URL that contains the variables `search_term` and `client_access_token`.\n",
      "search_term = \"Missy Elliott\"\n",
      "genius_search_url = f\"http://api.genius.com/search?q={search_term}&access_token={client_access_token}\"\n",
      "This URL is basically all we need to make a Genius API request. Want proof? Run the cell below and print this URL, then copy and paste it into a new tab in your web browser.\n",
      "print(genius_search_url)\n",
      "It doesn't look pretty, but that's a bunch of Genius data about Missy Elliott!\n",
      "\n",
      "We can programmatically do the same thing by again using the Python library `requests` with this URL. Instead of getting the `.text` of the response, as we did before, we're going to use `.json()`.\n",
      "\n",
      "[JSON](https://www.w3schools.com/whatis/whatis_json.asp) is a data format that is commonly used by APIs. It's kind of like a more complex CSV file. JSON data can be nested and contains key/value pairs (much like a Python dictionary).\n",
      "import requests\n",
      "response = requests.get(genius_search_url)\n",
      "json_data = response.json()\n",
      "The JSON data that we get from our Missy Elliott API query looks something like this:\n",
      "json_data\n",
      "We can index this data (again, like a Python dictionary) and look at the first \"hit\" about Missy Elliott from Genius.com.\n",
      "json_data['response']['hits'][0]\n",
      "We can tell that this data describes the song \"Work It\" and contains other information about the song, such as its number of Genius annotations, its number of web page views, and links to images of its album cover.\n",
      "## Looping Through JSON Data\n",
      "## Get Song Titles\n",
      "for song in json_data['response']['hits']:\n",
      "    print(song['result']['full_title'])\n",
      "## Get Song Tiles and Page View Counts\n",
      "for song in json_data['response']['hits']:\n",
      "    print(song['result']['full_title'], song['result']['stats']['pageviews'])\n",
      "## Transform Song Titles and Page View Counts into a DataFrame\n",
      "We can loop through this data, append it into a list, and then transform that list into a Pandas dataframe by calling `pd.DataFrame()`\n",
      "import pandas as pd\n",
      "missy_songs = []\n",
      "for song in json_data['response']['hits']:\n",
      "    missy_songs.append([song['result']['full_title'], song['result']['stats']['pageviews']])\n",
      "    \n",
      "#Make a Pandas dataframe from a list\n",
      "missy_df = pd.DataFrame(missy_songs)\n",
      "missy_df.columns = ['song_title', 'page_views']\n",
      "missy_df\n",
      "## Transform Song Titles, Page View Counts, & Album Covers into a DataFrame\n",
      "Just for fun, we can do the same thing but also add links to images of Missy Elliott's album art—and we can actually display those images, too!\n",
      "\n",
      "To display images in a Pandas dataframe, you need to run `from IPython.core.display import HTML` and make the function `get_image_html()`. We're going to take the image URLs and make them into HTML objects.\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    image_html = f\"<img src='{link}' width='100'>\"\n",
      "    return image_html\n",
      "missy_songs = []\n",
      "for song in json_data['response']['hits']:\n",
      "    missy_songs.append([song['result']['full_title'], song['result']['stats']['pageviews'], song['result']['song_art_image_url']])\n",
      "    \n",
      "missy_df = pd.DataFrame(missy_songs)\n",
      "missy_df.columns = ['song_title', 'page_views','album_cover_url']\n",
      "\n",
      "#Use the function get_image_html()\n",
      "missy_df['album_cover'] = missy_df['album_cover_url'].apply(get_image_html)\n",
      "missy_df\n",
      "If we call `HTML()` on our dataframe and add the method `.to_html(escape=False)` to the dataframe, then it should display the dataframe with viewable images.\n",
      "HTML(missy_df.to_html(escape=False))\n",
      "## Your Turn! \n",
      "Replace \"Jorja Smith\" with any artist/musician of your choosing and run the following cells.\n",
      "search_term = \"Jorja Smith\"\n",
      "genius_search_url = f\"http://api.genius.com/search?q={search_term}&access_token={client_access_token}\"\n",
      "response = requests.get(genius_search_url)\n",
      "json_data = response.json()\n",
      "songs = []\n",
      "for song in json_data['response']['hits']:\n",
      "    songs.append((song['result']['full_title'], song['result']['stats']['pageviews'], song['result']['song_art_image_url']))\n",
      "    \n",
      "artist_df = pd.DataFrame(songs)\n",
      "artist_df.columns = ['song_title', 'page_views', 'album_cover_url']\n",
      "\n",
      "artist_df['album_cover'] = artist_df['album_cover_url'].apply(get_image_html)\n",
      "HTML(artist_df.to_html(escape=False))\n",
      "## Git and GitHub\n",
      "[Download relevant files here](https://melaniewalsh.org/APIs.zip)\n",
      " \n",
      "<img src=\"https://miro.medium.com/max/1366/1*mtsk3fQ_BRemFidhkel3dA.png\" width=100%, border=2>\n",
      "## What is Git?\n",
      "## Why is Git Useful?\n",
      "\n",
      "## What is GitHub?\n",
      "For this class, however, the main reason that we're learning about Git is to learn about GitHub. GitHub is a website/social network that's built on top of the Git version control software. It allows you to store and easily publish projects. GitHub has become a primary place for people to publish datasets and share code. Remember The Pudding's film dialogue data? They published it [on GitHub](https://github.com/matthewfdaniels/scripts/)!\n",
      "<img src=\"../images/Pudding-Github.png\" width=100%, border=2>\n",
      "Because GitHub is a treasure trove for cultural data analysis and because it's so widely used in programming culture, it's important for us to learn about.\n",
      "## Negative Critiques of GitHub\n",
      "<a href=\"https://www.theatlantic.com/technology/archive/2020/01/ice-contract-github-sparks-developer-protests/604339/\", border=2><img src=\"../images/GitHub-ICE-protest.png\" width=100%, border=2></a, border=2>\n",
      "However, it's also important to foreground some recent backlash against GitHub regarding their decision to renew a contract with U.S. Immigrations and Customs Enforcement (ICE). This decision has motivated some employees, developers, and GitHub users to protest and/or boycott the platform.\n",
      "\n",
      "You can read more about the controversy in [\"The Schism at the Heart of the Open-Source Movement\"](https://www.theatlantic.com/technology/archive/2020/01/ice-contract-github-sparks-developer-protests/604339/) and [Dear GitHub](https://github.com/drop-ice/dear-github-2.0/blob/master/README.md).\n",
      "## How to Access Data and Code on GitHub\n",
      "We're going to learn a few Git and GitHub basics by using a repository that I made for our class, which contains all the Jupyter notebooks, data, and images that you've been downloading via zip files: https://github.com/melaniewalsh/Intro-Cultural-Analytics-Notebooks. The frustrating experience of downloading all those zip files will hopefully make the value of Git and GitHub very clear. Now you can use Git to get all relevant class files!\n",
      "## Anatomy of a GitHub Page\n",
      "<img src=\"../images/GitHub-Anatomy.png\" width=100%, border=2>\n",
      "At first, GitHub pages look a little weird. The above diagram points to the some of the most important features.\n",
      "## Install Git\n",
      "To use Git, you first need to isntall Git. Instructions for installing Git can be found here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\n",
      "\n",
      "To install Git on a Windows computer, you need to install something called Git for Windows (which comes with something called Git Bash). The Digital Humanities Research Institute offers [helpful step-by-step instructions](https://github.com/DHRI-Curriculum/install/blob/master/sections/git.md#windows) here.\n",
      "!git --version\n",
      "## Download a GitHub Repository `git clone`\n",
      "To download an entire GitHub repository, first click the green \"Clone or download button\" and then copy the URL.\n",
      "<img src=\"../images/clone-button.png\" width=100%, border=2>\n",
      "Then, from the command line (or from Git Bash), type `git clone` and paste the URL for the desired GitHub repo.\n",
      "!git clone https://github.com/melaniewalsh/Intro-Cultural-Analytics-Notebooks.git\n",
      "This `git clone` command should download a folder called \"Intro-Cultural-Analytics-Notebooks,\" which contains all the files and folders in the repository.\n",
      "If you navigate into that folder and list its contents, you can see that it matches the GitHub website.\n",
      "cd Intro-Cultural-Analytics-Notebooks/\n",
      "ls\n",
      "## Update a GitHub Repository `git pull`\n",
      "Ok here's where Git will be very convenient. If I update the \"Introduction to Cultural Analytics\" repository with new files, you can update your copy of the repository and get those new files with a single command `git pull`, which \"pulls\" down the updates from GitHub.\n",
      "For example, I just added a new file called \"something_newer.txt\", and if you run the `git pull` command...\n",
      "!git pull\n",
      "then it will update the repository and download that file into the right place. To double check, you can run `ls` and see that \"something_newer.txt\" is now there.\n",
      "ls\n",
      "## Web Scraping\n",
      "Inspired by web scraping lessons from [Lauren Klein](https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class4-web-scraping-complete.ipynb) and [Allison Parrish](https://github.com/aparrish/dmep-python-intro/blob/master/scraping-html.ipynb)\n",
      "How did the data journalists of *The Pudding* actually collect the necessary screenplay and rap song data for their visualizations and analyses? \n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100%, border=2>\n",
      "<img src='../images/Pudding-rap-viz.png' width=100%, border=2>\n",
      "Well, they almost certainly used some form of web scraping. Web scraping is a way of computationally extracting data from the internet. It's one of the two ways of computationally collecting data from the internet that we're going to discuss in this class.\n",
      "## Why Do We Need To Scrape At All?\n",
      "To understand the necessity and significance of web scraping, let's walk through the likely data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
      "\n",
      "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red! These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project.\n",
      "Let's take a look:\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "urls\n",
      "This is an extremely useful dataset! But how can we actually use these URLs to get workable, computationally tractable text data? Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file....\n",
      "\n",
      "But that route would be suuuuper slow and painstaking, not to mention that we would lose some crucial data along the way—for example, information that might help us automatically distinguish the title of the movie from the screenplay itself. It would be much better if we could programmatically access the text data attached to every URL.\n",
      "## Responses and Requests\n",
      "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*.\n",
      "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%, border=2>\n",
      "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
      "<img src=\"../images/request-response.png\" width=100%, border=2>\n",
      "import requests\n",
      "## `.get()`\n",
      "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
      "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")\n",
      "## HTTP Status Code\n",
      "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error.\n",
      "response\n",
      "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL...\n",
      "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")\n",
      "<img src=\"../images/Ghostboogers.png\" width=100%, border=2>\n",
      "bad_response\n",
      "## Grab the `.text`\n",
      "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below.\n",
      "html_string = response.text\n",
      "Voila! Here's the screenplay now in a variable.\n",
      "print(html_string)\n",
      "## Looping Requests\n",
      "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies.\n",
      "sample_urls = urls.sample(10)\n",
      "sample_urls\n",
      "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code.\n",
      "def scrape_screenplay(url):\n",
      "    response = requests.get(url)\n",
      "    html_string = response.text\n",
      "    return html_string\n",
      "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay.\n",
      "for url in sample_urls['script_url']:\n",
      "    full_screenplay = scrape_screenplay(url)\n",
      "    sample_screenplay = full_screenplay[:900]\n",
      "    print(f\"\\n🎬🎬🎬🎬🎬🎬🎬\\n{sample_screenplay}\\n🎬🎬🎬🎬🎬🎬🎬\\n\")\n",
      "## BeautifulSoup & HTML\n",
      "Not all web pages will be as easy to scrape as these screenplay files, however. Let's say we wanted to scrape the lyrics for Missy Elliott's song \"The Rain (Supa Dupa Fly)\" (1997) from *Genius*.\n",
      "<img src=\"../images/Missy-Elliott.png\" width=100%, border=2>\n",
      "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Ghostbusters* page and that it contains a lot of information beyond the lyrics. Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too.\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called BeautifulSoup, which parses HTML documents.\n",
      "\n",
      "To understand BeautifulSoup and HTML, we're going to briefly depart from our Missy Elliot lyrics challenge to consider a much simpler website. (But we will return to Missy soon!) This toy website was made by the poet, programmer, and professor Allison Parrish explicitly for the purposes of teaching BeautifulSoup.\n",
      "## HTML\n",
      "Parrish's website is titled \"Kittens and the TV Shows They Love,\" and it can be found at the following URL: http://static.decontextualize.com/kittens.html Let's check it out.\n",
      "<img src=\"../images/kittens-web.png\" width=100%, border=2>\n",
      "If we use our requests library on this Kittens TV website, this is what we get:\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "### HTML Tags\n",
      "This is an HTML document. HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. \n",
      "| <img\\, border=2> | Image                         |\n",
      "\n",
      "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Kittens and the TV Shows They Love</h1>`\n",
      "### HTML Attributes, Classes, and IDs\n",
      "HTML elements sometimes come with even more information inside a tag, which will often be a keyword (like class or id) followed by an equals sing `+` and a further descriptor such as `<div class=\"kitten\">`\n",
      "We need to know about tags and attributes/classes/IDs because this is how we're going to extract specific data with BeautifulSoup.\n",
      "## BeautifulSoup\n",
      "from bs4 import BeautifulSoup\n",
      "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) we want to use, which always be \"html.parser\" in this class.\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "document\n",
      "## `.find()` HTML Elements\n",
      "We can use the `.find()` method to find and extract certain elements, such as a main header.\n",
      "document.find(\"h1\")\n",
      "If we want only the text contained between those tags, we can use `.text` to extract just the text.\n",
      "document.find(\"h1\").text\n",
      "Find the HTML element that contains an image. Hint: the HTML image tag is \"img\"\n",
      "#Your Code Here\n",
      "## `.find_all()` HTML Elements\n",
      "You can also extract multiple HTML elements at a time with `.find_all()`\n",
      "document.find_all(\"img\")\n",
      "document.find(\"h2\")\n",
      "document.find_all(\"h2\")\n",
      "document.find_all(\"h2\").string\n",
      "all_h2_headers = document.find_all(\"h2\")\n",
      "h2_headers = []\n",
      "for header in all_h2_headers:\n",
      "    header_contents = header.text\n",
      "    h2_headers.append(header_contents)\n",
      "h2_headers\n",
      "List Comprehension?\n",
      "h2_headers = [header.text for header in all_h2_headers]\n",
      "h2_headers\n",
      "\n",
      "\n",
      "\n",
      "response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "html_string = resp.text\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "import requests\n",
      "response = requests.get(\"https://genius.com/albums/Common/Black-america-again\")\n",
      "html_str = response.text\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_str = response.text\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "document\n",
      "missy_lyrics = document.find(\"p\").text\n",
      "print(missy_lyrics)\n",
      "## Web Scraping\n",
      "How did the data journalists of *The Pudding* actually collect the necessary screenplay and rap song data for their visualizations and analyses? \n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100%, border=2>\n",
      "<img src='../images/Pudding-rap-viz.png' width=100%, border=2>\n",
      "Well, they almost certainly used some form of web scraping. Web scraping is a way of computationally extracting data from the internet. It's one of the two ways of computationally collecting data from the internet that we're going to discuss.\n",
      "## Why Do We Need To Scrape At All?\n",
      "To understand the necessity and significance of web scraping, let's walk through the probable data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
      "\n",
      "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red! These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project.\n",
      "Let's take a look:\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "urls\n",
      "This is an extremely useful dataset! But how can we use these URLs to get workable, computationally tractable text data?\n",
      "\n",
      "Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file....but that route would be super slow and painstaking, and it would also cause us to lose some crucial data along the way—for example, information that might help us automatically distinguish the title of the movie from the screenplay itself. It would be much better if we could programmatically access the text data attached to every URL.\n",
      "## Responses and Requests\n",
      "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*.\n",
      "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%, border=2>\n",
      "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server that stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
      "<img src=\"../images/request-response.png\" width=100%, border=2>\n",
      "import requests\n",
      "## `.get()`\n",
      "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
      "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")\n",
      "## HTTP Status Code\n",
      "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error.\n",
      "response\n",
      "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL...\n",
      "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")\n",
      "<img src=\"../images/Ghostboogers.png\" width=100%, border=2>\n",
      "bad_response\n",
      "## Grab the `.text`\n",
      "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the next section.\n",
      "html_string = response.text\n",
      "Voila! Here's the screenplay now in a variable.\n",
      "print(html_string)\n",
      "## Looping Requests\n",
      "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies.\n",
      "sample_urls = urls.sample(10)\n",
      "sample_urls\n",
      "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code.\n",
      "def scrape_screenplay(url):\n",
      "    response = requests.get(url)\n",
      "    html_string = response.text\n",
      "    return html_string\n",
      "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay.\n",
      "for url in sample_urls['script_url']:\n",
      "    full_screenplay = scrape_screenplay(url)\n",
      "    sample_screenplay = full_screenplay[:900]\n",
      "    print(f\"\\n🎬🎬🎬🎬🎬🎬🎬\\n{sample_screenplay}\\n🎬🎬🎬🎬🎬🎬🎬\\n\")\n",
      "sample_urls['screenplay'] = sample_urls['script_url'].apply(scrape_screenplay)\n",
      "sample_urls\n",
      "print(html_string)\n",
      "delimiters = \"\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t|\\r\\n\\r\\n\"\n",
      "html_string[:10000]\n",
      "split_by_lines = html_string[1500:4000].split('\\n')\n",
      "split_by_lines\n",
      "characters = []\n",
      "dialogue = []\n",
      "character_dialogue = []\n",
      "\n",
      "for line in split_by_lines:\n",
      "    character = ''\n",
      "    if line.startswith(\"\\t\\t\\t\\t\\t\\t\"):\n",
      "        character = line\n",
      "    elif line.startswith(\"\\t\\t\\t\"):\n",
      "            #character.append(line)\n",
      "            print(line)\n",
      "screenplay_split = re.split(delimiters,html_string)\n",
      "character_dialogue = []\n",
      "for section in screenplay_split[:40]:\n",
      "    if section.isupper():\n",
      "        #if section.endswith(\"t\"):\n",
      "            print(section)\n",
      "for section in screenplay_split[:40]:\n",
      "    if section:\n",
      "        #chunk = chunk.strip()\n",
      "        section = section.replace('\\t', '')\n",
      "        section = section.replace('\\r', '')\n",
      "        split_dialogue = section.split('\\n', 1)\n",
      "        character = split_dialogue[0]\n",
      "        dialogue = split_dialogue[1]\n",
      "        dialogue = dialogue.replace(\"\\n\", \" \")\n",
      "        print(repr((character, dialogue)))\n",
      "        \n",
      "        #tmp = pd.DataFrame(datalist)\n",
      "        #tmp.columns = ['novel','person','label','weight'] \n",
      "        #make and append pandas dataframe\n",
      "#dialogue_df = pd.DataFrame()\n",
      "\n",
      "character_dialogue = []\n",
      "\n",
      "for section in screenplay_split:\n",
      "    \n",
      "    if \"\\n\\t\\t\" in section:\n",
      "        section = section.replace('\\t', '')\n",
      "        section = section.replace('\\r', '')\n",
      "        \n",
      "        split_dialogue = section.split('\\n', 1)\n",
      "        \n",
      "        character = split_dialogue[0]\n",
      "        dialogue = split_dialogue[1]\n",
      "        \n",
      "        dialogue = dialogue.replace(\"\\n\", \" \")\n",
      "        \n",
      "        if character.isupper():\n",
      "            character_dialogue.append((character, dialogue))\n",
      "\n",
      "dialogue_df = pd.DataFrame(character_dialogue)\n",
      "dialogue_df.columns = ['character', 'dialogue']\n",
      "dialogue_df.groupby('character')['num_words_spoken'].sum().sort_values(ascending=False)\n",
      "def calculate_num_words(text):\n",
      "    num_words = len(text)\n",
      "    return num_words\n",
      "dialogue_df['num_words_spoken'] = dialogue_df['dialogue'].apply(calculate_num_words)\n",
      "character_dialogue\n",
      "for section in screenplay_split[:40]:\n",
      "    if \"\\n\\t\\t\" in section:\n",
      "        #chunk = chunk.strip()\n",
      "        section = section.replace(''\n",
      "        split_dialogue = section.split('\\r\\n\\t\\t\\t\\t')\n",
      "        character = split_dialogue[0]\n",
      "        #character = character.strip()\n",
      "        #character = character.replace('', '')\n",
      "        dialogue = split_dialogue[1:]\n",
      "        dialogue = \" \".join(dialogue)\n",
      "        dialogue = dialogue.replace(\"\\n\", \"\")\n",
      "        print(repr(f\"CHARACTER : {character} DIALOGUE: {dialogue}\"))\n",
      "        \n",
      "for section in screenplay_split[:40]:\n",
      "    if \"\\n\\t\\t\" in section:\n",
      "        #chunk = chunk.strip()\n",
      "        \n",
      "        split_dialogue = section.split('\\r\\n\\t\\t\\t\\t')\n",
      "        character = split_dialogue[0]\n",
      "        #character = character.strip()\n",
      "        #character = character.replace('', '')\n",
      "        dialogue = split_dialogue[1:]\n",
      "        dialogue = \" \".join(dialogue)\n",
      "        dialogue = dialogue.replace(\"\\n\", \"\")\n",
      "        \n",
      "        print(f\"CHARACTER : {character} DIALOGUE: {dialogue}\")\n",
      "\n",
      "        #print(\"CHUNK\\n\", chunk)\n",
      "\n",
      "re.split(delimiters,html_string)\n",
      "screenplay_split[:4]\n",
      "sample = ['apple', 'pear', 'bag']\n",
      "sample[0]\n",
      "sample[1:-1]\n",
      "for chunk in split_ghost[:40]:\n",
      "    print(chunk.split('\\r\\n\\t\\t\\\\'))\n",
      "for chunk in split_ghost[:40]:\n",
      "    if \"\\r\\n\\t\\t\" in chunk:\n",
      "        #chunk = chunk.strip()\n",
      "        split_dialogue = chunk.split('\\r\\n\\t\\t\\t\\\\')\n",
      "        character = split_dialogue[0]\n",
      "        dialogue = split_dialogue[1:]\n",
      "        print(character, dialogue)                             \n",
      "        #print(\"CHUNK\\n\", chunk)\n",
      "html_string.split('\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t').split('\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t')\n",
      "response = requests.get(\"http://www.dailyscript.com/scripts/Legally%20Blonde.txt\")\n",
      "\n",
      "html_string = response.text\n",
      "html_string\n",
      "print(html_string.replace('\\n', '\\t'))\n",
      "html_string.split('\\n\\n')\n",
      "screenplay_by_lines = html_string.split('\\n')\n",
      "html_string.split('\\n\\n')[5]\n",
      "def detect_character(text):\n",
      "    for line in \n",
      "    character = ''\n",
      "    if text.isupper():\n",
      "        character = text\n",
      "    return character\n",
      "#characters = []\n",
      "for line in screenplay_by_lines:\n",
      "    for text in line:\n",
      "        print(text)\n",
      "for line in screenplay_by_lines:\n",
      "    if line.startswith(\"INT.\"):\n",
      "        print(line)\n",
      "characters = []\n",
      "for line in screenplay_by_lines:\n",
      "        if line.isupper():\n",
      "            if '\\n\\n' not in line:\n",
      "            #if line.endswith('\\n'):\n",
      "                if 'INT.' not in line and 'EXT.' not in line:\n",
      "                    character = line.strip()\n",
      "                    characters.append(character)\n",
      "lb_characters = set(characters)\n",
      "re.match(\"[A-Z\\n]+\", \"HELLO\").group(0)\n",
      "re.findall('[A-Z]*', html_string) \n",
      "re.findall(\"?:([A-Z]+ *[A-Z]+)\\n).*?(?=$|([A-Z]+ *[A-Z]+)\\n\", html_string)\n",
      "print(re.match(\"[A-Z\\n]+\", \"HELLO\\n\"))\n",
      "html_string\n",
      "\n",
      "print(re.findall(\"[A-Z]+\\n\", html_string))\n",
      "for line in screenplay_by_lines:\n",
      "    print(re.match(\"[A-Z]+\", line).group(0))\n",
      "lb_characters = set(characters)\n",
      "lb_characters\n",
      "for character in characters:\n",
      "    #for line in screenplay_by_lines:\n",
      "        #if line == character:\n",
      "        \n",
      "            dialogue = \n",
      "characters = []\n",
      "for line in screenplay_by_lines:\n",
      "    characters.append(detect_character(line))\n",
      "    #print(detect_character(line))\n",
      "    #print(len(line))\n",
      "characters \n",
      "split_words = re.split(\"\\W+\", html_string)\n",
      "re.findall(\"[A-Z].*[^A-Z]\", html_string)\n",
      "screenplay_by_lines[1]\n",
      "screenplay_by_lines[50:]\n",
      "characters = []\n",
      "line_numbers = []\n",
      "for number, line in enumerate(screenplay_by_lines):\n",
      "    if line.isupper():\n",
      "        if '\\n\\n' not in line:\n",
      "            #if line.endswith('\\n'):\n",
      "                if 'INT.' not in line and 'EXT.' not in line:\n",
      "                    character = line.strip()\n",
      "                    characters.append((character, number))\n",
      "                \n",
      "                    line_numbers.append(number)\n",
      "characters\n",
      "line_numbers\n",
      "import regex as re\n",
      "type(html_string)\n",
      "re.findall('[A-Z][^A-Z]*', html_string)\n",
      "for line in screenplay_by_lines:\n",
      "    re.findall('[A-Z][^A-Z]*', line)\n",
      "for line in screenplay_by_lines:\n",
      "    #if line.endswith('\\n\\n'):\n",
      "    if line.isupper():\n",
      "        print(line)\n",
      "response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "html_str = resp.text\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "## Collecting Twitter Data\n",
      "[Download relevant files here](https://melaniewalsh.org/Collecting-Twitter-Data.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to collect Twitter data with the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "\n",
      "Because twarc relies on Twitter's API, we need to apply for a Twitter developer account and create a Twitter application before we use it. You can find instructions for the application process and for installing and configuring twarc here: [Twitter Collection Setup](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html).\n",
      "## Twitter API (Free Version)\n",
      "With the free version of the Twitter API, there are basically two ways to collect your own Twitter data—in real time or ~7 days in the past. To get data any further in the past requires a paid version of the Twitter API. Twarc allows you to collect tweets both in real time and ~7 days in the past with `twarc filter` and `twarc search`.\n",
      "\n",
      "Some of our work with twarc in this notebook will take place from the command line (aka through Jupyter command line functions). Remember that the exclamation point `!` at the beginning of a Jupyter cell allows us to access command line functions from a Jupyter notebook. Any cell that begins with a `!` can also be run from your Terminal or PowerShell.\n",
      "## Collect Tweets in Real Time (`twarc filter`)\n",
      "`twarc filter \"search term\" > my_file.jsonl`\n",
      "Run for 10 seconds and then hit stop:\n",
      "`twarc  filter \"coronavirus\" > coronavirus_filter.jsonl`\n",
      "!timeout 10s twarc  filter \"coronavirus\" > coronavirus_filter.jsonl\n",
      "!timeout 5m twarc  filter \"coronavirus\" > coronavirus_filter.jsonl\n",
      "!twarc  filter \"coronavirus\" > coronavirus_filter.jsonl\n",
      "Run for 10 seconds and then hit stop:\n",
      "!timeout 10s twarc filter \"Shakespeare\" > shakespeare_filter.jsonl\n",
      "Run for 10 seconds and then hit stop:\n",
      "!timeout 10s twarc filter \"Bernie\" > bernie_filter.jsonl\n",
      "Stop-Process -Name \"twarc\"\n",
      "UTF 8 probs\n",
      "https://stackoverflow.com/questions/57131654/using-utf-8-encoding-chcp-65001-in-command-prompt-windows-powershell-window/57134096#57134096\n",
      "## Check Number of Tweets Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l coronavirus_filter.jsonl\n",
      "!wc -l shakespeare_filter.jsonl\n",
      "!wc -l bernie_filter.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c “” coronavirus_filter.jsonl\n",
      "!find /v /c “” shakespeare_filter.jsonl\n",
      "!find /v /c “” bernie_filter.jsonl\n",
      "## Collect Tweets From Last 7 days (`twarc search`)\n",
      "Run for 10 seconds and then hit stop:\n",
      "!twarc search \"coronavirus\" > coronavirus_search.jsonl \n",
      "Run for 10 seconds and then hit stop:\n",
      "!twarc search \"Shakespeare\" > shakespeare_search.jsonl \n",
      "Run for 10 seconds and then hit stop:\n",
      "!twarc search \"Bernie\" > bernie_search.jsonl \n",
      "## Check Number of Tweets Collected\n",
      "!wc -l coronavirus_search.jsonl\n",
      "!wc -l shakespeare_search.jsonl\n",
      "!wc -l bernie_search.jsonl\n",
      "## Crafting a Good Twitter Query\n",
      "We made relatively simple queries to Twitter's API in the examples above. They're all single words. But there are also more complex ways to make queries.\n",
      "\n",
      "To craft a good Twitter search query, it's important to understand (and explore!) these myriad ways. A researcher named Igor Brigadir has compiled a wonderful resource that details many of the Twitter API search operators: https://github.com/igorbrigadir/twitter-advanced-search/blob/master/README.md\n",
      "## Search for Exact Phrases\n",
      "`twarc search \"\\\"an exact phrase\\\"\"`\n",
      "You can search for an *exact* phrase in a tweet by including the phrase in escaped `\\` quotation marks, as above.\n",
      "### Not with a bang\n",
      "The first phrase that we're going to search for comes from the conclusion of T.S. Eliot's 1925 [poem \"The Hollow Men\"](https://msu.edu/~jungahre/transmedia/the-hollow-men.html):\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">**Not with a bang but with a whimper.**\n",
      "You've probably heard these lines before, even if you didn't know that they were written by the modernist poet T.S. Eliot. This phrase is a striking example of a bit of literary, poetic language that has gone \"viral\" in 21st-century American culture, both on and off the internet.\n",
      "Run for 20 seconds and then hit stop:\n",
      "!twarc search \"\\\"not with a bang but with a\\\"\" > bang.jsonl\n",
      "## Search for General Phrases\n",
      "### Touch my face\n",
      "The other phrase we're going to search for comes from public health recommendations about preventing the spread of the coronavirus: that people should avoid touching their faces. Many people are, in light of these recommendations, discovering that it's actually very difficult not to touch your own face.\n",
      "\n",
      "Now the avoidance of touching one's face has sprouted up as a funny Twitter meme. These various \"touch my face\" memes serves as an interesting example of how online communities produce comedy and levity even in times of stress and crisis.\n",
      "%%html\n",
      "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Working on not touching my face :) <a href=\"https://t.co/qfyNdrDReh\">pic.twitter.com/qfyNdrDReh</a></p>&mdash; Hannah (@McBBQSauce) <a href=\"https://twitter.com/McBBQSauce/status/1235700933801242626?ref_src=twsrc%5Etfw\">March 5, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
      "Run for 20 seconds and then hit stop:\n",
      "!twarc search \"touch my face min_retweets:10\" > face.jsonl \n",
      "  \n",
      "Great! Now we have some Twitter data. But before we dive into analysis, we need to complete one more step. We need to convert this JSON data to CSV data, which is easier to work with. Luckily, there's a twarc \"utility\" for that very purpose.\n",
      "## Get Twarc Utilities\n",
      "There are a number of twarc \"utilities\" that enable you to manipulate and analyze Twitter data. With them, you can convert JSON data to CSV data, count up the most frequent emojis, make a network visualization, and more.\n",
      "\n",
      "These utilities are not available from the `pip install twarc` installation. To access the twarc utilities, you'll need to `git clone` the [twarc GitHub repository](https://github.com/DocNow/twarc) or download it as a zip file.\n",
      "\n",
      "The twarc repository should already be downloaded in your relevant files, but if you uncomment the line below, you can also clone the repository with this line of code.\n",
      "#!git clone https://github.com/DocNow/twarc.git\n",
      "## Use Twarc Utilities\n",
      "`python twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "To use a twarc utility, you need to call `python` from the command line and then include the utility's file path (which should be in the \"twarc/utils\" subfolder). Note that if your Jupyter notebook is in exactly the same directory as the \"twarc\" repository, then you can run the code as above. However, if your notebook is somewhere else, you will have to direct it to the correct location of \"twarc/utils\". For example`python /Users/melaniewalsh/twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "## Convert JSON to CSV\n",
      "`python twarc/utils/json2csv.py json_file.jsonl > csv_file.csv`\n",
      "Make \"bang.jsonl\" into \"bang.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text bang.jsonl > bang.csv\n",
      "Make \"face.jsonl\" into \"face.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text face.jsonl > face.csv\n",
      "  \n",
      "Read in tweet CSV files with Pandas\n",
      "import pandas\n",
      "Set Pandas display options so columns are wider and more columns are visible\n",
      "pandas.set_option('max_colwidth', 5000)\n",
      "pandas.set_option('max_columns', 40)\n",
      "pandas.set_option('max_rows', 100)\n",
      "bang_df = pandas.read_csv('bang.csv')\n",
      "face_df = pandas.read_csv('face.csv')\n",
      "Check what Twitter metadata exists in this CSV file\n",
      "bang_df.columns\n",
      "As you can see above, there is a *lot* of metadata that comes with every tweet!\n",
      "Check the size of dataframe (number of rows = number of tweets)\n",
      "bang_df.shape\n",
      "face_df.shape\n",
      "Preview dataframes\n",
      "bang_df.head()\n",
      "face_df.head()\n",
      "  \n",
      "  \n",
      "## Filter Twitter Data to Only Categories of Interest\n",
      "bang_df[['created_at', 'tweet_type', 'media', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "bang_df = bang_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "face_df = face_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "## Display Links and Images in Twitter Data\n",
      "To display links and images in our Twitter dataframe, run the cells below. We're converting the image URL into an HTML image tag and then displaying our dataframe as an HTML object.\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "bang_df['media'] = bang_df['media'].fillna(\"No Image\")\n",
      "bang_df['media']= bang_df['media'].apply(get_image_html)\n",
      "\n",
      "face_df['media'] = face_df['media'].fillna(\"No Image\")\n",
      "face_df['media']= face_df['media'].apply(get_image_html)\n",
      "### Not With a Bang\n",
      "HTML(bang_df.to_html(render_links=True, escape=False))\n",
      "### Touch My Face\n",
      "HTML(face_df.to_html(render_links=True, escape=False))\n",
      "HTML(face_df[['text', 'media', 'retweet_count']].to_html(render_links=True, escape=False))\n",
      "## Sort By Top Retweets\n",
      "bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "bang_rt_sorted = bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(bang_rt_sorted.to_html(render_links=True, escape=False))\n",
      "face_rt_sorted = face_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(face_rt_sorted.to_html(render_links=True, escape=False))\n",
      "## Identify Top Hashtags\n",
      "`twarc/utils/tags.py tweets.jsonl`\n",
      "!python twarc/utils/tags.py bang.jsonl\n",
      "!python twarc/utils/tags.py face.jsonl\n",
      "## Create a Word Cloud\n",
      "`twarc/utils/wordcloud.py tweets.jsonl`\n",
      "!python twarc/utils/wordcloud.py bang.jsonl > not_with_a_bang.html\n",
      "[not_with_a_bang.html](not_with_a_bang.html)\n",
      "from IPython.display import IFrame\n",
      "IFrame(src=\"not_with_a_bang.html\", width=800, height=800)\n",
      "!python twarc/utils/wordcloud.py face.jsonl > touch_my_face.html\n",
      "from IPython.display import IFrame\n",
      "IFrame(src=\"touch_my_face.html\", width=800, height=800)\n",
      "## Count Emojis `twarc/utils/emojis.py`\n",
      "!python twarc/utils/emojis.py bang.jsonl | head -n 20\n",
      "!python twarc/utils/emojis.py face.jsonl | head -n 20\n",
      "## Your Turn!\n",
      "Now choose your own Twitter search term or query.\n",
      "## Collect Tweets From Last 7 Days\n",
      "!twarc search \"your search query\" > your_search.jsonl \n",
      "## Count How Many Tweets You Collected\n",
      "!wc -l your_search.jsonl\n",
      "## Convert Your JSON data to CSV data\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text  your_search.jsonl > your_search.csv\n",
      "## Read in as Pandas dataframe\n",
      "your_df = pandas.read_csv('your_search.csv')\n",
      "## Add Metadata\n",
      "Filter your dataframe and add at least one new metadata column that we haven't explored yet.\n",
      "your_df.columns\n",
      "When you run the cell below, right-click to \"Enable Scrolling for Outputs\" and scroll through to see what the new metadata category looks like. Discuss this category with your group and how you might use it for a Twitter analysis.\n",
      "your_df[['created_at', 'tweet_type', 'media', 'tweet_url', '#YOUR NEW METADATA HERE', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "Now save your filtered dataframe as `filtered_df`\n",
      "#Your Code Here\n",
      "## Explore Data with Links and Images\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "filtered_df['media'] = filtered_df['media'].fillna(\"No Image\")\n",
      "filtered_df['media']= filtered_df['media'].apply(get_image_html)\n",
      "HTML(filtered_df.to_html(render_links=True, escape=False))\n",
      "## Sort Your Twitter Data by Top Retweets\n",
      "#Your code here\n",
      "What is the most retweeted tweet in your dataset?\n",
      "\n",
      "## Count Most Frequent Emojis\n",
      "!python twarc/utils/emojis.py your_search.jsonl | head -n 20\n",
      "## Web Scraping\n",
      "How did the data journalists of *The Pudding* actually collect the necessary screenplay and rap song data for their visualizations and analyses? \n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100%, border=2>\n",
      "<img src='../images/Pudding-rap-viz.png' width=100%, border=2>\n",
      "Well, they almost certainly used some form of web scraping. Web scraping is a way of computationally extracting data from the internet. It's one of the two ways of computationally collecting data from the internet that we're going to discuss.\n",
      "## Why Do We Need To Scrape At All?\n",
      "To understand the necessity and significance of web scraping, let's walk through the probable data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
      "\n",
      "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red! These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project.\n",
      "Let's take a look:\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "urls\n",
      "This is an extremely useful dataset! But how can we use these URLs to get workable, computationally tractable text data?\n",
      "\n",
      "Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file....but that route would be super slow and painstaking, and it would also cause us to lose some crucial data along the way—for example, information that might help us automatically distinguish the title of the movie from the screenplay itself. It would be much better if we could programmatically access the text data attached to every URL.\n",
      "## Responses and Requests\n",
      "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*.\n",
      "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%, border=2>\n",
      "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server that stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
      "<img src=\"../images/request-response.png\" width=100%, border=2>\n",
      "import requests\n",
      "## `.get()`\n",
      "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
      "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")\n",
      "## HTTP Status Code\n",
      "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error.\n",
      "response\n",
      "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL...\n",
      "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")\n",
      "<img src=\"../images/Ghostboogers.png\" width=100%, border=2>\n",
      "bad_response\n",
      "## Grab the `.text`\n",
      "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the next section.\n",
      "html_string = response.text\n",
      "Voila! Here's the screenplay now in a variable.\n",
      "print(html_string)\n",
      "## Looping Requests\n",
      "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies.\n",
      "sample_urls = urls.sample(10)\n",
      "sample_urls\n",
      "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code.\n",
      "def scrape_screenplay(url):\n",
      "    response = requests.get(url)\n",
      "    html_string = response.text\n",
      "    return html_string\n",
      "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay.\n",
      "for url in sample_urls['script_url']:\n",
      "    full_screenplay = scrape_screenplay(url)\n",
      "    sample_screenplay = full_screenplay[:900]\n",
      "    print(f\"\\n🎬🎬🎬🎬🎬🎬🎬\\n{sample_screenplay}\\n🎬🎬🎬🎬🎬🎬🎬\\n\")\n",
      "sample_urls['screenplay'] = sample_urls['script_url'].apply(scrape_screenplay)\n",
      "sample_urls\n",
      "print(html_string)\n",
      "delimiters = \"\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t|\\r\\n\\r\\n\"\n",
      "html_string[:10000]\n",
      "split_by_lines = html_string[1500:4000].split('\\n')\n",
      "split_by_lines\n",
      "characters = []\n",
      "dialogue = []\n",
      "character_dialogue = []\n",
      "\n",
      "for line in split_by_lines:\n",
      "    character = ''\n",
      "    if line.startswith(\"\\t\\t\\t\\t\\t\\t\"):\n",
      "        character = line\n",
      "    elif line.startswith(\"\\t\\t\\t\"):\n",
      "            #character.append(line)\n",
      "            print(line)\n",
      "screenplay_split = re.split(delimiters,html_string)\n",
      "character_dialogue = []\n",
      "for section in screenplay_split[:40]:\n",
      "    if section.isupper():\n",
      "        #if section.endswith(\"t\"):\n",
      "            print(section)\n",
      "for section in screenplay_split[:40]:\n",
      "    if section:\n",
      "        #chunk = chunk.strip()\n",
      "        section = section.replace('\\t', '')\n",
      "        section = section.replace('\\r', '')\n",
      "        split_dialogue = section.split('\\n', 1)\n",
      "        character = split_dialogue[0]\n",
      "        dialogue = split_dialogue[1]\n",
      "        dialogue = dialogue.replace(\"\\n\", \" \")\n",
      "        print(repr((character, dialogue)))\n",
      "        \n",
      "        #tmp = pd.DataFrame(datalist)\n",
      "        #tmp.columns = ['novel','person','label','weight'] \n",
      "        #make and append pandas dataframe\n",
      "#dialogue_df = pd.DataFrame()\n",
      "\n",
      "character_dialogue = []\n",
      "\n",
      "for section in screenplay_split:\n",
      "    \n",
      "    if \"\\n\\t\\t\" in section:\n",
      "        section = section.replace('\\t', '')\n",
      "        section = section.replace('\\r', '')\n",
      "        \n",
      "        split_dialogue = section.split('\\n', 1)\n",
      "        \n",
      "        character = split_dialogue[0]\n",
      "        dialogue = split_dialogue[1]\n",
      "        \n",
      "        dialogue = dialogue.replace(\"\\n\", \" \")\n",
      "        \n",
      "        if character.isupper():\n",
      "            character_dialogue.append((character, dialogue))\n",
      "\n",
      "dialogue_df = pd.DataFrame(character_dialogue)\n",
      "dialogue_df.columns = ['character', 'dialogue']\n",
      "dialogue_df.groupby('character')['num_words_spoken'].sum().sort_values(ascending=False)\n",
      "def calculate_num_words(text):\n",
      "    num_words = len(text)\n",
      "    return num_words\n",
      "dialogue_df['num_words_spoken'] = dialogue_df['dialogue'].apply(calculate_num_words)\n",
      "character_dialogue\n",
      "for section in screenplay_split[:40]:\n",
      "    if \"\\n\\t\\t\" in section:\n",
      "        #chunk = chunk.strip()\n",
      "        section = section.replace(''\n",
      "        split_dialogue = section.split('\\r\\n\\t\\t\\t\\t')\n",
      "        character = split_dialogue[0]\n",
      "        #character = character.strip()\n",
      "        #character = character.replace('', '')\n",
      "        dialogue = split_dialogue[1:]\n",
      "        dialogue = \" \".join(dialogue)\n",
      "        dialogue = dialogue.replace(\"\\n\", \"\")\n",
      "        print(repr(f\"CHARACTER : {character} DIALOGUE: {dialogue}\"))\n",
      "        \n",
      "for section in screenplay_split[:40]:\n",
      "    if \"\\n\\t\\t\" in section:\n",
      "        #chunk = chunk.strip()\n",
      "        \n",
      "        split_dialogue = section.split('\\r\\n\\t\\t\\t\\t')\n",
      "        character = split_dialogue[0]\n",
      "        #character = character.strip()\n",
      "        #character = character.replace('', '')\n",
      "        dialogue = split_dialogue[1:]\n",
      "        dialogue = \" \".join(dialogue)\n",
      "        dialogue = dialogue.replace(\"\\n\", \"\")\n",
      "        \n",
      "        print(f\"CHARACTER : {character} DIALOGUE: {dialogue}\")\n",
      "\n",
      "        #print(\"CHUNK\\n\", chunk)\n",
      "\n",
      "re.split(delimiters,html_string)\n",
      "screenplay_split[:4]\n",
      "sample = ['apple', 'pear', 'bag']\n",
      "sample[0]\n",
      "sample[1:-1]\n",
      "for chunk in split_ghost[:40]:\n",
      "    print(chunk.split('\\r\\n\\t\\t\\\\'))\n",
      "for chunk in split_ghost[:40]:\n",
      "    if \"\\r\\n\\t\\t\" in chunk:\n",
      "        #chunk = chunk.strip()\n",
      "        split_dialogue = chunk.split('\\r\\n\\t\\t\\t\\\\')\n",
      "        character = split_dialogue[0]\n",
      "        dialogue = split_dialogue[1:]\n",
      "        print(character, dialogue)                             \n",
      "        #print(\"CHUNK\\n\", chunk)\n",
      "html_string.split('\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t').split('\\r\\n\\r\\n\\t\\t\\t\\t\\t\\t')\n",
      "response = requests.get(\"http://www.dailyscript.com/scripts/Legally%20Blonde.txt\")\n",
      "\n",
      "html_string = response.text\n",
      "html_string\n",
      "print(html_string.replace('\\n', '\\t'))\n",
      "html_string.split('\\n\\n')\n",
      "screenplay_by_lines = html_string.split('\\n')\n",
      "html_string.split('\\n\\n')[5]\n",
      "def detect_character(text):\n",
      "    for line in \n",
      "    character = ''\n",
      "    if text.isupper():\n",
      "        character = text\n",
      "    return character\n",
      "#characters = []\n",
      "for line in screenplay_by_lines:\n",
      "    for text in line:\n",
      "        print(text)\n",
      "for line in screenplay_by_lines:\n",
      "    if line.startswith(\"INT.\"):\n",
      "        print(line)\n",
      "characters = []\n",
      "for line in screenplay_by_lines:\n",
      "        if line.isupper():\n",
      "            if '\\n\\n' not in line:\n",
      "            #if line.endswith('\\n'):\n",
      "                if 'INT.' not in line and 'EXT.' not in line:\n",
      "                    character = line.strip()\n",
      "                    characters.append(character)\n",
      "lb_characters = set(characters)\n",
      "re.match(\"[A-Z\\n]+\", \"HELLO\").group(0)\n",
      "re.findall('[A-Z]*', html_string) \n",
      "re.findall(\"?:([A-Z]+ *[A-Z]+)\\n).*?(?=$|([A-Z]+ *[A-Z]+)\\n\", html_string)\n",
      "print(re.match(\"[A-Z\\n]+\", \"HELLO\\n\"))\n",
      "html_string\n",
      "\n",
      "print(re.findall(\"[A-Z]+\\n\", html_string))\n",
      "for line in screenplay_by_lines:\n",
      "    print(re.match(\"[A-Z]+\", line).group(0))\n",
      "lb_characters = set(characters)\n",
      "lb_characters\n",
      "for character in characters:\n",
      "    #for line in screenplay_by_lines:\n",
      "        #if line == character:\n",
      "        \n",
      "            dialogue = \n",
      "characters = []\n",
      "for line in screenplay_by_lines:\n",
      "    characters.append(detect_character(line))\n",
      "    #print(detect_character(line))\n",
      "    #print(len(line))\n",
      "characters \n",
      "split_words = re.split(\"\\W+\", html_string)\n",
      "re.findall(\"[A-Z].*[^A-Z]\", html_string)\n",
      "screenplay_by_lines[1]\n",
      "screenplay_by_lines[50:]\n",
      "characters = []\n",
      "line_numbers = []\n",
      "for number, line in enumerate(screenplay_by_lines):\n",
      "    if line.isupper():\n",
      "        if '\\n\\n' not in line:\n",
      "            #if line.endswith('\\n'):\n",
      "                if 'INT.' not in line and 'EXT.' not in line:\n",
      "                    character = line.strip()\n",
      "                    characters.append((character, number))\n",
      "                \n",
      "                    line_numbers.append(number)\n",
      "characters\n",
      "line_numbers\n",
      "import regex as re\n",
      "type(html_string)\n",
      "re.findall('[A-Z][^A-Z]*', html_string)\n",
      "for line in screenplay_by_lines:\n",
      "    re.findall('[A-Z][^A-Z]*', line)\n",
      "for line in screenplay_by_lines:\n",
      "    #if line.endswith('\\n\\n'):\n",
      "    if line.isupper():\n",
      "        print(line)\n",
      "response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "html_str = resp.text\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "## Web Scraping\n",
      "Inspired by web scraping lessons from [Lauren Klein](https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class4-web-scraping-complete.ipynb) and [Allison Parrish](https://github.com/aparrish/dmep-python-intro/blob/master/scraping-html.ipynb)\n",
      "How did the data journalists of *The Pudding* actually collect the necessary screenplay and rap song data for their visualizations and analyses? \n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100%, border=2>\n",
      "<img src='../images/Pudding-rap-viz.png' width=100%, border=2>\n",
      "Well, they almost certainly used some form of web scraping. Web scraping is a way of computationally extracting data from the internet. It's one of the two ways of computationally collecting data from the internet that we're going to discuss in this class.\n",
      "## Why Do We Need To Scrape At All?\n",
      "To understand the necessity and significance of web scraping, let's walk through the likely data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
      "\n",
      "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red! These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project.\n",
      "Let's take a look:\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "urls\n",
      "This is an extremely useful dataset! But how can we actually use these URLs to get workable, computationally tractable text data? Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file....\n",
      "\n",
      "But that route would be suuuuper slow and painstaking, not to mention that we would lose some crucial data along the way—for example, information that might help us automatically distinguish the title of the movie from the screenplay itself. It would be much better if we could programmatically access the text data attached to every URL.\n",
      "## Responses and Requests\n",
      "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*.\n",
      "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%, border=2>\n",
      "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
      "<img src=\"../images/request-response.png\" width=100%, border=2>\n",
      "import requests\n",
      "## `.get()`\n",
      "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
      "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")\n",
      "## HTTP Status Code\n",
      "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error.\n",
      "response\n",
      "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL...\n",
      "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")\n",
      "<img src=\"../images/Ghostboogers.png\" width=100%, border=2>\n",
      "bad_response\n",
      "## Grab the `.text`\n",
      "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below.\n",
      "html_string = response.text\n",
      "Voila! Here's the screenplay now in a variable.\n",
      "print(html_string)\n",
      "## Looping Requests\n",
      "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies.\n",
      "sample_urls = urls.sample(10)\n",
      "sample_urls\n",
      "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code.\n",
      "def scrape_screenplay(url):\n",
      "    response = requests.get(url)\n",
      "    html_string = response.text\n",
      "    return html_string\n",
      "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay.\n",
      "for url in sample_urls['script_url']:\n",
      "    full_screenplay = scrape_screenplay(url)\n",
      "    sample_screenplay = full_screenplay[:900]\n",
      "    print(f\"\\n🎬🎬🎬🎬🎬🎬🎬\\n{sample_screenplay}\\n🎬🎬🎬🎬🎬🎬🎬\\n\")\n",
      "## BeautifulSoup & HTML\n",
      "Not all web pages will be as easy to scrape as these screenplay files, however. Let's say we wanted to scrape the lyrics for Missy Elliott's song \"The Rain (Supa Dupa Fly)\" (1997) from *Genius*.\n",
      "<img src=\"../images/Missy-Elliott.png\" width=100%, border=2>\n",
      "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Ghostbusters* page and that it contains a lot of information beyond the lyrics. Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too.\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called BeautifulSoup, which parses HTML documents.\n",
      "\n",
      "To understand BeautifulSoup and HTML, we're going to briefly depart from our Missy Elliot lyrics challenge to consider a much simpler website. (But we will return to Missy soon!) This toy website was made by the poet, programmer, and professor Allison Parrish explicitly for the purposes of teaching BeautifulSoup.\n",
      "## HTML\n",
      "Parrish's website is titled \"Kittens and the TV Shows They Love,\" and it can be found at the following URL: http://static.decontextualize.com/kittens.html Let's check it out.\n",
      "<img src=\"../images/kittens-web.png\" width=100%, border=2>\n",
      "If we use our requests library on this Kittens TV website, this is what we get:\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "### HTML Tags\n",
      "This is an HTML document. HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. \n",
      "| <img\\, border=2> | Image                         |\n",
      "\n",
      "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Kittens and the TV Shows They Love</h1>`\n",
      "### HTML Attributes, Classes, and IDs\n",
      "HTML elements sometimes come with even more information inside a tag, which will often be a keyword (like class or id) followed by an equals sing `+` and a further descriptor such as `<div class=\"kitten\">`\n",
      "We need to know about tags and attributes/classes/IDs because this is how we're going to extract specific data with BeautifulSoup.\n",
      "## BeautifulSoup\n",
      "from bs4 import BeautifulSoup\n",
      "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) we want to use, which always be \"html.parser\" in this class.\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "document\n",
      "## `.find()` HTML Elements\n",
      "We can use the `.find()` method to find and extract certain elements, such as a main header.\n",
      "document.find(\"h1\")\n",
      "If we want only the text contained between those tags, we can use `.text` to extract just the text.\n",
      "document.find(\"h1\").text\n",
      "Find the HTML element that contains an image. Hint: the HTML image tag is \"img\"\n",
      "#Your Code Here\n",
      "## `.find_all()` HTML Elements\n",
      "You can also extract multiple HTML elements at a time with `.find_all()`\n",
      "document.find_all(\"img\")\n",
      "document.find(\"h2\")\n",
      "document.find_all(\"h2\")\n",
      "document.find_all(\"h2\").string\n",
      "all_h2_headers = document.find_all(\"h2\")\n",
      "h2_headers = []\n",
      "for header in all_h2_headers:\n",
      "    header_contents = header.text\n",
      "    h2_headers.append(header_contents)\n",
      "h2_headers\n",
      "List Comprehension?\n",
      "h2_headers = [header.text for header in all_h2_headers]\n",
      "h2_headers\n",
      "\n",
      "\n",
      "\n",
      "response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "html_string = resp.text\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "import requests\n",
      "response = requests.get(\"https://genius.com/albums/Common/Black-america-again\")\n",
      "html_str = response.text\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_str = response.text\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "document\n",
      "missy_lyrics = document.find(\"p\").text\n",
      "print(missy_lyrics)\n",
      "## Configure Twarc in the Cloud\n",
      "twitter_handle = \"\"\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "configuration = f\"\"\"[{twitter_handle}]\n",
      "consumer_key={consumer_key}\n",
      "consumer_secret = {consumer_secret}\n",
      "access_token = {access_token}\n",
      "access_token_secret= {access_token_secret}\n",
      "\"\"\"\n",
      "with open(\"~/.twarc\", \"w\") as file_object:\n",
      "    file_object.write(configuration)\n",
      "## Web Scraping\n",
      "[Download relevant files here](https://melaniewalsh.org/Web-Scraping.zip)\n",
      "Inspired by web scraping lessons from [Lauren Klein](https://github.com/laurenfklein/emory-qtm340/blob/master/notebooks/class4-web-scraping-complete.ipynb) and [Allison Parrish](https://github.com/aparrish/dmep-python-intro/blob/master/scraping-html.ipynb)\n",
      "How did the data journalists of *The Pudding* actually collect the necessary screenplay and rap song data for their visualizations and analyses? \n",
      "<img src='../images/Pudding-film-dialogue-Mean-Girls.png' width=100%, border=2>\n",
      "<img src='../images/Pudding-rap-viz.png' width=100%, border=2>\n",
      "Well, they almost certainly used some form of web scraping. Web scraping is a way of computationally extracting data from the internet. It's one of the two ways of computationally collecting data from the internet that we're going to discuss in this class.\n",
      "## Why Do We Need To Scrape At All?\n",
      "To understand the necessity and significance of web scraping, let's walk through the likely data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
      "\n",
      "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). Go Big Red! These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project.\n",
      "Let's take a look:\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "urls\n",
      "This is an extremely useful dataset! But how can we actually use these URLs to get workable, computationally tractable text data? Well, we could manually navigate to each URL and then copy and paste each screenplay into a plain text file....\n",
      "\n",
      "But that route would be suuuuper slow and painstaking, not to mention that we would lose some crucial data along the way—for example, information that might help us automatically distinguish the title of the movie from the screenplay itself. It would be much better if we could programmatically access the text data attached to every URL.\n",
      "## Responses and Requests\n",
      "The first step down this more efficient web scraping path is to import a Python library called [requests](https://requests.readthedocs.io/en/master/), which will help us access the web page data associated with every URL. We're going to practice by **requesting** the screenplay data for the movie *Ghostbusters*.\n",
      "<img src=\"https://pbs.twimg.com/profile_images/1203012648406667264/RR4pig4F_400x400.jpg\" width=100%, border=2>\n",
      "When you type in a URL in your search address bar, you're sending an HTTP **request** for a web page, and the server which stores that web page will accordingly send back a **response**, some web page data that your browser will render.\n",
      "<img src=\"../images/request-response.png\" width=100%, border=2>\n",
      "import requests\n",
      "## `.get()`\n",
      "With the `.get()` method, we can request to \"get\" web page data for a specific URL, which we will store in a varaible called `response`.\n",
      "response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostbusters.txt\")\n",
      "## HTTP Status Code\n",
      "If you check out `response`, it will simply tell you its [HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), aka whether the request was successful or not. \"200\" is a successful response, while \"404\" is a common \"Page Not Found\" error.\n",
      "response\n",
      "Let's see what happens if I change the title of the movie from *Ghostbusters* to *Ghostboogers* in the URL...\n",
      "bad_response = requests.get(\"http://www.scifiscripts.com/scripts/Ghostboogers.txt\")\n",
      "<img src=\"../images/Ghostboogers.png\" width=100%, border=2>\n",
      "bad_response\n",
      "## Grab the `.text`\n",
      "To actually get at the text data in the reponse, we need to use `.text`, which we will save in a variable called `html_string`. The text data that we're getting is formatted in the HTML markup language, which we will talk more about in the BeautifulSoup section below.\n",
      "html_string = response.text\n",
      "Voila! Here's the screenplay now in a variable.\n",
      "print(html_string)\n",
      "## Looping Requests\n",
      "Let's quickly demonstrate how we might loop through the URLs and get text data for each film. We're going to create a smaller dataframe from the Cornell Movie Dialogue Corpus, which consists of 10 randomly selected movies.\n",
      "import pandas as pd\n",
      "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')\n",
      "sample_urls = urls.sample(10)\n",
      "sample_urls\n",
      "Then we're going to make a function called `scrape_screenplay()` that includes our `requests.get()` and `response.text` code.\n",
      "def scrape_screenplay(url):\n",
      "    response = requests.get(url)\n",
      "    html_string = response.text\n",
      "    return html_string\n",
      "Then we're going to loop through every URL in our smaller sample dataframe, scrape each screenplay from each URL, and then print the first 900 characters for each screenplay.\n",
      "for url in sample_urls['script_url']:\n",
      "    full_screenplay = scrape_screenplay(url)\n",
      "    sample_screenplay = full_screenplay[:900]\n",
      "    print(f\"\\n🎬🎬🎬🎬🎬🎬🎬\\n{sample_screenplay}\\n🎬🎬🎬🎬🎬🎬🎬\\n\")\n",
      "## BeautifulSoup & HTML\n",
      "Not all web pages will be as easy to scrape as these screenplay files, however. Let's say we wanted to scrape the lyrics for Missy Elliott's song \"The Rain (Supa Dupa Fly)\" (1997) from *Genius*.\n",
      "<img src=\"../images/Missy-Elliott.png\" width=100%, border=2>\n",
      "Even at a glance, we can tell that this *Genius* web page is a lot more complicated than the *Ghostbusters* page and that it contains a lot of information beyond the lyrics. Sure enough, if we use our requests library again and try to grab the data for this web page, the underlying data is much more complicated, too.\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "How can we extract just the song lyrics from this messy soup of a document? Luckily there's a Python library that can help us called BeautifulSoup, which parses HTML documents.\n",
      "\n",
      "To understand BeautifulSoup and HTML, we're going to briefly depart from our Missy Elliot lyrics challenge to consider a much simpler website. (But we will return to Missy soon!) This toy website was made by the poet, programmer, and professor Allison Parrish explicitly for the purposes of teaching BeautifulSoup.\n",
      "## HTML\n",
      "Parrish's website is titled \"Kittens and the TV Shows They Love,\" and it can be found at the following URL: http://static.decontextualize.com/kittens.html Let's check it out.\n",
      "<img src=\"../images/kittens-web.png\" width=100%, border=2>\n",
      "If we use our requests library on this Kittens TV website, this is what we get:\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "print(html_string)\n",
      "### HTML Tags\n",
      "This is an HTML document. HTML stands for HyperText Markup Language. It is the standard language for writing web page documents. The most important thing you need to know about HTML is that the language uses HTML \"tags\" to represent different elements, such as a main header `<h1>`. \n",
      "| <img\\, border=2> | Image                         |\n",
      "\n",
      "HTML tags often, but not always, require a \"closing\" tag. For example, the main header \"Kittens and the TV Shows They Love\" will be surrounded by `<h1>` (opening tag) and `</h1>` (closing tag) on either side: `<h1>Kittens and the TV Shows They Love</h1>`\n",
      "### HTML Attributes, Classes, and IDs\n",
      "HTML elements sometimes come with even more information inside a tag. This will often be a keyword (like `class` or `id`) followed by an equals sign `=` and a further descriptor such as `<div class=\"kitten\">`\n",
      "We need to know about tags as well as attributes, classes, and IDs because this is how we're going to extract specific HTML data with BeautifulSoup.\n",
      "## BeautifulSoup\n",
      "from bs4 import BeautifulSoup\n",
      "To make a BeautifulSoup document, we call `BeautifulSoup()` with two parameters: the `html_string` from our HTTP request and [the kind of parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use) that we want to use, which will always be `\"html.parser\"` for our purposes.\n",
      "response = requests.get(\"http://static.decontextualize.com/kittens.html\")\n",
      "html_string = response.text\n",
      "\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "document\n",
      "## `.find()` HTML Elements\n",
      "We can use the `.find()` method to find and extract certain elements, such as a main header.\n",
      "document.find(\"h1\")\n",
      "If we want only the text contained between those tags, we can use `.text` to extract just the text.\n",
      "document.find(\"h1\").text\n",
      "type(document.find(\"h1\").text)\n",
      "Find the HTML element that contains an image. Hint: the HTML image tag is \"img\"\n",
      "document.find(\"img\")\n",
      "## `.find_all()` HTML Elements\n",
      "You can also extract multiple HTML elements at a time with `.find_all()`\n",
      "document.find_all(\"img\")\n",
      "document.find_all(\"div\", attrs={\"class\": \"kitten\"})\n",
      "document.find(\"h2\").text\n",
      "document.find_all(\"h2\")\n",
      "Let's try to extact the text from all the header2 elements:\n",
      "document.find_all(\"h2\").text\n",
      "Uh oh. That didn't work! In order to extract text data from multiple HTML elements, we need a `for` loop and some list-building.\n",
      "all_h2_headers = document.find_all(\"h2\")\n",
      "all_h2_headers\n",
      "First we will make an empty list called `h2_headers`. Then `for` each `header` in `all_h2_headers`, we will grab the `.text`, put it into a variable called `header_contents`, then `.append()` it to our `h2_headers` list.\n",
      "h2_headers = []\n",
      "for header in all_h2_headers:\n",
      "    header_contents = header.text\n",
      "    h2_headers.append(header_contents)\n",
      "h2_headers\n",
      "### List Comprehension?\n",
      "How might we transform this exact same `for` loop into a one-line list comprehension instead? Refer back to [List Comprehensions](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to jog your memory.\n",
      "#Your Code Here\n",
      "**Check answer here**\n",
      "h2_headers = [header.text for header in all_h2_headers]\n",
      "h2_headers\n",
      "## Inspect The HTML 🧐\n",
      "Most times if you're looking to extract something from an HTML document, it's best to use your \"Inspect\" capabilities in your web browser. You can hover over elements that you're interested in and find that specific element in the HTML.\n",
      "<img src=\"../images/inspect.png\" width=100%, border=2>\n",
      "For example, if we hover over the main header:\n",
      "<img src=\"../images/inspect-h1.png\" width=100%, border=2>\n",
      "Or if we hover over a link:\n",
      "<img src=\"../images/inspect-a.png\" width=100%, border=2>\n",
      "## Back to Missy Elliott — Your Turn!\n",
      "Ok so now we've learned a little bit about how to use BeautifulSoup to parse HTML documents. So how would we apply what we've learned to extract Missy Elliott lyrics?\n",
      "<img src=\"../images/Missy-Elliott.png\" width=100%, border=2>\n",
      "response = requests.get(\"https://genius.com/Missy-elliott-the-rain-supa-dupa-fly-lyrics\")\n",
      "html_str = response.text\n",
      "\n",
      "document = BeautifulSoup(html_str, \"html.parser\")\n",
      "document\n",
      "What HTML element do we need to \"find\" to extract the song lyrics?\n",
      "missy_lyrics = #Your Code Here\n",
      "**Check answer here**\n",
      "missy_lyrics = document.find(\"p\")\n",
      "print(missy_lyrics)\n",
      "What HTML element do we need to \"find\" to extract the title?\n",
      "song_title = #Your Code Here\n",
      "print(song_title)\n",
      "## Git and GitHub\n",
      "[Download relevant files here](https://melaniewalsh.org/APIs.zip)\n",
      " \n",
      "<img src=\"https://miro.medium.com/max/1366/1*mtsk3fQ_BRemFidhkel3dA.png\" width=100%, border=2>\n",
      "## What is Git?\n",
      "## Why is Git Useful?\n",
      "\n",
      "## What is GitHub?\n",
      "For this class, however, the main reason that we're learning about Git is to learn about GitHub. GitHub is a website/social network that's built on top of the Git version control software. It allows you to store and easily publish projects. GitHub has become a primary place for people to publish datasets and share code. Remember The Pudding's film dialogue data? They published it [on GitHub](https://github.com/matthewfdaniels/scripts/)!\n",
      "<img src=\"../images/Pudding-Github.png\" width=100%, border=2>\n",
      "Because GitHub is a treasure trove for cultural data analysis and because it's so widely used in programming culture, it's important for us to learn about.\n",
      "## Negative Critiques of GitHub\n",
      "<a href=\"https://www.theatlantic.com/technology/archive/2020/01/ice-contract-github-sparks-developer-protests/604339/\", border=2><img src=\"../images/GitHub-ICE-protest.png\" width=100%, border=2></a, border=2>\n",
      "However, it's also important to foreground some recent backlash against GitHub regarding their decision to renew a contract with U.S. Immigrations and Customs Enforcement (ICE). This decision has motivated some employees, developers, and GitHub users to protest and/or boycott the platform.\n",
      "\n",
      "You can read more about the controversy in [\"The Schism at the Heart of the Open-Source Movement\"](https://www.theatlantic.com/technology/archive/2020/01/ice-contract-github-sparks-developer-protests/604339/) and [Dear GitHub](https://github.com/drop-ice/dear-github-2.0/blob/master/README.md).\n",
      "## How to Access Data and Code on GitHub\n",
      "We're going to learn a few Git and GitHub basics by using a repository that I made for our class, which contains all the Jupyter notebooks, data, and images that you've been downloading via zip files: https://github.com/melaniewalsh/Intro-Cultural-Analytics-Notebooks. The frustrating experience of downloading all those zip files will hopefully make the value of Git and GitHub very clear. Now you can use Git to get all relevant class files!\n",
      "## Anatomy of a GitHub Page\n",
      "<img src=\"../images/GitHub-Anatomy.png\" width=100%, border=2>\n",
      "At first, GitHub pages look a little weird. The above diagram points to the some of the most important features.\n",
      "## Install Git\n",
      "To use Git, you first need to isntall Git. Instructions for installing Git can be found here: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\n",
      "\n",
      "To install Git on a Windows computer, you need to install something called Git for Windows (which comes with something called Git Bash). The Digital Humanities Research Institute offers [helpful step-by-step instructions](https://github.com/DHRI-Curriculum/install/blob/master/sections/git.md#windows) here.\n",
      "!git --version\n",
      "## Download a GitHub Repository `git clone`\n",
      "To download an entire GitHub repository, first click the green \"Clone or download button\" and then copy the URL.\n",
      "<img src=\"../images/clone-button.png\" width=100%, border=2>\n",
      "Then, from the command line (or from Git Bash), type `git clone` and paste the URL for the desired GitHub repo.\n",
      "!git clone https://github.com/melaniewalsh/Intro-Cultural-Analytics-Notebooks.git\n",
      "This `git clone` command should download a folder called \"Intro-Cultural-Analytics-Notebooks,\" which contains all the files and folders in the repository.\n",
      "If you navigate into that folder and list its contents, you can see that it matches the GitHub website.\n",
      "cd Intro-Cultural-Analytics-Notebooks/\n",
      "ls\n",
      "## Update a GitHub Repository `git pull`\n",
      "Ok here's where Git will be very convenient. If I update the \"Introduction to Cultural Analytics\" repository with new files, you can update your copy of the repository and get those new files with a single command `git pull`, which \"pulls\" down the updates from GitHub.\n",
      "For example, I just added a new file called \"something_newer.txt\", and if you run the `git pull` command...\n",
      "!git pull\n",
      "then it will update the repository and download that file into the right place. To double check, you can run `ls` and see that \"something_newer.txt\" is now there.\n",
      "ls\n",
      "## Song Genius API\n",
      "[Download relevant files here](https://melaniewalsh.org/APIs.zip)\n",
      "In the previous lessons, we collected internet data by **\"scraping\"** the surface of Genius.com web pages. We extracted song titles and lyrics based on HTML tags — in other words, based on how the Genius web pages appeared when we, as users, navigated to those pages. But there's another major way of collecting internet data called **Application Programming Interfaces (APIs)**. In this lesson, we're going to use the Genius API and LyricsGenius— a Python package that a data scientist named John Miller created to work with the Genius API—to scrape all the song lyrics for any album on Genius.com.\n",
      "<img src=\"../images/Genius-API.png\" width=100%, border=2>\n",
      "## API Keys\n",
      "To use the Genius API, you need a special API key (specifically a \"Client Access Token\"), which is kind of like a password. Many APIs require authentication keys to gain access to them. To get your necessary Genius API keys, you need to navigate to the following URL: https://genius.com/api-clients.\n",
      "\n",
      "You'll be prompted to sign up for [a Genius account](https://genius.com/signup_or_login), which is required to gain API access. Signing up for a Genius account is free and easy. You just need a Genius nickname (which must be one word), an email address, and a password.\n",
      "<img src=\"../images/Genius-login.png\" width=100%, border=2>\n",
      "Once you're signed in, you should be taken to https://genius.com/api-clients, where you need to click the button that says \"New API Client.\"\n",
      "<img src=\"../images/Genius-New-API.png\" width=100%, border=2>\n",
      "After clicking \"New API Client,\" you'll be prompted to fill out a short form about the \"App\" that you need the Genius API for. You only need to fill out \"App Name\" and \"App Website URL.\"\n",
      "<img src=\"../images/New-API-Client.png\" width=100%, border=2>\n",
      "It doesn't really matter what you type in. You can simply put \"Song Lyrics Project\" for the \"App Name\" and the URL for our course website \"https://melaniewalsh.github.io/Intro-Cultural-Analytics/\" for the \"App Website URL.\"\n",
      "When you click \"Save,\" you'll be given a series of API Keys: a \"Client ID\" and a \"Client Secret.\" To generate your \"Client Access Token,\" which is the API key that we'll be using in this notebook, you need to click \"Generate Access Token\".\n",
      "<img src=\"../images/Access-Token.png\" width=100%, border=2>\n",
      "Finally, copy and paste your \"Client Access Token\" into the quotation marks below, and run the cell to save your variable \n",
      "client_access_token = \"INSERT YOUR CLIENT ACCESS TOKEN\"\n",
      "### Protecting Your API Key\n",
      "For this lesson, if you just copy and paste your Genius API key into your Jupyter notebook, everything should be fine. But that's actually not the best way of storing your API keys. If you published this notebook to GitHub, for example, other people might be able to read and use/steal your API key.\n",
      "\n",
      "For this reason, it's best practice to keep your API keys away from your code, such as in another file. For example, I made a new Python file called \"api_key.py\" that contains just one variable `your_client_access_token = \"MY API KEY\"`, and I can import this variable into my notebook with `import api_key`. \n",
      "import api_key\n",
      "By importing this Python file/module, I get access to the variable `your_client_access_token` without ever explicitly typing my secret API token in this notebook. If I wanted to publish this notebook to GitHub, then I could ignore or leave out the \"api_key.py\" file that actually contains my Client Access Token.\n",
      "api_key.your_client_access_token\n",
      "client_access_token = api_key.your_client_access_token\n",
      "## Making an API Request\n",
      "Making an API request looks a lot like typing a specially-formatted URL. That's kind of what it is. But instead of getting a rendered HTML web page in return, you get some data in return.\n",
      "\n",
      "There are a few different ways that we can query the Genius API, all of which are discussed in the [Genius API documentation](https://docs.genius.com/#songs-h2). The way we're going to cover in this lesson is [the basic search](https://docs.genius.com/#search-h2), which allows you to get a bunch of Genius data about any artist or songs that you search for, and it looks something like this:\n",
      "`http://api.genius.com/search?q={search_term}&access_token={client_access_token}`\n",
      "Sticking with our Missy Elliott theme/obsession, we're going to search for Genius data about Missy Elliott.\n",
      "\n",
      "First we're going to assign the string \"Missy Elliott\" to the variable `search_term`. Then we're going to make an f-string URL that contains the variables `search_term` and `client_access_token`.\n",
      "search_term = \"Missy Elliott\"\n",
      "genius_search_url = f\"http://api.genius.com/search?q={search_term}&access_token={client_access_token}\"\n",
      "This URL is basically all we need to make a Genius API request. Want proof? Run the cell below and print this URL, then copy and paste it into a new tab in your web browser.\n",
      "print(genius_search_url)\n",
      "It doesn't look pretty, but that's a bunch of Genius data about Missy Elliott!\n",
      "\n",
      "We can programmatically do the same thing by again using the Python library `requests` with this URL. Instead of getting the `.text` of the response, as we did before, we're going to use `.json()`.\n",
      "\n",
      "[JSON](https://www.w3schools.com/whatis/whatis_json.asp) is a data format that is commonly used by APIs. It's kind of like a more complex CSV file. JSON data can be nested and contains key/value pairs (much like a Python dictionary).\n",
      "import requests\n",
      "response = requests.get(genius_search_url)\n",
      "json_data = response.json()\n",
      "The JSON data that we get from our Missy Elliott API query looks something like this:\n",
      "json_data\n",
      "We can index this data (again, like a Python dictionary) and look at the first \"hit\" about Missy Elliott from Genius.com.\n",
      "json_data['response']['hits'][0]\n",
      "We can tell that this data describes the song \"Work It\" and contains other information about the song, such as its number of Genius annotations, its number of web page views, and links to images of its album cover.\n",
      "## Looping Through JSON Data\n",
      "## Get Song Titles\n",
      "for song in json_data['response']['hits']:\n",
      "    print(song['result']['full_title'])\n",
      "## Get Song Tiles and Page View Counts\n",
      "for song in json_data['response']['hits']:\n",
      "    print(song['result']['full_title'], song['result']['stats']['pageviews'])\n",
      "## Transform Song Titles and Page View Counts into a DataFrame\n",
      "We can loop through this data, append it into a list, and then transform that list into a Pandas dataframe by calling `pd.DataFrame()`\n",
      "import pandas as pd\n",
      "missy_songs = []\n",
      "for song in json_data['response']['hits']:\n",
      "    missy_songs.append([song['result']['full_title'], song['result']['stats']['pageviews']])\n",
      "    \n",
      "#Make a Pandas dataframe from a list\n",
      "missy_df = pd.DataFrame(missy_songs)\n",
      "missy_df.columns = ['song_title', 'page_views']\n",
      "missy_df\n",
      "## Transform Song Titles, Page View Counts, & Album Covers into a DataFrame\n",
      "Just for fun, we can do the same thing but also add links to images of Missy Elliott's album art—and we can actually display those images, too!\n",
      "\n",
      "To display images in a Pandas dataframe, you need to run `from IPython.core.display import HTML` and make the function `get_image_html()`. We're going to take the image URLs and make them into HTML objects.\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    image_html = f\"<img src='{link}' width='100'>\"\n",
      "    return image_html\n",
      "missy_songs = []\n",
      "for song in json_data['response']['hits']:\n",
      "    missy_songs.append([song['result']['full_title'], song['result']['stats']['pageviews'], song['result']['song_art_image_url']])\n",
      "    \n",
      "missy_df = pd.DataFrame(missy_songs)\n",
      "missy_df.columns = ['song_title', 'page_views','album_cover_url']\n",
      "\n",
      "#Use the function get_image_html()\n",
      "missy_df['album_cover'] = missy_df['album_cover_url'].apply(get_image_html)\n",
      "missy_df\n",
      "If we call `HTML()` on our dataframe and add the method `.to_html(escape=False)` to the dataframe, then it should display the dataframe with viewable images.\n",
      "HTML(missy_df.to_html(escape=False))\n",
      "## Your Turn! \n",
      "Replace \"Jorja Smith\" with any artist/musician of your choosing and run the following cells.\n",
      "search_term = \"Jorja Smith\"\n",
      "genius_search_url = f\"http://api.genius.com/search?q={search_term}&access_token={client_access_token}\"\n",
      "response = requests.get(genius_search_url)\n",
      "json_data = response.json()\n",
      "songs = []\n",
      "for song in json_data['response']['hits']:\n",
      "    songs.append((song['result']['full_title'], song['result']['stats']['pageviews'], song['result']['song_art_image_url']))\n",
      "    \n",
      "artist_df = pd.DataFrame(songs)\n",
      "artist_df.columns = ['song_title', 'page_views', 'album_cover_url']\n",
      "\n",
      "artist_df['album_cover'] = artist_df['album_cover_url'].apply(get_image_html)\n",
      "HTML(artist_df.to_html(escape=False))\n",
      "## LyricsGenius\n",
      "It's relatively common for software developers and others to create special Python packages that help people work with a particular API—even if they don't work for the company or project that designed the API. Lucky for us, this is the case with the Genius API.\n",
      "\n",
      "A data scientist named John Miller wrote a Python package called [LyricsGenius,](https://github.com/johnwmillr/LyricsGenius) which you can see on GitHub below. \n",
      "<a href=\"https://github.com/johnwmillr/LyricsGenius\", border=2><img src=\"../images/LyricsGenius-Git1.png\" width=100%, border=2></a, border=2>\n",
      "LyricsGenius makes working with the Genius API easier, but it also adds some functionality that is not offered by the Genius API. Remember when I said that companies typically don't offer access to their most lucrative data? Well, the Genius API doesn't offer you a way to get access to song lyrics. That's the bread and butter of the whole website!\n",
      "\n",
      "To solve this pesky problem, LyricsGenius combines the Genius API with the web scraping library BeautifulSoup (which we are now familiar with!) in order to get and save song lyrics.\n",
      "<img src=\"../images/LyricsGenius-Git2.png\" width=100%, border=2>\n",
      "## Install the Package\n",
      "To install LyricsGenius (and get the most updated version from GitHub), run:\n",
      "!pip install git+https://github.com/johnwmillr/LyricsGenius.git\n",
      "Copy and paste your Genius \"Client Access Token\" into the quotation marks below, and run the cell to save your variable :\n",
      "client_access_token = \"INSERT YOUR CLIENT ACCESS TOKEN\"\n",
      "To import and set up LyricsGenius, run:\n",
      "import lyricsgenius\n",
      "LyricsGenius = lyricsgenius.Genius(client_access_token)\n",
      "## Get Songs and Lyrics By a Specific Artist\n",
      "To get the top songs and song lyrics from a specific artist you can use the method `.search_artist()`:\n",
      "artist = LyricsGenius.search_artist(\"Missy Elliott\", max_songs=6)\n",
      "To access the song titles, you can run `artist.songs`:\n",
      "artist.songs\n",
      "Inside each of those songs, LyricsGenius has already saved the song lyrics. You can access these lyrics by looping through `artist.songs` and pulling out `song.lyrics`:\n",
      "for song in artist.songs:\n",
      "    print(song.lyrics)\n",
      "## Get Specific Song and Lyrics By a Specific Artist\n",
      "To get the song lyrics from a specific artist, you can use the method `.search_song()`\n",
      "song = LyricsGenius.search_song(\"Missy Elliott\", \"Work It\")\n",
      "song.lyrics\n",
      "### Save Lyrics to .txt File\n",
      "song.save_lyrics(extension='txt')\n",
      "## Get Songs and Lyrics For a Specific Album\n",
      "As you can see, LyricsGenius is an extremely useful Python package! But one thing that we can't do with LyricsGenius is get all the song lyrics for a particular album.\n",
      "\n",
      "So we're going to use the web scraping functions that we wrote in the last lesson to get all the song titles for a specific album, then use LyricsGenius to get the lyrics for each of those songs, and then save them all as text files in a directory.\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "import lyricsgenius\n",
      "import requests\n",
      "from pathlib import Path\n",
      "### Make RegEx Function To Clean Up Songs\n",
      "def clean_up(song_title):\n",
      "\n",
      "    if \"Ft\" in song_title:\n",
      "        before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "        song_title_before_ft = before_ft_pattern.search(song_title).group(0)\n",
      "        clean_song_title = song_title_before_ft.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    else:\n",
      "        song_title_no_lyrics = song_title.replace(\"Lyrics\", \"\")\n",
      "        clean_song_title = song_title_no_lyrics.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    return clean_song_title\n",
      "### Make Function To Scrape Song Titles For Album\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    \n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    \n",
      "    response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_string = response.text\n",
      "    document = BeautifulSoup(html_string, \"html.parser\")\n",
      "    song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [song_title.text for song_title in song_title_tags]\n",
      "    \n",
      "    clean_songs = []\n",
      "    for song_title in song_titles:\n",
      "        clean_song = clean_up(song_title)\n",
      "        clean_songs.append(clean_song)\n",
      "        \n",
      "    return clean_songs\n",
      "### Make Function To Download Lyrics For All Songs in Album\n",
      "client_access_token = \"YOUR CLIENT ACCESS TOKEN\"\n",
      "def download_album_lyrics(artist, album_name): \n",
      "    \n",
      "    # Set up LyricsGenius with your Genius API client access token\n",
      "    #client_access_token = Your-Client-Access-Token\n",
      "    LyricsGenius = lyricsgenius.Genius(client_access_token)\n",
      "    LyricsGenius.remove_section_headers = True\n",
      "    \n",
      "    # With the function that we previously created, go to Genius.com and get all song titles for a particular artist's album\n",
      "    clean_songs = get_all_songs_from_album(artist, album_name)\n",
      "    \n",
      "    for song in clean_songs:\n",
      "        \n",
      "        #For each song in the list, search for that song with LyricsGenius\n",
      "        song_object = LyricsGenius.search_song(song, artist)\n",
      "        \n",
      "        #If the song is not empty\n",
      "        if song_object != None:\n",
      "            \n",
      "            #Do some cleaning and prep for the filename of the song\n",
      "            artist_title = artist.replace(\" \", \"-\")\n",
      "            album_title = album_name.replace(\" \", \"-\")\n",
      "            song_title = song.replace(\"/\", \"-\")\n",
      "            song_title = song.replace(\" \", \"-\")\n",
      "            \n",
      "            #Establish the filename for each song inside a directory that begins with the artist's name and album title\n",
      "            custom_filename=f\"{artist_title}_{album_title}/{song_title}\"\n",
      "            \n",
      "            #A line of code that we need to create a directory\n",
      "             #os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
      "            Path(f\"{artist_title}_{album_title}\").mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            #Save the lyrics for the song as a text file\n",
      "            song_object.save_lyrics(filename=custom_filename, extension='txt', sanitize=False)\n",
      "        \n",
      "        #If the song doesn't contain lyrics\n",
      "        else:\n",
      "            print('No lyrics')\n",
      "download_album_lyrics(\"Missy Elliott\", \"Under Construction\")\n",
      "## Advanced Twarc Start and Stop Instructions\n",
      "## How To Keep Twarc Running\n",
      "`nohup twarc filter \"search term\" > search_term.jsonl &`\n",
      "## How To Stop Twarc From Running\n",
      "Find the process ID and kill the process by process ID\n",
      " Mac/Chrome OS\n",
      "!ps aux | grep twarc\n",
      "!kill 80884\n",
      "Kill the process by name\n",
      " Mac/Chrome OS\n",
      "!pkill -f \"twarc\"\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!Stop-Process -Name \"twarc\"\n",
      "## How To Keep Twarc Running in the Cloud\n",
      "\n",
      "## Google Compute\n",
      "## Create a Google Cloud Platform Project\n",
      "\n",
      "## Twarc Cloud\n",
      "!git clone https://github.com/justinlittman/twarc-cloud.git\n",
      "Or download it and unzip it.\n",
      "## Install Terraform\n",
      "Mac OS/Chrome\n",
      "Install with Homebrew\n",
      "If you don't have Homebrew, you can install with:\n",
      "!/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n",
      "!brew install terraform\n",
      "Windows\n",
      "Install with [Chocolatey](https://chocolatey.org/install)\n",
      "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n",
      "choco install terraform\n",
      "## Install Twarc-Cloud Requirements\n",
      "%cd ~/twarc-cloud\n",
      "%ls\n",
      "!pip install -r requirements.txt\n",
      "## Configure Terraform To Work With Your AWS Account\n",
      "### Example\n",
      "# Available from https://console.aws.amazon.com/iam/home?#security_credential\n",
      "access_key = \"AKODFFDsZJAFHXRJTMXZA\"\n",
      "secret_key = \"kA+cX4nZ/Ot5FFzxFPXv9W7qnrOgaf3W0YJeR\"\n",
      "region = \"us-east-1\"\n",
      "# This has to be globally unique, so pick appropriately.\n",
      "bucket_name = \"twarc-cloud\"\n",
      "access_key = \"AKIAJK23Q2R4KYBES2SQ\"\n",
      "secret_key = \"UKx+11+Qn/HEkdJCJmIU0uBWH+BxkKlgrOHWhZiC\"\n",
      "region = \"us-east-2\"\n",
      "bucket_name = \"my-twarc-cloud\"\n",
      "access_key = \"YOUR-ACCESS-KEY\"\n",
      "secret_key = \"YOUR-SECRET-kEY\"\n",
      "region = \"YOUR-REGION\n",
      "bucket_name = \"YOUR-BUCKET-NAME\"\n",
      "Then run the two cells below:\n",
      "aws_configuration = f\"\"\"\n",
      "access_key = \"{access_key}\"\n",
      "secret_key = \"{secret_key}\"\n",
      "region = \"{region}\"\n",
      "# This has to be globally unique, so pick appropriately.\n",
      "bucket_name = \"{bucket_name}\"\n",
      "\"\"\"\n",
      "import os\n",
      "\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \"twarc-cloud/terraform/terraform.tfvars\")\n",
      "\n",
      "with open(config_filename, \"w\") as file_object:\n",
      "    file_object.write(aws_configuration)\n",
      "%cd ~/twarc-cloud/terraform/\n",
      "!terraform init\n",
      "!echo \"yes\" | terraform apply | tail -16\n",
      "## Configure Twarc Cloud\n",
      "access_key = \"AKIA5KHG6BTSOY6EM4F3\"\n",
      "secret_key = \"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\"\n",
      "#bucket_name\n",
      "bucket = \"my-twarc-cloud\"\n",
      "\n",
      "#the quoted string inside \"public_subnets\"\n",
      "subnet = \"subnet-08d6f8d559dbd2a30\"\n",
      "\n",
      "security_group_id = \"sg-0d0acf71eadc146d3\"\n",
      "\n",
      "task_role_arn = \"arn:aws:iam::915312348388:role/twarc-cloud-ecs-task-role\"\n",
      "\n",
      "#exectution_role_arn\n",
      "task_execution_role_arn = \"arn:aws:iam::915312348388:role/twarc-cloud-ecs-task-execution-role\"\n",
      "\n",
      "event_role_arn = \"arn:aws:iam::915312348388:role/twarc-cloud-event-role\"\n",
      "\n",
      "cluster = \"twarc-cloud-cluster\"\n",
      "access_key = \"YOUR-ACCESS-KEY\"\n",
      "secret_key = \"YOUR-SECRET-kEY\"\n",
      "\n",
      "region = \"YOUR-REGION\n",
      "bucket_name = \"YOUR-BUCKET-NAME\"\n",
      "twarc_cloud_configuration = f\"\"\"[DEFAULT]\n",
      "access_key = {access_key}\n",
      "secret_key = {secret_key}\n",
      "bucket = {bucket}\n",
      "subnet = {subnet}\n",
      "security_group = {security_group_id}\n",
      "task_role_arn = {task_role_arn}\n",
      "task_execution_role_arn = {task_execution_role_arn}\n",
      "event_role_arn = {event_role_arn}\n",
      "cluster = {cluster}\n",
      "log_group = twarc-cloud-container\n",
      "image_tag = version-1\n",
      "# Optional: Provides notification of errors.\n",
      "# honeybadger_key = afjk34sf3\n",
      "region = us-east-2\n",
      "\"\"\"\n",
      "import os\n",
      "\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \"twarc-cloud/twarc_cloud.ini\")\n",
      "\n",
      "with open(config_filename, \"w\") as file_object:\n",
      "    file_object.write(twarc_cloud_configuration)\n",
      "## Test Twarc Cloud\n",
      "%cd ~/twarc-cloud\n",
      "!python twarc_cloud.py\n",
      "## Run A Live Tweet Collection with Twarc Cloud\n",
      "### Collect User Timelines\n",
      "!python3 twarc_cloud.py collection-config template user_timeline --id=test_collection\n",
      "!python3 twarc_cloud.py collection-config keys\n",
      "!python3 twarc_cloud.py collection add\n",
      "!python3 twarc_cloud.py collection-config screennames @KingJames @realDonaldTrump @sarahkendzior\n",
      "!echo \"Y\" | python3 twarc_cloud.py collection-config update\n",
      "!python3 twarc_cloud.py collection schedule test_collection \"rate(7 days)\"\n",
      "!python3 twarc_cloud.py collection download test_collection\n",
      "!python twarc_cloud.py collection stop test_collection\n",
      "### Collect Tweets in Real Time\n",
      "!python3 twarc_cloud.py collection-config template filter --id=filter_collection\n",
      "!python3 twarc_cloud.py collection-config keys\n",
      "!python3 twarc_cloud.py collection add\n",
      "!echo \"Y\" | python3 twarc_cloud.py collection-config update\n",
      "!python twarc_cloud.py collection filter-start filter_collection\n",
      "!python twarc_cloud.py harvest list\n",
      "### Collect Tweets in Search\n",
      "!python3 twarc_cloud.py collection-config template search --id=search_test\n",
      "!python3 twarc_cloud.py collection-config keys\n",
      "!python3 twarc_cloud.py collection add\n",
      "!echo \"Y\" | python3 twarc_cloud.py collection-config update\n",
      "!python3 twarc_cloud.py collection once search_test\n",
      "!python3 twarc_cloud.py collection download search_test\n",
      "!python twarc_cloud.py collection stop test_collection\n",
      "!python3 twarc_cloud.py collection download filter_collection\n",
      "!python3 twarc_cloud.py collection-config template user_timeline --id=test_collection\n",
      "!python3 twarc_cloud.py collection-config keys\n",
      "!python3 twarc_cloud.py collection add\n",
      "!python3 twarc_cloud.py collection-config screennames @KingJames @realDonaldTrump @sarahkendzior\n",
      "!echo \"Y\" | python3 twarc_cloud.py collection-config update\n",
      "!python3 twarc_cloud.py collection schedule test_collection \"rate(7 days)\"\n",
      "!python3 twarc_cloud.py collection download test_collection\n",
      "First you need to set up a \"collection\"\n",
      "!python twarc_cloud.py collection-config template filter --id=search_collection\n",
      "Then add your API keys\n",
      "!python twarc_cloud.py collection-config keys\n",
      "import json\n",
      "with open(\"collection.json\", \"r\") as jsonFile:\n",
      "    data = json.load(jsonFile)\n",
      "data\n",
      "#Type of Search: filter, search, \n",
      "data[\"filter\"][\"track\"]\n",
      "data[\"filter\"][\"track\"] = \"shakespeare\" \n",
      "with open(\"collection.json\", \"w\") as jsonFile:\n",
      "    json.dump(data, jsonFile, indent=4)\n",
      "{\n",
      "  \"id\": \"my_collection\",\n",
      "  \"keys\": {\n",
      "    \"consumer_key\": \"HoZXZP6eW5CW86V9EcEw4oBcP\",\n",
      "    \"consumer_secret\": \"JKvzWsuXWsgDuAFeBW7i5MVLyDZ60AhXwSD0slBTQ70SX8Byga\",\n",
      "    \"access_token\": \"285395514-ZzvKkqW5fJrolpwqyRiNPDtaQ43R517c050APBQ0\",\n",
      "    \"access_token_secret\": \"m6ItBTrTV5SHWwXPSpDujWOpILIARaijHrtixxKWsHRKn\"\n",
      "  },\n",
      "  \"type\": \"filter\",\n",
      "  \"filter\": {\n",
      "    \"track\": \"https://twitter.com/lithub/status/1247582133301981185\",\n",
      "    \"follow\": \"<Comma separated list of user ids>\",\n",
      "    \"max_records\": \"<Optional. Maximum number of records to collect per harvester.\"\n",
      "  },\n",
      "  \"timestamp\": \"2020-04-08T17:18:20.193290\"\n",
      "}\n",
      "\n",
      "Then add the collection\n",
      "!python twarc_cloud.py collection add\n",
      "!python twarc_cloud.py collection\n",
      "!python twarc_cloud.py collection-config\n",
      "!python ~/twarc-cloud/twarc_cloud.py collection list\n",
      "!yes | python twarc_cloud.py collection-config update\n",
      "!python twarc_cloud.py collection filter-start search_collection\n",
      "!python twarc_cloud.py collection filter-stop search_collection\n",
      "!python twarc_cloud.py harvest list\n",
      "!python twarc_cloud.py harvest running my_collection\n",
      "\n",
      "## Collecting Twitter Data\n",
      "[Download relevant files here](https://melaniewalsh.org/Collecting-Twitter-Data.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to collect Twitter data with the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "\n",
      "Because twarc relies on Twitter's API, we need to apply for a Twitter developer account and create a Twitter application before we use it. You can find instructions for the application process and for installing and configuring twarc here: [Twitter Collection Setup](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html).\n",
      "## Twitter API (Free Version)\n",
      "With the free version of the Twitter API, there are basically two ways to collect your own Twitter data—in real time or ~7 days in the past. To get data any further in the past requires a paid version of the Twitter API. Twarc allows you to collect tweets both in real time and ~7 days in the past with `twarc filter` and `twarc search`.\n",
      "\n",
      "Some of our work with twarc in this notebook will take place from the command line (aka through Jupyter command line functions). Remember that the exclamation point `!` at the beginning of a Jupyter cell allows us to access command line functions from a Jupyter notebook. Any cell that begins with a `!` can also be run from your Terminal or PowerShell.\n",
      "## Collect Tweets in Real Time (`twarc filter`)\n",
      "`twarc filter \"search term\" > my_file.jsonl`\n",
      "Run for 10 seconds and then hit stop:\n",
      "`twarc  filter \"coronavirus\" > coronavirus_filter.jsonl`\n",
      "!timeout 10s twarc  filter \"coronavirus\" > coronavirus_filter.jsonl\n",
      "!timeout 5m twarc  filter \"coronavirus\" > coronavirus_filter.jsonl\n",
      "!twarc  filter \"coronavirus\" > coronavirus_filter.jsonl\n",
      "Run for 10 seconds and then hit stop:\n",
      "!timeout 10s twarc filter \"Shakespeare\" > shakespeare_filter.jsonl\n",
      "Run for 10 seconds and then hit stop:\n",
      "!timeout 10s twarc filter \"Bernie\" > bernie_filter.jsonl\n",
      "Stop-Process -Name \"twarc\"\n",
      "UTF 8 probs\n",
      "https://stackoverflow.com/questions/57131654/using-utf-8-encoding-chcp-65001-in-command-prompt-windows-powershell-window/57134096#57134096\n",
      "## Check Number of Tweets Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l coronavirus_filter.jsonl\n",
      "!wc -l shakespeare_filter.jsonl\n",
      "!wc -l bernie_filter.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c “” coronavirus_filter.jsonl\n",
      "!find /v /c “” shakespeare_filter.jsonl\n",
      "!find /v /c “” bernie_filter.jsonl\n",
      "## Collect Tweets From Last 7 days (`twarc search`)\n",
      "Run for 10 seconds and then hit stop:\n",
      "!twarc search \"coronavirus\" > coronavirus_search.jsonl \n",
      "Run for 10 seconds and then hit stop:\n",
      "!twarc search \"Shakespeare\" > shakespeare_search.jsonl \n",
      "Run for 10 seconds and then hit stop:\n",
      "!twarc search \"Bernie\" > bernie_search.jsonl \n",
      "## Check Number of Tweets Collected\n",
      "!wc -l coronavirus_search.jsonl\n",
      "!wc -l shakespeare_search.jsonl\n",
      "!wc -l bernie_search.jsonl\n",
      "## Crafting a Good Twitter Query\n",
      "We made relatively simple queries to Twitter's API in the examples above. They're all single words. But there are also more complex ways to make queries.\n",
      "\n",
      "To craft a good Twitter search query, it's important to understand (and explore!) these myriad ways. A researcher named Igor Brigadir has compiled a wonderful resource that details many of the Twitter API search operators: https://github.com/igorbrigadir/twitter-advanced-search/blob/master/README.md\n",
      "## Search for Exact Phrases\n",
      "`twarc search \"\\\"an exact phrase\\\"\"`\n",
      "You can search for an *exact* phrase in a tweet by including the phrase in escaped `\\` quotation marks, as above.\n",
      "### Not with a bang\n",
      "The first phrase that we're going to search for comes from the conclusion of T.S. Eliot's 1925 [poem \"The Hollow Men\"](https://msu.edu/~jungahre/transmedia/the-hollow-men.html):\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">**Not with a bang but with a whimper.**\n",
      "You've probably heard these lines before, even if you didn't know that they were written by the modernist poet T.S. Eliot. This phrase is a striking example of a bit of literary, poetic language that has gone \"viral\" in 21st-century American culture, both on and off the internet.\n",
      "Run for 20 seconds and then hit stop:\n",
      "!twarc search \"\\\"not with a bang but with a\\\"\" > bang.jsonl\n",
      "## Search for General Phrases\n",
      "### Touch my face\n",
      "The other phrase we're going to search for comes from public health recommendations about preventing the spread of the coronavirus: that people should avoid touching their faces. Many people are, in light of these recommendations, discovering that it's actually very difficult not to touch your own face.\n",
      "\n",
      "Now the avoidance of touching one's face has sprouted up as a funny Twitter meme. These various \"touch my face\" memes serves as an interesting example of how online communities produce comedy and levity even in times of stress and crisis.\n",
      "%%html\n",
      "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Working on not touching my face :) <a href=\"https://t.co/qfyNdrDReh\">pic.twitter.com/qfyNdrDReh</a></p>&mdash; Hannah (@McBBQSauce) <a href=\"https://twitter.com/McBBQSauce/status/1235700933801242626?ref_src=twsrc%5Etfw\">March 5, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
      "Run for 20 seconds and then hit stop:\n",
      "!twarc search \"touch my face min_retweets:10\" > face.jsonl \n",
      "  \n",
      "Great! Now we have some Twitter data. But before we dive into analysis, we need to complete one more step. We need to convert this JSON data to CSV data, which is easier to work with. Luckily, there's a twarc \"utility\" for that very purpose.\n",
      "## Get Twarc Utilities\n",
      "There are a number of twarc \"utilities\" that enable you to manipulate and analyze Twitter data. With them, you can convert JSON data to CSV data, count up the most frequent emojis, make a network visualization, and more.\n",
      "\n",
      "These utilities are not available from the `pip install twarc` installation. To access the twarc utilities, you'll need to `git clone` the [twarc GitHub repository](https://github.com/DocNow/twarc) or download it as a zip file.\n",
      "\n",
      "The twarc repository should already be downloaded in your relevant files, but if you uncomment the line below, you can also clone the repository with this line of code.\n",
      "#!git clone https://github.com/DocNow/twarc.git\n",
      "## Use Twarc Utilities\n",
      "`python twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "To use a twarc utility, you need to call `python` from the command line and then include the utility's file path (which should be in the \"twarc/utils\" subfolder). Note that if your Jupyter notebook is in exactly the same directory as the \"twarc\" repository, then you can run the code as above. However, if your notebook is somewhere else, you will have to direct it to the correct location of \"twarc/utils\". For example`python /Users/melaniewalsh/twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "## Convert JSON to CSV\n",
      "`python twarc/utils/json2csv.py json_file.jsonl > csv_file.csv`\n",
      "Make \"bang.jsonl\" into \"bang.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text bang.jsonl > bang.csv\n",
      "Make \"face.jsonl\" into \"face.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text face.jsonl > face.csv\n",
      "  \n",
      "Read in tweet CSV files with Pandas\n",
      "import pandas\n",
      "Set Pandas display options so columns are wider and more columns are visible\n",
      "pandas.set_option('max_colwidth', 5000)\n",
      "pandas.set_option('max_columns', 40)\n",
      "pandas.set_option('max_rows', 100)\n",
      "bang_df = pandas.read_csv('bang.csv')\n",
      "face_df = pandas.read_csv('face.csv')\n",
      "Check what Twitter metadata exists in this CSV file\n",
      "bang_df.columns\n",
      "As you can see above, there is a *lot* of metadata that comes with every tweet!\n",
      "Check the size of dataframe (number of rows = number of tweets)\n",
      "bang_df.shape\n",
      "face_df.shape\n",
      "Preview dataframes\n",
      "bang_df.head()\n",
      "face_df.head()\n",
      "  \n",
      "  \n",
      "## Filter Twitter Data to Only Categories of Interest\n",
      "bang_df[['created_at', 'tweet_type', 'media', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "bang_df = bang_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "face_df = face_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "## Display Links and Images in Twitter Data\n",
      "To display links and images in our Twitter dataframe, run the cells below. We're converting the image URL into an HTML image tag and then displaying our dataframe as an HTML object.\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "bang_df['media'] = bang_df['media'].fillna(\"No Image\")\n",
      "bang_df['media']= bang_df['media'].apply(get_image_html)\n",
      "\n",
      "face_df['media'] = face_df['media'].fillna(\"No Image\")\n",
      "face_df['media']= face_df['media'].apply(get_image_html)\n",
      "### Not With a Bang\n",
      "HTML(bang_df.to_html(render_links=True, escape=False))\n",
      "### Touch My Face\n",
      "HTML(face_df.to_html(render_links=True, escape=False))\n",
      "HTML(face_df[['text', 'media', 'retweet_count']].to_html(render_links=True, escape=False))\n",
      "## Sort By Top Retweets\n",
      "bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "bang_rt_sorted = bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(bang_rt_sorted.to_html(render_links=True, escape=False))\n",
      "face_rt_sorted = face_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(face_rt_sorted.to_html(render_links=True, escape=False))\n",
      "## Identify Top Hashtags\n",
      "`twarc/utils/tags.py tweets.jsonl`\n",
      "!python twarc/utils/tags.py bang.jsonl\n",
      "!python twarc/utils/tags.py face.jsonl\n",
      "## Create a Word Cloud\n",
      "`twarc/utils/wordcloud.py tweets.jsonl`\n",
      "!python twarc/utils/wordcloud.py bang.jsonl > not_with_a_bang.html\n",
      "[not_with_a_bang.html](not_with_a_bang.html)\n",
      "from IPython.display import IFrame\n",
      "IFrame(src=\"not_with_a_bang.html\", width=800, height=800)\n",
      "!python twarc/utils/wordcloud.py face.jsonl > touch_my_face.html\n",
      "from IPython.display import IFrame\n",
      "IFrame(src=\"touch_my_face.html\", width=800, height=800)\n",
      "## Count Emojis `twarc/utils/emojis.py`\n",
      "!python twarc/utils/emojis.py bang.jsonl | head -n 20\n",
      "!python twarc/utils/emojis.py face.jsonl | head -n 20\n",
      "## Your Turn!\n",
      "Now choose your own Twitter search term or query.\n",
      "## Collect Tweets From Last 7 Days\n",
      "!twarc search \"your search query\" > your_search.jsonl \n",
      "## Count How Many Tweets You Collected\n",
      "!wc -l your_search.jsonl\n",
      "## Convert Your JSON data to CSV data\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text  your_search.jsonl > your_search.csv\n",
      "## Read in as Pandas dataframe\n",
      "your_df = pandas.read_csv('your_search.csv')\n",
      "## Add Metadata\n",
      "Filter your dataframe and add at least one new metadata column that we haven't explored yet.\n",
      "your_df.columns\n",
      "When you run the cell below, right-click to \"Enable Scrolling for Outputs\" and scroll through to see what the new metadata category looks like. Discuss this category with your group and how you might use it for a Twitter analysis.\n",
      "your_df[['created_at', 'tweet_type', 'media', 'tweet_url', '#YOUR NEW METADATA HERE', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "Now save your filtered dataframe as `filtered_df`\n",
      "#Your Code Here\n",
      "## Explore Data with Links and Images\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "filtered_df['media'] = filtered_df['media'].fillna(\"No Image\")\n",
      "filtered_df['media']= filtered_df['media'].apply(get_image_html)\n",
      "HTML(filtered_df.to_html(render_links=True, escape=False))\n",
      "## Sort Your Twitter Data by Top Retweets\n",
      "#Your code here\n",
      "What is the most retweeted tweet in your dataset?\n",
      "\n",
      "## Count Most Frequent Emojis\n",
      "!python twarc/utils/emojis.py your_search.jsonl | head -n 20\n",
      "## Twitter Data Analysis\n",
      "[Download relevant files here](https://melaniewalsh.org/Collecting-Twitter-Data-v2.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to analyze and explore Twitter data with Pandas and the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "\n",
      "This lesson presumes that you've already installed and configured twarc and collected some Twitter data (covered in the previous lesson).\n",
      "## Read in Tweet CSV files with Pandas\n",
      "import pandas as pd\n",
      "Set Pandas display options so columns are wider and more columns are visible\n",
      "pd.set_option('max_colwidth', 5000)\n",
      "pd.set_option('max_columns', 40)\n",
      "pd.set_option('max_rows', 100)\n",
      "bang_df = pd.read_csv('bang.csv')\n",
      "face_df = pd.read_csv('face.csv')\n",
      "Check what Twitter metadata exists in this CSV file\n",
      "bang_df.columns\n",
      "As you can see above, there is a *lot* of metadata that comes with every tweet!\n",
      "Check the size of dataframe (number of rows = number of tweets)\n",
      "bang_df.shape\n",
      "face_df.shape\n",
      "Preview dataframes\n",
      "bang_df.head()\n",
      "face_df.head()\n",
      "  \n",
      "  \n",
      "## Filter Twitter Data to Only Categories of Interest\n",
      "bang_df[['created_at', 'tweet_type', 'media', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "bang_df = bang_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "face_df = face_df[['created_at', 'tweet_type', 'media', 'tweet_url', 'text', 'retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "## Display Links and Images in Twitter Data\n",
      "To display links and images in our Twitter dataframe, run the cells below. We're converting the image URL into an HTML image tag and then displaying our dataframe as an HTML object.\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "bang_df['media'] = bang_df['media'].fillna(\"No Image\")\n",
      "bang_df['media']= bang_df['media'].apply(get_image_html)\n",
      "\n",
      "face_df['media'] = face_df['media'].fillna(\"No Image\")\n",
      "face_df['media']= face_df['media'].apply(get_image_html)\n",
      "**Not With a Bang**\n",
      "HTML(bang_df.to_html(render_links=True, escape=False))\n",
      "**Touch My Face**\n",
      "HTML(face_df.to_html(render_links=True, escape=False))\n",
      "Filter to just text, images, and retweet count\n",
      "HTML(face_df[['media', 'text', 'retweet_count']].to_html(render_links=True, escape=False))\n",
      "## Sort By Top Retweets\n",
      "**Not With a Bang**\n",
      "bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "bang_rt_sorted = bang_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(bang_rt_sorted.to_html(render_links=True, escape=False))\n",
      "**Touch My Face**\n",
      "face_rt_sorted = face_df.sort_values(by='retweet_count', ascending=False)\n",
      "HTML(face_rt_sorted.to_html(render_links=True, escape=False))\n",
      "## Identify Top Hashtags\n",
      "`twarc/utils/tags.py tweets.jsonl`\n",
      ", border=2> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Heads up Windows users! Remember that twarc utilities may not work on your computer by default. If you get a UnicodeEncodeError, it's because Windows computers do not use Unicode (UTF-8) by default. However, you can make UTF-8 your default by following [these instructions](https://scholarslab.github.io/learn-twarc/08-win-region-settings) and restarting your comptuer. Then twarc utilities should work.\n",
      "!python twarc/utils/tags.py face.jsonl\n",
      "## Create a Word Cloud\n",
      "`twarc/utils/wordcloud.py tweets.jsonl`\n",
      "!python twarc/utils/wordcloud.py bang.jsonl > not_with_a_bang.html\n",
      "[not_with_a_bang.html](not_with_a_bang.html)\n",
      "%%html\n",
      "<iframe src=\"not_with_a_bang.html\" width=800, height=800></iframe>\n",
      "!python twarc/utils/wordcloud.py face.jsonl > touch_my_face.html\n",
      "%%html\n",
      "<iframe src=\"touch_my_face.html\", width=800, height=800></iframe>\n",
      "## Count Emojis\n",
      "`python twarc/utils/emojis.py tweets.jsonl --number 10`\n",
      "!pip install emoji\n",
      "!python twarc/utils/emojis.py face.jsonl --number 10\n",
      "## Your Turn!\n",
      "Now choose your own Twitter search term or query.\n",
      "## Collect Tweets From Last 7 Days\n",
      "!twarc search \"your search query\" > your_search.jsonl \n",
      "## Count How Many Tweets You Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l your_search.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" your_search.jsonl\n",
      "## Convert Your JSON data to CSV data\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text  your_search.jsonl > your_search.csv\n",
      "## Read in as Pandas dataframe\n",
      "import pandas as pd\n",
      "your_df = pd.read_csv('your_search.csv')\n",
      "## Add Metadata\n",
      "Filter your dataframe and add at least one new metadata column that we haven't explored yet.\n",
      "your_df.columns\n",
      "When you run the cell below, right-click to \"Enable Scrolling for Outputs\" and scroll through to see what the new metadata category looks like. Discuss this category with your group and how you might use it for a Twitter analysis.\n",
      "your_df[['created_at', 'tweet_type', '#YOUR NEW METADATA HERE','media', 'tweet_url', 'text', 'rt_text','retweet_count',  'urls', 'user_name', 'user_location', 'hashtags', ]].head(100)\n",
      "Now save your filtered dataframe as `filtered_df`\n",
      "#Your Code Here\n",
      "## Explore Data with Links and Images\n",
      "from IPython.core.display import HTML\n",
      "def get_image_html(link):\n",
      "    if link != \"No Image\":\n",
      "        image_html = f\"<a href= '{link}'>'<img src='{link}' width='500px'></a>                            \"\n",
      "    else:\n",
      "        image_html = \"No Image\"\n",
      "    return image_html\n",
      "filtered_df['media'] = filtered_df['media'].fillna(\"No Image\")\n",
      "filtered_df['media']= filtered_df['media'].apply(get_image_html)\n",
      "HTML(filtered_df.to_html(render_links=True, escape=False))\n",
      "## Sort Your Twitter Data by Top Retweets\n",
      "#Your code here\n",
      "What is the most retweeted tweet in your dataset?\n",
      "**#**Your Answer Here\n",
      "## Count Most Frequent Emojis\n",
      "!python twarc/utils/emojis.py your_search.jsonl --number 10\n",
      "## Getting Cultural Data\n",
      "\n",
      "## Get All Song Lyrics From Album\n",
      "!pip install git+https://github.com/johnwmillr/LyricsGenius.git\n",
      "import lyricsgenius\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from pathlib import Path\n",
      "client_access_token = \"YOUR CLIENT ACCESS TOKEN\"\n",
      "def clean_up(song_title):\n",
      "\n",
      "    if \"Ft\" in song_title:\n",
      "        before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "        song_title_before_ft = before_ft_pattern.search(song_title).group(0)\n",
      "        clean_song_title = song_title_before_ft.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    else:\n",
      "        song_title_no_lyrics = song_title.replace(\"Lyrics\", \"\")\n",
      "        clean_song_title = song_title_no_lyrics.strip()\n",
      "        clean_song_title = clean_song_title.replace(\"/\", \"-\")\n",
      "    \n",
      "    return clean_song_title\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    \n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    \n",
      "    response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_string = response.text\n",
      "    document = BeautifulSoup(html_string, \"html.parser\")\n",
      "    song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [song_title.text for song_title in song_title_tags]\n",
      "    \n",
      "    clean_songs = []\n",
      "    for song_title in song_titles:\n",
      "        clean_song = clean_up(song_title)\n",
      "        clean_songs.append(clean_song)\n",
      "        \n",
      "    return clean_songs\n",
      "def download_album_lyrics(artist, album_name): \n",
      "    \n",
      "    # Set up LyricsGenius with your Genius API client access token\n",
      "    #client_access_token = Your-Client-Access-Token\n",
      "    LyricsGenius = lyricsgenius.Genius(client_access_token)\n",
      "    LyricsGenius.remove_section_headers = True\n",
      "    \n",
      "    # With the function that we previously created, go to Genius.com and get all song titles for a particular artist's album\n",
      "    clean_songs = get_all_songs_from_album(artist, album_name)\n",
      "    \n",
      "    for song in clean_songs:\n",
      "        \n",
      "        #For each song in the list, search for that song with LyricsGenius\n",
      "        song_object = LyricsGenius.search_song(song, artist)\n",
      "        \n",
      "        #If the song is not empty\n",
      "        if song_object != None:\n",
      "            \n",
      "            #Do some cleaning and prep for the filename of the song\n",
      "            artist_title = artist.replace(\" \", \"-\")\n",
      "            album_title = album_name.replace(\" \", \"-\")\n",
      "            song_title = song.replace(\"/\", \"-\")\n",
      "            song_title = song.replace(\" \", \"-\")\n",
      "            \n",
      "            #Establish the filename for each song inside a directory that begins with the artist's name and album title\n",
      "            custom_filename=f\"{artist_title}_{album_title}/{song_title}\"\n",
      "            \n",
      "            #A line of code that we need to create a directory\n",
      "             #os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
      "            Path(f\"{artist_title}_{album_title}\").mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            #Save the lyrics for the song as a text file\n",
      "            song_object.save_lyrics(filename=custom_filename, extension='txt', sanitize=False)\n",
      "        \n",
      "        #If the song doesn't contain lyrics\n",
      "        else:\n",
      "            print('No lyrics')\n",
      "download_album_lyrics(\"Missy Elliott\", \"Under Construction\")\n",
      "## Web Scraping Plus Regular Expressions\n",
      "[Download relevant files here](https://melaniewalsh.org/Web-Scraping-Plus-Regex.zip)\n",
      "In this lesson, we're going to build a web scraping function called `get_all_songs_from_album` that will accept any artist name and album title (as long as they appear on Genius.com) and return a list of songs from that artist's album. In the next lesson, we're going to use these lists of songs to scrape lyrics for entire albums.\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "We're again going to use the `requests` library and the `BeautifulSoup` library to scrape our list of album songs. The first album that we're going to scrape is Missy Elliott's \"Under Construction\" (2002), which debuted at No. 3 on The Billboard Top 200 charts.\n",
      "<img src=\"../images/Missy-Under-Construction.png\" width=100%, border=2>\n",
      "response = requests.get(\"https://genius.com/albums/Missy-elliott/Under-construction\")\n",
      "html_string = response.text\n",
      "document = BeautifulSoup(html_string, \"html.parser\")\n",
      "### Your Turn!\n",
      "We want to extract just the song titles from Missy Elliott's album \"Under Construction.\" Turn on your web browser's \"Inspect\" function and find the HTML tag associated with each song title.\n",
      "song_title_tags = document.find_all(\"h3\")\n",
      "song_title_tags\n",
      "Now write a `for` loop that extracts the text from each song title tag and `.appends()` it to a list called `song_titles`:\n",
      "song_titles = []\n",
      "for song in song_title_tags:\n",
      "    song_title_missy = song.text\n",
      "    song_titles.append(song_title_missy)\n",
      "song_titles\n",
      "\n",
      "Now transform that same `for` loop into a list comprehension.\n",
      "missy_song_titles = [song.text for song in song_title_tags]\n",
      "missy_song_titles\n",
      "Are there things in your list that aren't song titles??? If so, use `.find_all()` with more specific HTML attributes `attrs={}`\n",
      "song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "missy_song_titles = [#Your Code Here]\n",
      "missy_song_titles\n",
      " \n",
      "## Regular Expressions + String Methods\n",
      "Great! Now we have have our list of song titles from Missy Elliot's album \"Under Construction.\" But if you notice, these song titles are pretty messy, and we need to clean them up.\n",
      "\n",
      "To do so, we're going to use built-in string methods and a Python library called `re`, short for regular expressions. Regular expressions are basically like a very sophisticated find-and-replace. Regular expression are not exclusive to Python and are used in many programming languages as well as in search engines, text editors, and word processors.\n",
      "import re\n",
      "To practice with regular expressions, we're going to use a sample messy song title from our messy song titles list.\n",
      "sample_song = \"\\n              Back in the Day (Ft.\\xa0JAY-Z)\\n              Lyrics\\n\"\n",
      "Remember the string method `.replace()`? With this built-in string method, we can easily get rid of the new line characters `\\n` or the word \"Lyrics\" from our `sample_song`, which is very useful.\n",
      "sample_song.replace(\"\\n\", \"\")\n",
      "sample_song.replace(\"Lyrics\", \"\")\n",
      "## `re.sub()`\n",
      "However, with regular expressions, we can replace strings with even more power and flexibility.\n",
      "To replace a string with regular expressions, we use `re.sub(old_pattern, new_pattern, text_that_contains_pattern)`. We can do exactly the same thing that we did with the built-in string method `.replace()`.\n",
      "sample_song\n",
      "re.sub(\"\\n\", \"\", sample_song)\n",
      "## Special RegEx Characters\n",
      "But regular expressions have certain characters with special pattern-matching powers, which is what allows us to do more cleaning, manipulating, and searching than with basic string methods. Below are some of the special regular expression characters.\n",
      "\n",
      "| Regular Expression Pattern       | Matches |\n",
      "|:---------------------------:|:-----------------------------------------------------------------------------------------------------------:|\n",
      "| `.` | any character                                         | \n",
      "| `\\w` | word                                         | \n",
      "| `\\W`                      | NOT word                                           |  \n",
      "| `\\d` | digit                                         | \n",
      "| `\\D`                      | NOT digit                                           | \n",
      "| `\\s` | whitespace                                         | \n",
      "| `\\S`                      | NOT whitespace                                          | \n",
      "| `[abc]`                      | Any of abc                                         |\n",
      "| `[^abc]`                      | Not any of abc                                         | \n",
      "| `(abc)`                      | Specific capture of \"abc\"                                         \n",
      "| `+`                      | 1 or more instances                                       | \n",
      "| `*`                      | 0 or more instances                                         | \n",
      "| `?`                      | 0 or 1 instance                                        | \n",
      "                   \n",
      "\n",
      "You can explore and experiment with regular expression characters and combinations at [Regexr.com](https://regexr.com/4vhf1).\n",
      "We can replace anything that is not a word `\\W` with \" \":\n",
      "re.sub(\"\\W\", \" \", sample_song)\n",
      "Replace anything that is a word `\\w` with \" \":\n",
      "re.sub(\"\\w\", \" \", sample_song)\n",
      "The character `+` means \"match one or more instance\" of the pattern, which allows us to remove multiple not word patterns in a row.\n",
      "re.sub(\"\\W+\", \" \", sample_song)\n",
      "## `re.compile()`\n",
      "An efficient way to build and save a regular expression pattern is with `re.compile()`\n",
      "not_word_pattern = re.compile(\"\\W+\")\n",
      "re.sub(not_word_pattern, \" \", sample_song)\n",
      "## `re.search()`\n",
      "In addition to replacing text, we can also find and return text. With `re.search()`, we can find and return any particular pattern. The `re.search()` function returns something called a \"match object,\" which we can access with `.group()`.\n",
      "For example, searching with the pattern `\\w+` will return the very first word in `sample_song`:\n",
      "word_pattern = re.compile(\"\\w+\")\n",
      "word_pattern.search(sample_song)\n",
      "word_pattern.search(sample_song).group(0)\n",
      "## `re.findall()`\n",
      "The function `re.findall()` will return a list of every instance of a particular pattern.\n",
      "word_pattern = re.compile(\"\\w+\")\n",
      "word_pattern.findall(sample_song)\n",
      "When you combine special regular expression characters, you can make your pattern matching very specific and very powerful. If we had a document that contains a bunch of email addresses, we could use the pattern `[\\w.]+@[\\w.]+` to find and extract the words that appear on other side of the `@` character, aka [find and extract all the email addresses](https://regexr.com/4vhfa).\n",
      "text_with_emails = \"The important email addresses are important@cool.com, signficant@sweet.org\"\n",
      "extracted_emails = re.findall('[\\w.]+@[\\w.]+', text_with_emails)\n",
      "extracted_emails\n",
      "## Match Before a Certain String\n",
      "For our song titles, we might want to [extract everything that comes before \"(Ft.)\"](regexr.com/4vhfg) because we don't care as much about the featured artists, and because the featured artists makes the song titles really long. To match everything that comes before a certain string, we can use the pattern `.*(?=desired_pattern)` which matches 0 or more `*` of any character `.` that comes before `(?=)` the string \"desired_pattern.\"\n",
      "before_ft_pattern = re.compile(\".*(?=Ft)\")\n",
      "before_ft_pattern.search(sample_song).group(0)\n",
      "## Backslash Escape Characters\n",
      "Nice! We got everything before the featured artist. Well, almost. We still have a weird, lingering open parentheses. That's because we were matching \"Ft\" not \"(Ft\". Let's match everything before \"(Ft\" instead.\n",
      "\n",
      "To do so, we're going to have to make a slight adjustment. Remember that parentheses `()` are special regular expression characters. To make clear that we mean a literal parentheses and not a special regular expression character, we have to use an escape backslash `\\` before the character. \n",
      "before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "clean_sample_song_title = before_ft_pattern.search(sample_song).group(0)\n",
      "clean_sample_song_title \n",
      "## Strip Leading and Trailing Whitespace\n",
      "The last thing we'll do to clean up our song title is to use the built-in string method `.strip()` which strips leading and trailing whitespace.\n",
      "clean_sample_song_title.strip()\n",
      "## Build Functions and Put It All Together\n",
      "Let's put all this cleaning together in a function called `clean_up`. It will match and strip everything before \"(Ft.)\" if the song title contains a featured artist, and it will remove the word \"Lyrics\" and strip whitespace if the song title does not contain \"(Ft.)\".\n",
      "## `clean_up()`\n",
      "def clean_up(song_title):\n",
      "\n",
      "    if \"Ft\" in song_title:\n",
      "        before_ft_pattern = re.compile(\".*(?=\\(Ft)\")\n",
      "        song_title_before_ft = before_ft_pattern.search(song_title).group(0)\n",
      "        clean_song_title = song_title_before_ft.strip()\n",
      "    \n",
      "    else:\n",
      "        song_title_no_lyrics = song_title.replace(\"Lyrics\", \"\")\n",
      "        clean_song_title = song_title_no_lyrics.strip()\n",
      "    \n",
      "    return clean_song_title\n",
      "[clean_up(song) for song in missy_song_titles]\n",
      "## `get_all_songs_from_album()`\n",
      "We were able to extract the song titles for Missy Elliott's album \"Under Construction.\" Success! But now we want to make a function that can do the same thing for any artist and album title.\n",
      "\n",
      "Take a look at [Beyonce's album \"Lemonade\"](https://genius.com/albums/Beyonce/Lemonade) on Genius.com and see how the web page compares to Missy Elliott's \"Under Construction.\" They look extremely similar, right? Because all Genius album pages are identical, we can use the same code that we did for Missy Elliott and just substitute in different artist and album names with an f-string for the Genius URL:\n",
      "\n",
      "`f\"https://genius.com/albums/{artist}/{album_name}\"`\n",
      "def get_all_songs_from_album(artist, album_name):\n",
      "    \n",
      "    artist = artist.replace(\" \", \"-\")\n",
      "    album_name = album_name.replace(\" \", \"-\")\n",
      "    \n",
      "    response = requests.get(f\"https://genius.com/albums/{artist}/{album_name}\")\n",
      "    html_string = response.text\n",
      "    document = BeautifulSoup(html_string, \"html.parser\")\n",
      "    song_title_tags = document.find_all(\"h3\", attrs={\"class\": \"chart_row-content-title\"})\n",
      "    song_titles = [song_title.text for song_title in song_title_tags]\n",
      "    \n",
      "    clean_songs = []\n",
      "    for song_title in song_titles:\n",
      "        clean_song = clean_up(song_title)\n",
      "        clean_songs.append(clean_song)\n",
      "        \n",
      "    return clean_songs\n",
      "get_all_songs_from_album('Beyonce', 'Lemonade')\n",
      "get_all_songs_from_album('Taylor Swift', 'Red')\n",
      "get_all_songs_from_album('Mitski', 'Be The Cowboy')\n",
      "## Your Turn!\n",
      "get_all_songs_from_album('#Your Choice of Artist', '#Your Choice of Album')\n",
      "## Songs Lyrics Analysis\n",
      "Since we now have access to all these great song lyrics, we might as well run some basic analysis on them! We can use the word frequency code that we introduced during the first couple weeks to count the most frequent words in an entire album.\n",
      "import nltk\n",
      "nltk.download('stopwords')\n",
      "from collections import Counter\n",
      "from nltk.corpus import stopwords\n",
      "import os\n",
      "import re\n",
      "## Make a Function That Splits Up Words\n",
      "def split_into_words(any_chunk_of_text):\n",
      "    lowercase_text = any_chunk_of_text.lower()\n",
      "    split_words = re.split(\"\\W+\", lowercase_text)\n",
      "    return split_words\n",
      "## Make a Function That Counts Top 15 Words in Text File\n",
      "def get_most_frequent_words(filepath_of_text):\n",
      "    \n",
      "    nltk_stop_words = stopwords.words(\"english\")\n",
      "    number_of_desired_words = 15\n",
      "    \n",
      "    full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
      "\n",
      "    all_the_words = split_into_words(full_text)\n",
      "    meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "    meaningful_words_tally = Counter(meaningful_words)\n",
      "    most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "    return most_frequent_meaningful_words\n",
      "## Make a Function That Counts Top 15 Words in An Entire Dicrectory of Text Files\n",
      "def get_most_frequent_words_directory(directory):\n",
      "    \n",
      "    nltk_stop_words = stopwords.words(\"english\")\n",
      "    number_of_desired_words = 15\n",
      "    meaningful_words_tally = Counter()\n",
      "    \n",
      "    for filepath_of_text in os.listdir(directory):\n",
      "        if filepath_of_text.endswith(\".txt\"):\n",
      "    \n",
      "            full_text = open(f\"{directory}/{filepath_of_text}\", encoding=\"utf-8\").read()\n",
      "\n",
      "            all_the_words = split_into_words(full_text)\n",
      "            meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
      "            meaningful_words_tally.update(meaningful_words)\n",
      "    \n",
      "    most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
      "\n",
      "    return most_frequent_meaningful_words\n",
      "get_most_frequent_words_directory(\"Missy-Elliott_Under-Construction\")\n",
      "## Data Viz With Pandas\n",
      "import pandas as pd\n",
      "frequencies = get_most_frequent_words_directory(\"Missy-Elliott_Under-Construction\")\n",
      "word_frequency_df = pd.DataFrame(frequencies)\n",
      "word_frequency_df.columns = ['word', 'word_count']\n",
      "word_frequency_df.plot(x='word', kind='barh', figsize=(10,5), title=\"Album Word Frequencies\", fontsize=20).invert_yaxis()\n",
      "## Words in Context\n",
      "from IPython.display import Markdown, display\n",
      "import glob\n",
      "from pathlib import Path\n",
      "word = \"ti\"\n",
      "for file in glob.glob(f\"{directory_path}/*.txt\"):\n",
      "    text = open(file).read()\n",
      "    for line in text.split(\"\\n\"):\n",
      "        if re.search(f\"\\\\b{word}\\\\b\", line):\n",
      "            line_with_bolding = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", line, flags=re.IGNORECASE)\n",
      "            display(Markdown(line_with_bolding))\n",
      "word = \"oh\"\n",
      "for file in glob.glob(f\"{directory_path}/*.txt\"):\n",
      "    song_title = Path(file).stem.replace(\"-\", \" \")\n",
      "    \n",
      "    text = open(file).read()\n",
      "    for line in text.split(\"\\n\"):\n",
      "        if re.search(f\"\\\\b{word}\\\\b\", line):\n",
      "            line_with_bolding = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", line, flags=re.IGNORECASE)\n",
      "            display(Markdown(f\"Line: {line_with_bolding} <br> From: {song_title} \"))\n",
      "word = \"Missy\"\n",
      "for file in glob.glob(f\"{directory_path}/*.txt\"):\n",
      "    song_title = Path(file).stem.replace(\"-\", \" \")\n",
      "    \n",
      "    text = open(file).read()\n",
      "    for line in text.split(\"\\n\"):\n",
      "        if re.search(f\"\\\\b{word}\\\\b\", line, flags=re.IGNORECASE):\n",
      "            line_with_bolding = re.sub(f\"\\\\b{word}\\\\b\", f\"**{word}**\", line, flags=re.IGNORECASE)\n",
      "            display(Markdown(f\"Line: {line_with_bolding} <br> From: {song_title} \"))\n",
      "## Display Images in Pandas DataFrame\n",
      "## Transform Song Titles, Page View Counts, & Album Covers into a DataFrame\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_colwidth\", 100)\n",
      "missy_df = pd.read_csv(\"Missy-Elliott-Album-Cover-Images.csv\")\n",
      "missy_df \n",
      "Just for fun, we can do the same thing but also add links to images of Missy Elliott's album art—and we can actually display those images, too!\n",
      "\n",
      "To display images in a Pandas dataframe, you need to run `from IPython.core.display import HTML` and make the function `get_image_html()`. We're going to take the image URLs and make them into HTML objects.\n",
      "def get_image_html(link):\n",
      "    image_html = f\"<img src='{link}' width='100'>\"\n",
      "    return image_html\n",
      "#Use the function get_image_html()\n",
      "missy_df['album_cover'] = missy_df['album_cover_url'].apply(get_image_html)\n",
      "missy_df\n",
      "from IPython.core.display import HTML\n",
      "If we call `HTML()` on our dataframe and add the method `.to_html(escape=False)` to the dataframe, then it should display the dataframe with viewable images.\n",
      "HTML(missy_df.to_html(escape=False))\n",
      "## From Local Images\n",
      "import pandas as pd\n",
      "ny_df = pd.read_csv(\"my-NY-photos.csv\")\n",
      "ny_df \n",
      "def get_image_html(link):\n",
      "    image_html = f\"<img src='{link}' width='100'>\"\n",
      "    return image_html\n",
      "ny_df['image'] = ny_df['image_path'].apply(get_image_html)\n",
      "ny_df\n",
      "HTML(ny_df.to_html(escape=False))\n",
      "## Application Programming Interfaces (APIs)\n",
      "[Download relevant files here](https://melaniewalsh.org/APIs.zip)\n",
      "## Pros and Cons\n",
      "\n",
      "## Collecting Twitter Data\n",
      "[Download relevant files here](https://melaniewalsh.org/Collecting-Twitter-Data-v2.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to collect Twitter data with the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "## Install and Configure Twarc\n",
      "Because twarc relies on Twitter's API, we need to apply for a Twitter developer account and create a Twitter application before we use it. You can find instructions for the application process and for installing and configuring twarc here: [Twitter Collection Setup](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html).\n",
      "🚨Skip this section if you've already configured twarc!!🚨\n",
      "\n",
      "You can configure twarc by running `twarc conifgure` on the command line. Or you can type your Twitter handle (without the @ symbol) and [API keys](https://developer.twitter.com/en/apps) into the quotation marks below and run the cell.\n",
      "#Insert Your Twitter API Info here\n",
      "\n",
      "twitter_handle = \"\"\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "\n",
      "#The Code That Will Configure Twarc\n",
      "configuration = f\"\"\"[{twitter_handle}]\n",
      "consumer_key={consumer_key}\n",
      "consumer_secret = {consumer_secret}\n",
      "access_token = {access_token}\n",
      "access_token_secret= {access_token_secret}\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \".twarc\")\n",
      "with open(config_filename, \"w\") as file_object:\n",
      "    file_object.write(configuration)\n",
      "## Twitter API (Free Version)\n",
      "With the free version of the Twitter API, there are basically two ways to collect your own Twitter data: in real time or ~7 days in the past. To get data any further in the past requires a paid version of the Twitter API. Twarc allows you to collect tweets both in real time and ~7 days in the past.\n",
      "## Collect Tweets in Real Time\n",
      "## Twarc From the Command Line\n",
      "The easiest way to collect tweets with twarc is to use the command line. To collect tweets in real time, you can use the command `twarc filter`, followed by a search query, then the output operator `>` and a filename of your choosing with the \".jsonl\" file extension (which outputs your Twitter data to this JSONL file).\n",
      "`twarc filter \"search term\" > my_file.jsonl`\n",
      "For example, to collect tweets in real time that include the word \"coronavirus,\" you would run:\n",
      "`twarc  filter \"coronavirus\" > coronavirus_filter.jsonl`\n",
      "## Starting and Stopping Twarc\n",
      "If you run `twarc filter` from your command line, `twarc` will keep running until you explicitly stop the process. You can stop a process on the command line by typing `Ctrl + C`.\n",
      "As you may recall, we can run command line functions in Jupyter notebooks by putting an exclamation point `!` at the beginning of a cell. For some reason, however, `!twarc filter` and `!twarc search` don't play very well in Jupyter notebooks (or at least they don't play well consistently). Sometimes when you start running them, they won't stop—even when you hit the stop button or try to interrupt the kernel (the equivalent of `Ctrl + C`).\n",
      "\n",
      "Because of this unpredictability, I recommend that you 1) open your Terminal or PowerShell 2) navigate to the directory that contains this Jupyter notebook 3) and experiment with the twarc code below by copying it and pasting it into your command line, where you can more easily stop the processes.\n",
      "<img src=\"../images/twarc-filter-powershell.png\" width=100%, border=2>\n",
      "<img src=\"../images/twarc-filter-Terminal.png\" widht=100%, border=2>\n",
      "Run a live collection of tweets that include the word \"coronavirus\":\n",
      "`twarc  filter \"coronavirus\" > coronavirus_filter.jsonl`\n",
      "To stop this process:\n",
      "`Ctrl + C`\n",
      "Run a live collection of tweets that include the word \"Shakespeare\":\n",
      "`twarc filter \"Shakespeare\" > shakespeare_filter.jsonl`\n",
      "To stop this process:\n",
      "`Ctrl + C`\n",
      "## Check Number of Tweets Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l coronavirus_filter.jsonl\n",
      "!wc -l shakespeare_filter.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" coronavirus_filter.jsonl\n",
      "!find /v /c \"\" shakespeare_filter.jsonl\n",
      "## Collect Tweets From Past 7 days\n",
      "## Twarc From the Command Line\n",
      "To collect tweets from approximately 7 days in the past, you can use the command `twarc search`, followed by a search query, then the output operator `>` and a filename of your choosing with the \".jsonl\" file extension (which outputs your Twitter data to this JSONL file).\n",
      "Run a collection of tweets from the past ~7 days that include the word \"coronavirus\" for 10 seconds:\n",
      "`twarc search \"coronavirus\" > coronavirus_search.jsonl` \n",
      "Run a collection of tweets from the last ~7 days that include the word \"Shakespeare\":\n",
      "`twarc search \"Shakespeare\" > shakespeare_search.jsonl`\n",
      "## Check Number of Tweets Collected\n",
      " Mac/Chrome OS\n",
      "!wc -l coronavirus_search.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" coronavirus_search.jsonl\n",
      "## Crafting a Good Twitter Query\n",
      "We made relatively simple queries to Twitter's API in the examples above. But there are more specific and more complex ways to make queries.\n",
      "\n",
      "To craft a good Twitter search query, it's important to understand and explore these myriad ways. A researcher named Igor Brigadir has compiled a wonderful resource that details many of the Twitter API search operators: https://github.com/igorbrigadir/twitter-advanced-search/blob/master/README.md\n",
      "## Search for Exact Phrases\n",
      "`twarc search \"\\\"an exact phrase\\\"\"`\n",
      "You can search for an *exact* phrase in a tweet by including the phrase in escaped `\\` quotation marks, as above.\n",
      "**\"Not with a bang but with a...\"**\n",
      "The first phrase that we're going to search for comes from the conclusion of T.S. Eliot's 1925 [poem \"The Hollow Men\"](https://msu.edu/~jungahre/transmedia/the-hollow-men.html):\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">This is the way the world ends<br>\n",
      ">**Not with a bang but with a whimper.**\n",
      "You've probably heard these lines before, even if you didn't know that they were written by the modernist poet T.S. Eliot. This phrase is a striking example of a bit of literary, poetic language that has gone \"viral\" in 21st-century American culture, both on and off the internet.\n",
      "Since there aren't a ton of tweets from the past 7 days that included the phrase \"not with a bang but with a\", we can run this `twarc search` from our Jupyter notebook. Because there is a small and finite number of tweets to be collected, this search will complete in a relatively short amount of time, and we don't have to worry about it running forever.\n",
      "!twarc search \"\\\"not with a bang but with a\\\"\" > not_with_bang_tweets.jsonl\n",
      "## Search for General Phrases\n",
      "**\"Touch my face\"**\n",
      "The other phrase we're going to search for comes from public health recommendations about preventing the spread of the coronavirus: that people should avoid touching their faces. Many people are, in light of these recommendations, discovering that it's actually very difficult not to touch your own face.\n",
      "\n",
      "Now the avoidance of touching one's face has sprouted up as a funny Twitter meme. These various \"touch my face\" memes serves as an interesting example of how online communities produce comedy and levity even in times of stress and crisis.\n",
      "%%html\n",
      "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Working on not touching my face :) <a href=\"https://t.co/qfyNdrDReh\">pic.twitter.com/qfyNdrDReh</a></p>&mdash; Hannah (@McBBQSauce) <a href=\"https://twitter.com/McBBQSauce/status/1235700933801242626?ref_src=twsrc%5Etfw\">March 5, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
      "Similarly, there aren't a ton of tweets from the past 7 days that included the phrase \"touch my face\", so we can also run this `twarc search` from our Jupyter notebook.\n",
      "!twarc search \"touch my face min_retweets:5\" > touch_my_face_tweets.jsonl \n",
      "  \n",
      "Great! Now we have some Twitter data. But before we dive into analysis, we need to complete one more step. We need to convert this JSON data to CSV data, which will be easier for us to work with. Luckily, there's a twarc \"utility\" for this very purpose.\n",
      "## Get Twarc Utilities\n",
      "There are a number of twarc \"utilities\" that enable you to manipulate and analyze Twitter data. With these utilities, you can do things such as convert JSON data to CSV data, count up the most frequent emojis used in tweets, make a network visualization of tweets and Twitter users, and more.\n",
      "\n",
      "These utilities are not available from the `pip install twarc` installation. To access the twarc utilities, you'll need to `git clone` the [twarc GitHub repository](https://github.com/DocNow/twarc) or download it as a zip file.\n",
      "\n",
      "The twarc repository should already be downloaded in your relevant files, but if you uncomment the line below, you can also clone the repository with this line of code.\n",
      "#!git clone https://github.com/DocNow/twarc.git\n",
      "## Use Twarc Utilities\n",
      "`python twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "To use a twarc utility, you need to call `python` from the command line and then include the utility's file path (they all should be in the \"twarc/utils\" subfolder).\n",
      "\n",
      "Note that if your Jupyter notebook is in exactly the same directory as the \"twarc\" repository, then you can run the code as above. However, if your Jupyter notebook is somewhere else, you will have to direct it to the correct location of \"twarc/utils\". For example`python /Users/melaniewalsh/twarc/utils/your_desired_util.py tweets.jsonl`\n",
      "## Convert JSON to CSV\n",
      "To convert a JSON file to a CSV file, you can run `python twarc/utils/json2csv.py` followed by the JSONL filename, the output operator `>` and your desired filename for the CSV file.\n",
      "`python twarc/utils/json2csv.py json_file.jsonl > csv_file.csv`\n",
      ", border=2> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Heads up Windows users! The twarc utility json2csv.py will probably not work on your computer by default. You'll likely get a UnicodeEncodeError because Windows computers do not use Unicode (UTF-8) by default. However, you can make UTF-8 your default by following [these instructions](https://scholarslab.github.io/learn-twarc/08-win-region-settings) and restarting your comptuer. Then json2csv.py should work.\n",
      "\n",
      "Make \"bang.jsonl\" into \"bang.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text not_with_bang_tweets.jsonl > not_with_bang_tweets.csv\n",
      "Make \"face.jsonl\" into \"face.csv\" (with an extra field added for the full version of the original retweeted text)\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text touch_my_face_tweets.jsonl > touch_my_face_tweets.csv\n",
      "!python twarc/utils/json2csv.py --extra-field rt_text retweeted_status.full_text bang.jsonl > bang_experiment.csv\n",
      "## (Optional) Twarc From Python/Jupyter Notebooks\n",
      "Though I recommend collecting tweets from the command line, you can also use twarc as a Python library and run it in a Jupyter notebook. To import twarc, run `from twarc import Twarc` (as in the cell below). We're also going to import a library called JSON to help us output a JSON file.\n",
      "from twarc import Twarc\n",
      "import json\n",
      "## Configure Twarc\n",
      "To use Twarc as a Python library, you'll once again need to configure twarc with your [API keys](https://developer.twitter.com/en/apps) (\\*sigh\\*). Copy and paste them into the quotation marks below.\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "Quick tip! If you've already set up your Twitter API keys with `twarc configure`, you can find your API keys by running `open ~/.twarc` (Mac/Chrome OS) from the command line or using the code below (Mac/Chrome OS/Windows):\n",
      " Mac/Chrome OS\n",
      "!open ~/.twarc\n",
      " Mac/Chrome OS <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='center-left', border=2> Windows \n",
      "import os\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \".twarc\")\n",
      "print(open(config_filename).read())\n",
      "These commands will open/print the \".twarc\" document that hosts your API keys, and you can simply copy and paste the correct information into the variables in the cell above, then run the cell below.\n",
      "twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "## Make Live Tweet Collection Function\n",
      "Below I've written a Python function called `collect_live_tweets()` that uses `twarc.filter()`. This function accepts a search query, the number of tweets that you want to collect, and a filename with a .jsonl extension. This function will output your Twitter data to a file with this filename.\n",
      "def collect_live_tweets(search_query, number_of_desired_tweets, filename):    \n",
      "    \n",
      "    twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "\n",
      "    tweets = []\n",
      "    with open(filename, 'w', encoding='utf-8') as outfile:\n",
      "        for tweet in twarc.filter(search_query):\n",
      "            if len(tweets) < number_of_desired_tweets:\n",
      "                tweets.append(tweet)\n",
      "                json.dump(tweet, outfile)\n",
      "                outfile.write('\\n')\n",
      "            else:\n",
      "                break\n",
      "    return\n",
      "## Run Live Tweet Collection Function\n",
      "collect_live_tweets(\"coronavirus\", 100, \"coronavirus_filter.jsonl\")\n",
      "Below I've written a Python function called `collect_past_tweets()` that uses `twarc.search()`. This function accepts a search query, the maximum number of tweets that you want to collect, and a filename with a .jsonl extension. This function will output your Twitter data to a file with this filename. \n",
      "## Make Past Tweet Collection Function\n",
      "def collect_past_tweets(search_query, number_of_max_tweets, filename):    \n",
      "    \n",
      "    twarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "\n",
      "    tweets = []\n",
      "    with open(filename, 'w', encoding='utf-8') as outfile:\n",
      "        for tweet in twarc.search(search_query):\n",
      "            if len(tweets) < number_of_max_tweets:\n",
      "                tweets.append(tweet)\n",
      "                json.dump(tweet, outfile)\n",
      "                outfile.write('\\n')\n",
      "            else:\n",
      "                break\n",
      "    return\n",
      "## Run Past Tweet Collection Function\n",
      "collect_past_tweets(\"coronavirus\", 1000, \"coronavirus_search.jsonl\")\n",
      "collect_past_tweets(\"\\\"not with a bang but with a\\\"\", 1000, \"bang.jsonl\")\n",
      "collect_past_tweets(\"touch my face min_retweets:10\", 2000, \"face.jsonl\")\n",
      "  \n",
      "## Twitter Data Collection\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "- [Twitter API Set Up](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Collection-Setup.html)\n",
      "- [Twitter Data Collection](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Collection.html)\n",
      "- [Twitter Data Analysis](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Analysis.html)\n",
      "- [Twitter Data Sharing](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Sharing.html)\n",
      "## Reddit Data Collection — With PRAW\n",
      "One way to collect Reddit data is with the Reddit API and [PRAW](https://praw.readthedocs.io/en/latest/getting_started/quick_start.html) (an acronym for **P**ython **R**eddit **W**rapper). \n",
      "!pip install praw\n",
      "import praw\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_colwidth\", 500)\n",
      "## Apply for Reddit API Access\n",
      "To apply for Reddit API access, [read the instructions here](https://www.reddit.com/wiki/api) and then [fill out an application and aggree to the Terms of Use here](https://docs.google.com/forms/d/e/1FAIpQLSezNdDNK1-P8mspSbmtC2r86Ee9ZRbC66u929cG2GX0T9UMyw/viewform). Once you have a Reddit developer account, create a PRAW instance with your client ID, client secret, and Reddit user name.\n",
      "reddit = praw.Reddit(client_id='your client id',\n",
      "                     client_secret='your client secret',\n",
      "                     user_agent='your reddit user name')\n",
      "## Get Reddit Posts From a Subreddit\n",
      "The following code draws from [TannerGilbert's PRAW tutorial code](https://github.com/TannerGilbert/Tutorials/blob/master/Reddit%20Webscraping%20using%20PRAW/Reddit%20API.ipynb).\n",
      "## Hot Posts\n",
      "hot_posts = reddit.subreddit('AmItheAsshole').hot(limit=10)\n",
      "for reddit_post in hot_posts:\n",
      "    print(reddit_post.title)\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').hot(limit=10):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "## Top Posts\n",
      "### By Day\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"day\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "### By Month\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"month\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "### By Year\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"year\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "### By All Time\n",
      "for reddit_post in reddit.subreddit('AmItheAsshole').top(\"all\", limit=5):\n",
      "    print(f\"✨Title✨\\n{reddit_post.title}\\n✨Text✨\\n{reddit_post.selftext}\\n\")\n",
      "To get Reddit posts by a specific date range, see the tutorial on using the Puhshift.io\n",
      "## Get Reddit Posts From a Subreddit and Make a DataFrame\n",
      "To see all the attributes that you can retrieve from a single Reddit post, consult [PRAW's \"Submission\" documentation](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html#praw.models.Submission).\n",
      "reddit_posts = []\n",
      "aita_subreddit = reddit.subreddit('AmItheAsshole')\n",
      "\n",
      "for reddit_post in aita_subreddit.top(\"month\", limit=10):\n",
      "    reddit_posts.append([reddit_post.title, reddit_post.score, reddit_post.id, reddit_post.subreddit, reddit_post.url, reddit_post.num_comments, reddit_post.selftext, reddit_post.created_utc])\n",
      "\n",
      "reddit_posts = pd.DataFrame(reddit_posts, columns=['title', 'upvote_score', 'post_id', 'subreddit', 'post_url', 'num_comments', 'post_body', 'full_date'])\n",
      "\n",
      "#Format date\n",
      "reddit_posts['full_date'] = pd.to_datetime(reddit_posts['full_date'], utc=True, unit='s')\n",
      "reddit_posts['date'] = reddit_posts['full_date'].dt.strftime(\"%Y-%m-%d\")\n",
      "reddit_posts\n",
      "## Save to CSV File\n",
      "reddit_posts.to_csv(\"top-reddit-aita-posts.csv\", encoding=\"utf-8\", index=False)\n",
      "## Get Comments From a Post\n",
      "https://praw.readthedocs.io/en/latest/tutorials/comments.html\n",
      "submission = reddit.submission(id=\"gcr7vr\")\n",
      "submission.comments.replace_more(limit=None)\n",
      "\n",
      "for comment in submission.comments:\n",
      "    print(f\"\\nAuthor:\\n{comment.author}\\n\\nComment:\\n{comment.body}\\n\\n-------------------------------------\")\n",
      "def get_comments(row):\n",
      "    submission = reddit.submission(id=row['post_id'])\n",
      "    submission.comments.replace_more(limit=None)\n",
      "    comments = [comment.body for comment in submission.comments]\n",
      "    return comments\n",
      "reddit_posts = []\n",
      "aita_subreddit = reddit.subreddit('Datasets')\n",
      "\n",
      "for reddit_post in aita_subreddit.top(\"month\", limit=10):\n",
      "    reddit_posts.append([reddit_post.title, reddit_post.score, reddit_post.id, reddit_post.subreddit, reddit_post.url, reddit_post.num_comments, reddit_post.selftext, reddit_post.created_utc])\n",
      "\n",
      "reddit_posts = pd.DataFrame(reddit_posts, columns=['title', 'upvote_score', 'post_id', 'subreddit', 'post_url', 'num_comments', 'post_body', 'full_date'])\n",
      "\n",
      "#Format date\n",
      "reddit_posts['full_date'] = pd.to_datetime(reddit_posts['full_date'], utc=True, unit='s')\n",
      "reddit_posts['date'] = reddit_posts['full_date'].dt.strftime(\"%Y-%m-%d\")\n",
      "reddit_posts\n",
      "reddit_posts['comments'] = reddit_posts.apply(get_comments, axis='columns')\n",
      "reddit_posts[['title','post_body', 'num_comments', 'comments']]\n",
      "\n",
      "## Reddit Data Collection\n",
      "<img src=\"https://www.redditinc.com/assets/images/site/reddit-logo.png\", border=2>\n",
      "- [Reddit Data Collection With PRAW](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Reddit-Data-Collection-With-Praw.html)\n",
      "- [Reddit Data Collection With Pushshift](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Reddit-Data-Collection-With-Pushshift.html)\n",
      "## Convert Kindle Book to Plain Text File\n",
      "## Purchase and Download Kindle Book\n",
      "- Purchase your Kindle book\n",
      "- Go to your Amazon account -> Manage content and devices\n",
      "<img src=\"../images/Calibre/Manage-content-devices.png\", border=2>\n",
      "- Click on the ellipsis \"...\" next to the text that you want to download and select \"Download & transfer via USB\"\n",
      "<img src=\"../images/Calibre/Download-USB-Dreams.png\", border=2>\n",
      "The file should download to your computer.\n",
      "## Download and Install Calibre\n",
      "Download Calibre: https://calibre-ebook.com/download\n",
      "## Download and Install DeDRM Plugin\n",
      "Download the latest release of DeDRM tools by clicking \"DeDRM_tools_6.7.0.zip\" (or whatever version number is most recent) here: https://github.com/apprenticeharper/DeDRM_tools/releases\n",
      "\n",
      "Unzip the file but do not unzip the plugins inside this folder.\n",
      "## Load DeDRM Plugin\n",
      "Go to Calibre -> Preferences -> Plugins -> Load plugin from file. Then navigate to the DeDRM_Plugin.zip file inside DeDRM_tools_6.7.0 folder. \n",
      "<img src=\"../images/Calibre/Preferences.png\", border=2>\n",
      "<img src=\"../images/Calibre/Plugins.png\", border=2>\n",
      "## Add Kindle Serial Number\n",
      "Finally, add your Kindle serial number to the plugin. To find your Kindle serial number, go to your Amazon account -> Manage content and devices -> Devices\n",
      "<img src=\"../images/Calibre/Serial-Number.png\", border=2>\n",
      "Then go back to Calibre's Preferences -> Plugins. Select \"Show only user installed plugins\", click on the DeDRM plugin and select \"Customize plugin\". Select \"eInk Kindle ebooks\" and add your Kindle serial number.\n",
      "<img src=\"../images/Calibre/Customize-eInk.png\", border=2>\n",
      "## Convert Book to Text File\n",
      "Now drag and drop your Kindle book (.azw) into Calibre. Once it's loaded, select \"Convert books\" form the Calibre menu, which will cause a new window to appear. Then choose \"TXT\" as the desired output format and select \"OK\".\n",
      "<img src=\"../images/Calibre/Output-Format.png\", border=2>\n",
      "You should find the plain text file in a directory called \"Calibre Library\", which should be in your home folder.\n",
      "<img src=\"../images/Calibre/Where-to-Find-Text-File.png\", border=2>\n",
      "## Reddit Data Collection — With Pushshift.io\n",
      "Another way to collect Reddit data is with [Pushshift.io](https://pushshift.io/). If you're looking to collect Reddit data within a certain date range, Pushshift may be a good option since this is not easy to accomplish with PRAW and Reddit's API. Pushshift also does not require an API key or registration.\n",
      "## Import Libraries\n",
      "import requests\n",
      "import json\n",
      "import pandas as pd\n",
      "pd.set_option(\"max_columns\", 80)\n",
      "pd.set_option(\"max_rows\", 100)\n",
      "import datetime\n",
      "## Make Function for Formatting Date Range as UTC Timestamps\n",
      "def format_date(date):\n",
      "    return int(datetime.datetime.strptime(date, \"%m-%d-%Y\").timestamp())\n",
      "## Format API Request URL\n",
      "Below I'm requesting the 50 most upvoted Reddit posts from the subreddit \"AmItheAsshole\" that were published between April 1, 2020 and April 30, 2020 (if they have at least an upvote score of 100).\n",
      "subreddit = \"AmItheAsshole\"\n",
      "start_date = \"04-01-2020\"\n",
      "end_date = \"04-30-2020\"\n",
      "num_posts = 50\n",
      "upvote_score = 100\n",
      "\n",
      "url = f\"\"\"\n",
      "https://api.pushshift.io/reddit/search/submission/\\\n",
      "?subreddit={subreddit}\\\n",
      "&sort=desc\\\n",
      "&sort_type=score\\\n",
      "&after={format_date(start_date)}\\\n",
      "&before={format_date(end_date)}\\\n",
      "&size={num_posts}\\\n",
      "&score=>{upvote_score}\"\"\"\n",
      "NOTE: The backslashes above `\\` allow us to break the URL up onto multiple lines. They're ultimately ignored in the URL.\n",
      "## Send API Request\n",
      "response = requests.get(url)\n",
      "reddit_data = response.json()['data']\n",
      "## Make API Response into DataFrame\n",
      "reddit_data = pd.DataFrame(reddit_data)\n",
      "reddit_data.head(3)\n",
      "## Examine Columns\n",
      "reddit_data.columns\n",
      "## Add Formatted Date Columns\n",
      "reddit_data['full_date'] = pd.to_datetime(reddit_data['created_utc'], utc=True, unit='s')\n",
      "reddit_data['date'] = reddit_data['full_date'].dt.strftime(\"%Y-%m-%d\")\n",
      "## Filter Columns\n",
      "reddit_data[['subreddit', 'date', 'score', 'num_comments','title', 'selftext', 'full_link','url']]\n",
      "## Twitter API Setup\n",
      "To collect Twitter data, we're going to work with the [Twitter API](https://developer.twitter.com/en/docs/basics/getting-started) and [twarc](https://github.com/DocNow/twarc), a Python package for collecting Twitter data through the Twitter API. To access the Twitter API, we first need to:\n",
      "\n",
      "**1.** Apply for a Twitter developer account\n",
      "\n",
      "**2.** Create a Twitter application\n",
      "\n",
      "The developer account will allow us to create an application, which will eventually get us a series of API keys and tokens, which we can then use to access Twitter data.\n",
      "\n",
      "According to Twitter, the reason for this somewhat drawn-out application process is to \"prevent abuse of the Twitter platform\" and \"better understand and serve our developer community.\"\n",
      "\n",
      "After getting our API keys, we then need to\n",
      "\n",
      "**3.** Install twarc\n",
      "\n",
      "**4.** Configure/set up twarc\n",
      "\n",
      "**5.** Download the twarc repository\n",
      "\n",
      "The following instructions will guide you through each part of this 5-step process.\n",
      "<img src=\"../images/Twitter/apply-for-access.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/developer-primary-reason.png\" width=100%, border=2>\n",
      "**4.** The next page of the application will ask: \"This is you, right?\" Confirm that your Twitter username and email are correct, select your country of residence (United States), and come up with a name for your application. It doesn't matter which name you choose. I'd suggest using your first name.\n",
      "<img src=\"../images/Twitter/twitter-use-explanation.png\" width=100%, border=2>\n",
      "\n",
      "<img src=\"../images/Twitter/developer-agreement.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/developer-success.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/app-home-page.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/create-app.png\" width=100%, border=2>\n",
      "<img src=\"../images/Twitter/keys-and-tokens.png\" width=100%, border=2>\n",
      "\n",
      "## 3. Install twarc\n",
      "\n",
      "To install twarc, simply run the following on the command line:\n",
      "!pip install twarc\n",
      "!twarc version\n",
      "## 4. Configure/Set up Twarc\n",
      "Now that twarc is installed on our computers, we need to set it up so that we can collect Twitter data with this tool. We need to submit and save our Twitter API keys into twarc.\n",
      "\n",
      "There are two options for configuring twarc.\n",
      "## Option 1 — Configure Twarc From the Command Line\n",
      "To configure twarc, open up your Terminal or PowerShell and copy and paste `twarc configure` into your command line.\n",
      "!twarc configure\n",
      "Twarc will prompt you to copy and paste in your Twitter consumer key and Twitter consumer secret. Then it will ask you to visit a URL to authorize access to the Twitter account that is associated with your API keys.\n",
      "Please enter your Twitter application credentials from apps.twitter.com:\n",
      "\n",
      "consumer key:\n",
      "\n",
      "consumer secret:\n",
      "\n",
      "Please log into Twitter and visit this URL in your browser:\n",
      "\n",
      "https://api.twitter.com/oauth/authorize?oauth_token=UNIQUE-TOKEN\n",
      "<img src=\"../images/Twitter/Twitter-authorize.png\" width=100%, border=2>\n",
      "\n",
      "Once you click \"Authorize App\", you will be redirected to the URL that is associated with your  API keys (likely our course website). You need to carefully inspect this URL because it actually contains the PIN that you need for the last step of twarc configuration. The URL will look something like this:\n",
      "`https://melaniewalsh.github.io/Intro-Cultural-Analytics/oauthtoken=YOUR-UNIQUE-TOKEN&oauthverifier=THIS-IS-THE-DISPLAYED-PIN-YOU-NEED`\n",
      "You need to copy and paste the part after `oauthverifier=` into the prompt at the command line:\n",
      "After you have authorized the application please enter the displayed PIN:\n",
      "If the PIN works, then you will get a happy successs message. Make sure to copy and paste this message into HW 6.\n",
      "The credentials for mellymeldubs have been saved to your configuration file at /Users/melaniewalsh/.twarc\n",
      "\n",
      "✨ ✨ ✨  Happy twarcing! ✨ ✨ ✨\n",
      "## Option 2 — Configure Twarc in This Notebook\n",
      "Copy your [API keys](https://developer.twitter.com/en/apps) and paste them into the quotation marks below. Also type in your Twitter handle without the @ symbol.\n",
      "twitter_handle = \"\"\n",
      "consumer_key= \"\"\n",
      "consumer_secret = \"\"\n",
      "access_token = \"\"\n",
      "access_token_secret= \"\"\n",
      "Then run the two cells below:\n",
      "configuration = f\"\"\"[{twitter_handle}]\n",
      "consumer_key={consumer_key}\n",
      "consumer_secret = {consumer_secret}\n",
      "access_token = {access_token}\n",
      "access_token_secret= {access_token_secret}\n",
      "\"\"\"\n",
      "import os\n",
      "config_filename = os.path.join(os.path.expanduser(\"~\"), \".twarc\")\n",
      "with open(config_filename, \"w\") as file_object:\n",
      "    file_object.write(configuration)\n",
      "To test whether twarc has been properly configured, run a sample search and see if Twitter data gets returned:\n",
      "!twarc search \"something incredibly obscure\"\n",
      "## 5. Download the Twarc Repository \n",
      "Finally, we also need to download the twarc repository from GitHub, because there are a few things in it that aren't included in the version of twarc that's installed through pip. To download the repository, run:\n",
      "!git clone https://github.com/DocNow/twarc.git\n",
      "If Git isn't working for some reason, you can also download the repository as a zip file: https://github.com/DocNow/twarc/archive/master.zip\n",
      "## Song Genius Data Collection\n",
      "<img src=\"../images/Missy-Under-Construction.png\" width=100%, border=2>\n",
      "- [Genius API](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Genius-API.html)\n",
      "- [LyricsGenius](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Lyrics-Genius.html)\n",
      "- [Get All Song Lyrics From Album](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Get-All-Songs-From-Album.html)\n",
      "- [Simply Song Lyrics Analysis](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Song-Lyrics-Analysis.html)\n",
      "\n",
      "## Twitter Data Sharing\n",
      "[Download relevant files here](https://melaniewalsh.org/Twitter-Data-Sharing.zip) or run `git pull` from command line in the \"Intro-Cultural-Analytics-Notebooks\" directory\n",
      "<img src=\"https://cfcdnpull-creativefreedoml.netdna-ssl.com/wp-content/uploads/2017/06/Twitter-featured.png\" width=100%, border=2>\n",
      "In this lesson, we're going to learn how to share Twitter data and access Twitter data that has been shared by others with the Python/command line tool [twarc](https://github.com/DocNow/twarc). This tool was developed by a project called [Documenting the Now](https://www.docnow.io/). The DocNow team develops tools and ethical frameworks for social media research.\n",
      "\n",
      "This lesson presumes that you've already installed and configured twarc (which was covered in [previous lessons](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Twitter-Data-Collection.html#Install-and-Configure-Twarc)).\n",
      "## Tweet IDs\n",
      "Twitter discourages developers and researchers from sharing full Twitter data openly on the web. They instead encourage developers and researchers to share *tweet IDs*:\n",
      "\n",
      "> [If you provide Twitter Content to third parties, including downloadable datasets or via an API, you may only distribute **Tweet IDs**, Direct Message IDs, and/or User IDs.](https://developer.twitter.com/en/developer-terms/policy#4-e)\n",
      "\n",
      "Tweet IDs are unique identifiers assigned to every tweet. They look like a random string of numbers: 1189206626135355397. Each tweet ID can be used to download the full data associated with that tweet (if the tweet still exists). This is a process called \"hydration.\"\n",
      "<img src=\"https://cdn.pixabay.com/photo/2013/07/12/19/24/sapling-154734_960_720.png\" width=100%, border=2>\n",
      "**Hydration: a young tweet ID sprouts into a full tweet (to be read in David Attenborough's voice)**\n",
      "There are actually two reasons that you might want to dehydrate tweets and/or hydrate tweet IDs: first, to responsibly share Twitter data with others and/or access Twitter data shared by others; second, to get more information about the Twitter data that *you yourself collected*.\n",
      "\n",
      "If you collected tweets in real time, for example, you collected those tweets immediately after they were published, which means that they will not contain any retweet or favorite count information. Nobody's had time to retweet them yet! So if you'd like to retroactively get retweet and favorite count information about your tweets, then you would want to dehydrate and rehydrate them.\n",
      "## Dehydrate Tweets\n",
      "`twarc dehydrate tweets.jsonl > tweet_ids.txt`\n",
      "To transform your Twitter data into a list of tweet IDs (so that you can share your data openly on the web), you can run the twarc command `twarc dehydrate` with the name of your JSONL file followed by the output operator `>` and the desired name of your tweet ID text file.\n",
      "\n",
      "> tweet ID —> tweet = hydration <br>\n",
      "> tweet ID <— tweet = dehydration\n",
      "Let's dehydrate the Twitter data that we collected a few weeks ago: a JSONL file of 685 tweets that mentioned the general phrase \"touch my face\" (most responding to public health recommendations that people should avoid touching their faces).\n",
      "!twarc dehydrate touch_my_face_tweets.jsonl > touch_my_face_tweet_ids.txt\n",
      "If we `open()` and `.read()` the tweet IDs file that we just created, it looks something like this:\n",
      "tweet_ids = open(\"touch_my_face_tweet_ids.txt\", encoding=\"utf-8\").read()\n",
      "print(tweet_ids)\n",
      "## Hydrate Tweets\n",
      "`twarc hydrate tweet_ids.txt > tweets.jsonl`\n",
      "To transform a list of tweet IDs into full Twitter data, you can run the twarc command `twarc hydrate` with the name of your tweet IDs text file followed by the output operator `>` and the desired name of your JSONL file.\n",
      "\n",
      "> tweet ID —> tweet = hydration <br>\n",
      "> tweet ID <— tweet = dehydration\n",
      "Now let's re-hydrate the Twitter data that we collected a few weeks ago based on the tweet IDs that we just dehydrated.\n",
      "!twarc hydrate touch_my_face_tweet_ids.txt > touch_my_face_tweets_REHYDRATED.jsonl\n",
      "tweet_json = open(\"touch_my_face_tweets_REHYDRATED.jsonl\", encoding=\"utf-8\").read()\n",
      "print(tweet_json)\n",
      "## Deleted Tweets & The Right To Be Forgotten\n",
      "What happens if someone decides to delete their tweet between the time when the tweet is first collected and the time when the tweet is \"hydrated\"? The deleted tweet will **not** be hydrated. The deleted tweet is no longer be accessible.\n",
      "\n",
      "To see how many tweets might be gone from our dataset, let's look at how many tweets are included in our rehydrated tweet file vs our original tweet file.\n",
      " Mac/Chrome OS\n",
      "!wc -l touch_my_face_tweets_REHYDRATED.jsonl\n",
      "!wc -l touch_my_face_tweets.jsonl\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" touch_my_face_tweets_REHYDRATED.jsonl\n",
      "!find /v /c \"\" touch_my_face_tweets.jsonl\n",
      "As you can see, our rehydrated tweet file is missing 10 tweets. Those tweets have either been deleted, been made private, or been suspended.\n",
      "## Separate Out Deleted Tweets (From Tweet IDs)\n",
      "`python twarc/utils/tweet_compliance.py tweet_ids.txt > hydrated_tweets.json 2> deleted_tweet_ids.txt`\n",
      "If you're working from someone else's tweet IDs, you can hydrate these tweet IDs and filter out the tweet IDs that have been deleted/made private/suspended by using the twarc utility `twarc/utils/tweet_compliance.py`, followed by the output operator `>`, a JSONL file name for your hydrated tweets, the number `2`, another output operator `>` and a file name for the deleted tweet IDs.\n",
      "!python twarc/utils/tweet_compliance.py touch_my_face_tweet_ids.txt > hydrated_tweets.json 2> touch_my_face_deleted_tweets.txt\n",
      " Mac/Chrome OS\n",
      "!wc -l touch_my_face_deleted_tweets.txt\n",
      "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Windows_logo_-_2012_derivative.svg/1024px-Windows_logo_-_2012_derivative.svg.png width=20 align='left', border=2> Windows \n",
      "!find /v /c \"\" touch_my_face_deleted_tweets.txt\n",
      "## Find Current Status of Tweets (From Tweet JSONL File)\n",
      "`python twarc/utils/deletes.py tweest.jsonl > current_status_of_tweets.txt`\n",
      "If you want to find out the current status of tweets that you've already collected, you can use the twarc utility `twarc/utils/deletes.py` followed by the output operator `>` then the file name for your text file.\n",
      "!python twarc/utils/deletes.py touch_my_face_tweets.jsonl > current_status_of_tweets.txt\n",
      "tweet_current_status = open(\"current_status_of_tweets.txt\", encoding=\"utf-8\").read()\n",
      "print(tweet_current_status)\n",
      "## Update/Enhance Twitter Data with Current Status of Tweets\n",
      "`python twarc/utils/deletes.py --enhance tweets.jsonl > tweets_with_current_status.jsonl`\n",
      "!python twarc/utils/deletes.py --enhance touch_my_face_tweets.jsonl > touch_my_face_tweets_CURRENT_STATUS.jsonl\n",
      "## Where to Find Tweet IDs\n",
      "DocNow Catalog: https://www.docnow.io/catalog/\n",
      "\n",
      "George Washington University Tweet IDs: https://dataverse.harvard.edu/dataverse/gwu-libraries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        #print(data['source'])\n",
    "\n",
    "        for cell in data['cells']:\n",
    "            #if cell['cell_type'] == 'markdown':\n",
    "                #for content in cell['source']:\n",
    "                    #print(cell['source'])\n",
    "                    #content = [\"\".join(inner_content) for inner_content in content]\n",
    "                    \n",
    "                    print(\"\".join(cell['source']))\n",
    "                    #print(type(content))\n",
    "                            #content = content.replace('\", border=2>', '\" border=2>')\n",
    "                        #print(content)\n",
    "                        #cell['source'] = list(content)\n",
    "                        #ell['source']\n",
    "                        #print(content)\n",
    "                        #with open(file, 'w') as json_file:\n",
    "                          #  json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change H1 headers to H2 headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        #print(data['source'])\n",
    "\n",
    "        for cell in data['cells']:\n",
    "            if cell['cell_type'] == 'markdown':\n",
    "                for content in cell['source']:\n",
    "                    #print(cell['source'])\n",
    "                    if \"# \" in content:\n",
    "                        content = re.sub(r\"(?<!#)# \", \"## \", content)\n",
    "                        #print(content)\n",
    "                        #content = content.replace('>', ', border=2>')\n",
    "                        cell['source'] = list(content)\n",
    "                        #ell['source']\n",
    "                        #print(content)\n",
    "                        with open(file, 'w') as json_file:\n",
    "                            json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"# header ## not header\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## header ## not header'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-bf30fa7f5b37>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-bf30fa7f5b37>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    contents* = [cell['source'] for cell in data['cells'] if cell['source'].contains('<img')]\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        contents = [cell['source'] for cell in data['cells'] if cell['source'].contains('<img')]\n",
    "        print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('Cultural-Data-Analysis/Pandas.ipynb') as f:\n",
    "    data = json.load(f)\n",
    "    #print(data['source'])\n",
    "\n",
    "    for cell in data['cells']:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            for content in cell['source']:\n",
    "                #print(cell['source'])\n",
    "                if \"<img\" in content:\n",
    "                    content = content.replace('>', ', border=1>')\n",
    "                    cell['source'] = list(content)\n",
    "                    #ell['source']\n",
    "                    #print(content)\n",
    "                    with open('Cultural-Data-Analysis/Pandas.ipynb', 'w') as json_file:\n",
    "                        json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
