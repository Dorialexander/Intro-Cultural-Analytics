{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## HW 10 \u2014 Named Entity Recognition"}, {"cell_type": "markdown", "metadata": {}, "source": "[Download relevant files](https://melaniewalsh.org/spacy.zip)"}, {"cell_type": "markdown", "metadata": {}, "source": "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 4 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-10.ipynb\" and submit it to Canvas by Friday, April 23rd at 5pm.\n\n*This notebook is exactly the same as the [\"Named Entity Recognition\" lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Named-Entity-Recognition.html), and submitting either notebook is fine."}, {"cell_type": "markdown", "metadata": {}, "source": "In this lesson, we're going to learn about a text analysis method called **Named Entity Recognition** (NER). This method will help us computationally identify people, places, and things (of various kinds) in a text or collection of texts."}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"../images/Ada-Lovelace-NER.png\" width=\"100%\" >"}, {"cell_type": "markdown", "metadata": {}, "source": "## Why is NER Useful?"}, {"cell_type": "markdown", "metadata": {}, "source": "Named Entity Recognition is useful for extracting key information from texts. You might use NER to identify the most frequently appearing characters in a novel or build a network of characters (something we'll do in a later lesson!). Or you might use NER to identify the geographic locations mentioned in texts, a first step toward mapping the locations (something we'll also do in a later lesson!)."}, {"cell_type": "markdown", "metadata": {}, "source": "## Natural Language Processing (NLP)"}, {"cell_type": "markdown", "metadata": {}, "source": "Named Entity Recognition is a fundamental task in the field of *natural language processing* (NLP). What is NLP, exactly? NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called *spellcheck*? How about autocomplete, Google translate, chat bots, and Siri? These are all examples of NLP in action!\n\nThanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts are now getting eerily good. (If you don't believe me, check out [this app that will autocomplete your sentences](https://transformer.huggingface.co/doc/gpt2-large/qCNMTfzephfZMBkryTNvSRKQ/edit) with GPT-2, a state-of-the-art text generation model. When I ran it, the model generated a mini-lecture from a \"university professor\" that sounds spookily close to home...)"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"../images/GPT-2.png\" >"}, {"cell_type": "markdown", "metadata": {}, "source": "Open-source NLP tools are getting very good, too. We're going to use one of these open-source tools, the Python library `spaCy`, for our Named Entity Recognition tasks in this lesson."}, {"cell_type": "markdown", "metadata": {}, "source": "## How spaCy Works"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"../images/Ada-Lovelace-NER.png\" >"}, {"cell_type": "markdown", "metadata": {}, "source": "The screenshot above shows spaCy correctly identifying named entities in Ada Lovelace's *New York Times* obituary (something that we'll test out for ourselves below). How does spaCy know that \"Ada Lovelace\" is a person and that \"1843\" is a date?"}, {"cell_type": "markdown", "metadata": {}, "source": "Well, spaCy doesn't *know*, not for sure anyway. Instead, spaCy is making a very educated guess. This \"guess\" is based on what spaCy has learned about the English language after seeing lots of other examples."}, {"cell_type": "markdown", "metadata": {}, "source": "That's a colloquial way of saying: spaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. (These texts were, in fact, often labeled and corrected by hand). This is similar to our <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/Topic-Modeling-Overview.html#1)-LDA-is-an-Unsupervised-Algorithm\">topic modeling work</a> from the previous lesson, except our topic model wasn't using labeled data.\n\nThe English-language spaCy model that we're going to use in this lesson was trained on an annotated corpus called [\"OntoNotes\"](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf): 2 million+ words drawn from \"news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,\" which were meticulously tagged by a group of researchers and professionals for people's names and places, for nouns and verbs, for subjects and objects, and much more. (Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.)\n\nWhen spaCy identifies people and places in Ada Lovelace's obituary, in other words, its NLP model is actually making a series of *predictions* about the text based on what it has learned about how people and places function in English-language sentences."}, {"cell_type": "markdown", "metadata": {}, "source": "## NER with spaCy"}, {"cell_type": "markdown", "metadata": {}, "source": "## Install spaCy"}, {"cell_type": "markdown", "metadata": {}, "source": "To use spaCy, we first need to install the library."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install -U spacy"}, {"cell_type": "markdown", "metadata": {}, "source": "## Import Libraries"}, {"cell_type": "markdown", "metadata": {}, "source": "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import spacy\nfrom spacy import displacy"}, {"cell_type": "markdown", "metadata": {}, "source": "We're also going to import the `Counter` module for counting people, places, and things, and the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from collections import Counter"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import pandas as pd\npd.set_option(\"max_rows\", 400)\npd.set_option(\"max_colwidth\", 400)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Download Language Model"}, {"cell_type": "markdown", "metadata": {}, "source": "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!python -m spacy download en_core_web_sm"}, {"cell_type": "markdown", "metadata": {}, "source": "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies \u2014 such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*"}, {"cell_type": "markdown", "metadata": {}, "source": "## Load Language Model"}, {"cell_type": "markdown", "metadata": {}, "source": "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import en_core_web_sm\nnlp = en_core_web_sm.load()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nlp = spacy.load('en_core_web_sm')"}, {"cell_type": "markdown", "metadata": {}, "source": "## Create a Processed spaCy Document"}, {"cell_type": "markdown", "metadata": {}, "source": "`document = nlp(open(filepath, , encoding='utf-8').read())`"}, {"cell_type": "markdown", "metadata": {}, "source": "Whenever we use spaCy, our first step will be to create a processed spaCy `document` with the loaded NLP model `nlp()`. Most of the heavy NLP lifting is done in this line of code. After processing, the `document` object will contain tons of juicy language data \u2014 named entities, sentence boundaries, parts of speech \u2014\u00a0and the rest of our work will be devoted to accessing this information.\n\nIn the cell below, we `open()` and `.read()` Ada Lovelace's obituary. Then we run`nlp()` on the text and create our `document`."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\n\ndocument = nlp(open(filepath, encoding='utf-8').read())"}, {"cell_type": "markdown", "metadata": {}, "source": "## spaCy Named Entities"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|:---:|:---:|\n|PERSON|People, including fictional.|\n|NORP|Nationalities or religious or political groups.|\n|FAC|Buildings, airports, highways, bridges, etc.|\n|ORG|Companies, agencies, institutions, etc.|\n|GPE|Countries, cities, states.|\n|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n|WORK_OF_ART|Titles of books, songs, etc.|\n|LAW|Named documents made into laws.|\n|LANGUAGE|Any named language.|\n|DATE|Absolute or relative dates or periods.|\n|TIME|Times smaller than a day.|\n|PERCENT|Percentage, including \u201d%\u201c.|\n|MONEY|Monetary values, including unit.|\n|QUANTITY|Measurements, as of weight or distance.|\n|ORDINAL|\u201cfirst\u201d, \u201csecond\u201d, etc.|\n|CARDINAL|Numerals that do not fall under another type.|\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Above is a Named Entities chart taken from [spaCy's website](https://spacy.io/api/annotation#named-entities), which shows the different named entities that spaCy can identify as well as their corresponding type labels. To quickly see spaCy's NER in action, we can use the [spaCy module `displacy`](https://spacy.io/usage/visualizers#ent) with the `style=` parameter set to \"ent\"  (short for entities):"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "displacy.render(document, style=\"ent\")"}, {"cell_type": "markdown", "metadata": {}, "source": "From a quick glance at the text above, we can see that spaCy is doing quite well with NER. But it's definitely not perfect.\n\nThough spaCy correctly identifies \"Ada Lovelace\" as a `PERSON` in the first sentence, just a few sentences later it labels her as a `WORK_OF_ART`. Though spaCy correctly identifies \"London\" as a place `GPE` a few paragraphs down, it incorrectly identifies \"Jacquard\" as a place `GPE`, too (when really \"Jacquard\" is a type of loom, named after [Marie Jacquard](https://en.wikipedia.org/wiki/Jacquard_machine)). \n\nThis inconsistency is very important to note and keep in mind. If we wanted to use spaCy's NER for a project, it would almost certainly require manual correction and cleaning. And even then it wouldn't be perfect. That's why understanding the limitations of this tool is so crucial. While spaCy's NER can be very good for identifying entities in broad strokes, it can't be relied upon for anything exact and fine-grained \u2014 not out of the box anyway."}, {"cell_type": "markdown", "metadata": {}, "source": "## Get Named Entities"}, {"cell_type": "markdown", "metadata": {}, "source": "All the named entities in our `document` can be found in the `document.ents` property. If we check out `document.ents`, we can see all the entities from Ada Lovelace's obituary."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "document.ents"}, {"cell_type": "markdown", "metadata": {}, "source": "Each of the named entities in `document.ents` contains [more information about itself](https://spacy.io/usage/linguistic-features#accessing), which we can access by iterating through the `document.ents` with a simple `for` loop. `For` each `named_entity` in `document.ents`, we will extract the `named_entity` and its corresponding `named_entity.label_`."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "for named_entity in document.ents:\n    print(named_entity, named_entity.label_)"}, {"cell_type": "markdown", "metadata": {}, "source": "To extract just the named entities that have been identified as `PERSON`, we can add a simple `if` statement into the mix:"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "for named_entity in document.ents:\n    if named_entity.label_ == \"PERSON\":\n        print(named_entity)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Practicing with *Lost in the City*"}, {"cell_type": "markdown", "metadata": {}, "source": "For the rest of this lesson, we're going to work with Edward P. Jones's short story collection *Lost in the City*."}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://mybinder.org/static/images/logo_social.png\" width=\"150\" align=\"left\" > *If you're using this Jupyter notebook in Binder (in the cloud), please uncomment the cell below and work with only the first story from _Lost in the City_. The Binder notebook is currently having issues loading the entire collection.*"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#file = \"../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt\"\n#document = nlp(open(file).read())"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "filepath = \"../texts/literature/Jones-Lost-in-The-City.txt\"\ndocument = nlp(open(filepath, encoding=\"utf-8\").read())"}, {"cell_type": "markdown", "metadata": {}, "source": "## Get People"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|:---:|:---:|\n|PERSON|People, including fictional.|"}, {"cell_type": "markdown", "metadata": {}, "source": "To extract and count the people identified in *Lost in the City*, we will follow the same model as above, using an `if` statement that will pull out words only if their \"ent\" label matches \"PERSON.\""}, {"cell_type": "markdown", "metadata": {}, "source": "> \ud83d\udc0d **Python Review** \ud83d\udc0d\n\n>*While we demonstrate how to extract named entities in the sections below, we're also going to reinforce some integral Python skills. Notice how we use `for` loops and `if` statements to `.append()` specific words to a list. Then we count the words in the list and make a pandas dataframe from the list.* "}, {"cell_type": "markdown", "metadata": {}, "source": "Here's the code all together:"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "people = []\nfor named_entity in document.ents:\n    if named_entity.label_ == \"PERSON\":\n        people.append(named_entity.text)\n        \npeople_tally = Counter(people)\n\ndf = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "Here's the code broken up. We make a list of all the people identified in *Lost in the City*:"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "people = []\nfor named_entity in document.ents:\n    if named_entity.label_ == \"PERSON\":\n        people.append(named_entity.text)"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "people"}, {"cell_type": "markdown", "metadata": {}, "source": "Then we count the unique people in this list with the `Counter()` module:"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "people_tally = Counter(people)"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "people_tally.most_common()"}, {"cell_type": "markdown", "metadata": {}, "source": "Then we make a dataframe from this list with `pd.DataFrame()`:"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "df = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "To write this dataframe (or any dataframe!) to a CSV file, we can use `df.to_csv()`. To create a CSV file of character counts, uncomment the cell below:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df.to_csv(\"Lost-in-the-City-characters.csv\", encoding='utf-8', index=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Get Places"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|:---:|:---:|\n|GPE|Countries, cities, states.|\n|LOC|Non-GPE locations, mountain ranges, bodies of water.|"}, {"cell_type": "markdown", "metadata": {}, "source": "To extract and count places, we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"GPE\" or \"LOC.\" These are the type labels for \"counties cities, states\" and \"locations, mountain ranges, bodies of water.\""}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "places = []\nfor named_entity in document.ents:\n    if named_entity.label_ == \"GPE\" or named_entity.label_ == \"LOC\":\n        places.append(named_entity.text)\n\nplaces_tally = Counter(places)\n\ndf = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "Do you notice anything off about this list...?"}, {"cell_type": "markdown", "metadata": {}, "source": "## Get Streets & Parks"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|:---:|:---:|\n|FAC|Buildings, airports, highways, bridges, etc.|"}, {"cell_type": "markdown", "metadata": {}, "source": "To extract and count streets and parks (which show up a lot in *Lost in the City*!), we can follow the same model as above, except we will change our `if` statement to check for \"ent\" labels that match \"FAC.\" This is the type label for \"buildings, airports, highways, bridges, etc.\""}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "streets = []\nfor named_entity in document.ents:\n    if named_entity.label_ == \"FAC\":\n        streets.append(named_entity.text)\n\nstreets_tally = Counter(streets)\n\ndf = pd.DataFrame(streets_tally.most_common(), columns = ['street', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "## Get Works of Art"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|:---:|:---:|\n|WORK_OF_ART|Titles of books, songs, etc.|"}, {"cell_type": "markdown", "metadata": {}, "source": "To extract and count works of art, we can follow a similar-ish model to the examples above. This time, however, we're going to make our code even more economical and efficient (while still changing our `if` statement to match the \"ent\" label \"WORK_OF_ART\")."}, {"cell_type": "markdown", "metadata": {}, "source": "> \ud83d\udc0d **Python Review** \ud83d\udc0d\n\n>We can use a [*list comprehension*](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html#List-Comprehensions) to get our list of named entities in a single line of code! Closely examine the first line of code below:"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "works_of_art = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"WORK_OF_ART\"]\n\nart_tally = Counter(works_of_art)\n\ndf = pd.DataFrame(art_tally.most_common(), columns = ['work_of_art', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "## Your Turn!"}, {"cell_type": "markdown", "metadata": {}, "source": "Now it's your turn to take a crack at NER with a whole new text!\n"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|:---:|:---:|\n|PERSON|People, including fictional.|\n|NORP|Nationalities or religious or political groups.|\n|FAC|Buildings, airports, highways, bridges, etc.|\n|ORG|Companies, agencies, institutions, etc.|\n|GPE|Countries, cities, states.|\n|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n|WORK_OF_ART|Titles of books, songs, etc.|\n|LAW|Named documents made into laws.|\n|LANGUAGE|Any named language.|\n|DATE|Absolute or relative dates or periods.|\n|TIME|Times smaller than a day.|\n|PERCENT|Percentage, including \u201d%\u201c.|\n|MONEY|Monetary values, including unit.|\n|QUANTITY|Measurements, as of weight or distance.|\n|ORDINAL|\u201cfirst\u201d, \u201csecond\u201d, etc.|\n|CARDINAL|Numerals that do not fall under another type.|\n"}, {"cell_type": "markdown", "metadata": {}, "source": "In this section, you're going to extract and count named entities from Barack Obama's memoir *The Audacity of Hope*. We're exploring Obama's memoir because it's chock full of named entities."}, {"cell_type": "markdown", "metadata": {}, "source": "Read in and process the text file"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "file = \"../texts/literature/Obama-The-Audacity-of-Hope.txt\"\n\ndocument = nlp(open(file, encoding='utf-8').read())"}, {"cell_type": "markdown", "metadata": {}, "source": "**1.** Choose a named entity from the possible spaCy named entities listed above. Extract, count, and make a dataframe from the most frequent named entities (of the type that you've chosen) in *The Audacity of Hope*. If you need help, study the examples above."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#Your Code Here \ud83d\udc47 \n"}, {"cell_type": "markdown", "metadata": {}, "source": "**2.** What is a result from this NER extraction that conformed to your expectations, that you find obvious or predictable? Why?"}, {"cell_type": "markdown", "metadata": {}, "source": "**#**Your answer here. (Double click this cell to type your answer.)"}, {"cell_type": "markdown", "metadata": {}, "source": "**3.** What is a result from this NER extraction that defied your expectations, that you find curious or counterintuitive? Why?"}, {"cell_type": "markdown", "metadata": {}, "source": "**#**Your answer here. (Double click this cell to type your answer.)"}, {"cell_type": "markdown", "metadata": {}, "source": "**4.** What's an insight that you might be able to glean about *The Audacity of Hope* based on your NER extraction?"}, {"cell_type": "markdown", "metadata": {}, "source": "**#**Your answer here. (Double click this cell to type your answer.)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 4}