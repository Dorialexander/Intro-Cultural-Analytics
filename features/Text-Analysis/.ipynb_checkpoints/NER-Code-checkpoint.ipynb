{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Named Entity Recognition \u2014 Code"}, {"cell_type": "markdown", "metadata": {}, "source": "[Download relevant files](https://melaniewalsh.org/spacy.zip)"}, {"cell_type": "markdown", "metadata": {}, "source": "This notebook is a streamlined version of a previous lesson on **Named Entity Recognition**. It is primarily intended for those who want to reuse the code without the previous lessons' overview and explanations."}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"../images/Ada-Lovelace-NER.png\" >"}, {"cell_type": "markdown", "metadata": {}, "source": "## Install spaCy"}, {"cell_type": "markdown", "metadata": {}, "source": "To use spaCy, we first need to install the library."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install -U spacy"}, {"cell_type": "markdown", "metadata": {}, "source": "## Import Libraries"}, {"cell_type": "markdown", "metadata": {}, "source": "Then we're going to import `spacy` and `displacy`, a special spaCy module for visualization."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "import spacy\nfrom spacy import displacy"}, {"cell_type": "markdown", "metadata": {}, "source": "We're also going to import the `Counter` module for counting people, places, and things later on; the `pandas` library for organizing and displaying data (we're also changing the pandas default max row and column width display setting)."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "from collections import Counter"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "import pandas as pd\npd.set_option(\"max_rows\", 400)\npd.set_option(\"max_colwidth\", 400)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Download Language Model"}, {"cell_type": "markdown", "metadata": {}, "source": "Next we need to download the English-language model (`en_core_web_sm`), which will be processing and making predictions about our texts. This is the model that was trained on the annotated \"OntoNotes\" corpus. You can download the `en_core_web_sm` model by running the cell below:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!python -m spacy download en_core_web_sm"}, {"cell_type": "markdown", "metadata": {}, "source": "*Note: spaCy offers [models for other languages](https://spacy.io/usage/models#languages) including German, French, Spanish, Portuguese, Italian, Dutch, Greek, Norwegian, and Lithuanian. Languages such as Russian, Ukrainian, Thai, Chinese, Japanese, Korean and Vietnamese don't currently have their own NLP models. However, spaCy offers language and tokenization support for many of these language with external dependencies \u2014 such as [PyviKonlpy](https://github.com/konlpy/konlpy) for Korean or [Jieba](https://github.com/fxsjy/jieba) for Chinese.*"}, {"cell_type": "markdown", "metadata": {}, "source": "## Load Language Model"}, {"cell_type": "markdown", "metadata": {}, "source": "Once the model is downloaded, we need to load it with `spacy.load()` and assign it to the variable `nlp`."}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "nlp = spacy.load('en_core_web_sm')"}, {"cell_type": "markdown", "metadata": {}, "source": "## Create a Processed spaCy Document"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "filepath = \"../texts/history/NYT-Obituaries/1852-Ada-Lovelace.txt\"\ntext = open(filepath, encoding='utf-8').read()\ndocument = nlp(text)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Get Named Entities"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|--- |--- |\n|PERSON|People, including fictional.|\n|NORP|Nationalities or religious or political groups.|\n|FAC|Buildings, airports, highways, bridges, etc.|\n|ORG|Companies, agencies, institutions, etc.|\n|GPE|Countries, cities, states.|\n|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n|WORK_OF_ART|Titles of books, songs, etc.|\n|LAW|Named documents made into laws.|\n|LANGUAGE|Any named language.|\n|DATE|Absolute or relative dates or periods.|\n|TIME|Times smaller than a day.|\n|PERCENT|Percentage, including \u201d%\u201c.|\n|MONEY|Monetary values, including unit.|\n|QUANTITY|Measurements, as of weight or distance.|\n|ORDINAL|\u201cfirst\u201d, \u201csecond\u201d, etc.|\n|CARDINAL|Numerals that do not fall under another type.|\n"}, {"cell_type": "markdown", "metadata": {}, "source": "All the named entities in our `document` can be found in the `document.ents` property. We can access the entity labels by iterating through the `document.ents` with a simple `for` loop and pulling out the `.label_` attribute."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "for named_entity in document.ents:\n    print(named_entity, named_entity.label_)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Get People"}, {"cell_type": "markdown", "metadata": {}, "source": "|Type Label|Description|\n|--- |--- |\n|PERSON|People, including fictional.|"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "people = [named_entity.text for named_entity in document.ents if named_entity.label_ == \"PERSON\"]\npeople_tally = Counter(people)\ndf = pd.DataFrame(people_tally.most_common(), columns=['character', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "## Process Long Documents (or Many Documents)"}, {"cell_type": "markdown", "metadata": {}, "source": "Rather than creating a single processed `document` with `nlp()`, we're going to create a bunch of smaller spaCy `documents` with `nlp.pipe()`. The [`nlp.pipe()`](https://spacy.io/usage/processing-pipelines#processing) method is faster and more efficient when we're processing many documents."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "filepath = \"../texts/literature/Little-Women.txt\"\ntext = open(filepath, encoding=\"utf-8\").read()\n\n#Split text on line breaks \nchunked_text = text.split('\\n')\n#Process each chunk of text and return a list of processed documents\nchunked_documents = list(nlp.pipe(chunked_text))"}, {"cell_type": "markdown", "metadata": {}, "source": "We `open()` and `.read()` our text file, then `.split()` the text on every line break `\\n` and process each chunk of the text as its own document, returning a list of `chunked_documents`."}, {"cell_type": "markdown", "metadata": {}, "source": "To extract people from all the `chunked_documents`, all we need to do is add one more `for` loop to our code and iterate through every document in `chunked_documents`."}, {"cell_type": "code", "execution_count": 29, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>place</th>\n", "      <th>count</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Jo</td>\n", "      <td>1295</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Laurie</td>\n", "      <td>482</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Beth</td>\n", "      <td>435</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>Amy</td>\n", "      <td>424</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>Meg</td>\n", "      <td>422</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>541</th>\n", "      <td>Gutenberg-tm</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>542</th>\n", "      <td>Project Gutenberg-tm's</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>543</th>\n", "      <td>Gregory B. Newby</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>544</th>\n", "      <td>Michael S. Hart</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>545</th>\n", "      <td>eBooks</td>\n", "      <td>1</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>546 rows \u00d7 2 columns</p>\n", "</div>"], "text/plain": ["                      place  count\n", "0                        Jo   1295\n", "1                    Laurie    482\n", "2                      Beth    435\n", "3                       Amy    424\n", "4                       Meg    422\n", "..                      ...    ...\n", "541            Gutenberg-tm      1\n", "542  Project Gutenberg-tm's      1\n", "543        Gregory B. Newby      1\n", "544         Michael S. Hart      1\n", "545                  eBooks      1\n", "\n", "[546 rows x 2 columns]"]}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": "people = []\nfor document in chunked_documents:\n    for named_entity in document.ents:\n        if named_entity.label_ == \"PERSON\":\n            people.append(named_entity.text)\n            \npeople_tally = Counter(people)\n\ndf = pd.DataFrame(people_tally.most_common(), columns=['place', 'count'])\ndf"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "places = [named_entity.text  for document in chunked_documents for named_entity in document.ents if named_entity.label_ == \"GPE\"]\n\nplaces_tally = Counter(places)\n\ndf = pd.DataFrame(places_tally.most_common(), columns=['place', 'count'])\ndf"}, {"cell_type": "markdown", "metadata": {}, "source": "To write these dataframe to a CSV file, we can use `df.to_csv()`:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df.to_csv(\"people.csv\", encoding='utf-8', index=False)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 4}